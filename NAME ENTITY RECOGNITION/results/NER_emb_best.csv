architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
"EMB_50","{'name': 'sequential_1', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 50, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 50)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9077073931694031,0.2391454428434372,0.9044883251190186,0.20871032774448395,1943.0,3340.0,1005.0,3690.0,9987.5,0.3755805373795684,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.09      1.00      0.17         2\n           2       0.72      0.90      0.80        20\n           3       0.74      0.94      0.83       101\n           4       0.04      1.00      0.08         8\n           5       0.06      1.00      0.11         5\n           6       0.00      0.00      0.00         1\n           7       0.29      1.00      0.44        10\n           8       0.30      1.00      0.46         3\n           9       0.18      0.67      0.29         3\n          10       0.00      1.00      0.00         0\n          11       0.20      0.67      0.31         3\n          12       0.00      1.00      0.00         0\n          13       0.35      0.86      0.50         7\n          14       0.40      0.80      0.53         5\n          15       0.75      0.82      0.78        11\n          16       0.31      0.82      0.45        11\n          17       0.37      0.74      0.50        46\n          18       0.73      0.97      0.83        33\n          19       0.71      0.83      0.77         6\n          20       0.48      0.98      0.64        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.50      0.92      0.65        12\n          25       0.75      0.85      0.80       102\n          26       0.80      0.82      0.81        39\n          27       0.63      0.92      0.75        39\n          28       0.65      0.92      0.76        12\n          29       0.14      1.00      0.25         1\n          30       0.00      0.00      0.00         1\n          31       0.04      0.33      0.07         9\n          32       0.64      0.81      0.71        68\n          33       0.00      0.00      0.00         1\n          34       0.82      0.79      0.81        34\n          35       0.76      0.90      0.82        31\n          36       0.83      1.00      0.91         5\n          37       0.05      1.00      0.10         2\n          38       0.20      0.92      0.33        12\n          39       0.86      0.86      0.86         7\n          40       0.63      0.88      0.74        49\n          41       0.11      0.67      0.18        12\n          42       0.55      0.81      0.66        27\n          43       0.08      1.00      0.14         9\n          44       0.01      1.00      0.02         3\n          45       0.09      0.53      0.16        19\n          46       0.90      0.90      0.90       656\n          47       0.42      1.00      0.59         5\n          48       0.32      0.86      0.46         7\n          49       0.21      1.00      0.34         5\n          50       0.03      1.00      0.06         1\n          51       0.86      0.86      0.86         7\n          52       0.01      0.40      0.03         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.22      0.83      0.34         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          62       0.90      0.97      0.93        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.31      0.93      0.46        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.13      1.00      0.24         2\n          72       0.10      0.75      0.17         4\n          73       0.86      0.89      0.87       567\n          75       0.72      1.00      0.84        13\n          76       0.10      0.83      0.18         6\n          77       0.15      1.00      0.27        11\n          78       0.45      0.98      0.62        53\n          79       0.16      1.00      0.28         6\n          80       0.00      1.00      0.00         0\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.50      0.50      0.50         2\n          84       0.42      0.83      0.56         6\n          85       0.00      1.00      0.00         0\n          86       0.24      0.71      0.36         7\n          87       0.84      1.00      0.92        27\n          88       0.70      1.00      0.82         7\n          89       0.40      1.00      0.57         2\n          90       0.50      0.92      0.65        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.53      0.75      0.62        24\n          97       0.20      1.00      0.33         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.05      0.67      0.09         6\n         103       0.20      0.50      0.28        20\n         104       0.21      0.92      0.34       110\n         105       0.40      1.00      0.57         2\n         106       0.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.67      1.00      0.80         2\n         109       0.00      1.00      0.00         0\n         110       0.00      1.00      0.00         0\n         111       0.90      0.98      0.94        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.10      1.00      0.19         5\n         116       0.60      0.98      0.74       132\n         117       0.33      1.00      0.50         2\n         118       0.21      1.00      0.35         8\n         119       0.99      0.50      0.67      5640\n\n    accuracy                           0.69     10056\n   macro avg       0.33      0.84      0.37     10056\nweighted avg       0.91      0.69      0.75     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.07      1.00      0.14         2\n           2       0.53      0.90      0.67        20\n           3       0.26      0.97      0.41       101\n           4       0.05      1.00      0.09         8\n           5       0.06      1.00      0.12         5\n           6       0.00      0.00      0.00         1\n           7       0.35      0.90      0.50        10\n           8       0.25      0.67      0.36         3\n           9       0.15      0.67      0.25         3\n          11       0.60      1.00      0.75         3\n          12       0.00      1.00      0.00         0\n          13       0.45      0.71      0.56         7\n          14       0.80      0.80      0.80         5\n          15       0.47      0.73      0.57        11\n          16       0.15      0.55      0.24        11\n          17       0.42      0.72      0.53        46\n          18       0.84      0.97      0.90        33\n          19       0.60      1.00      0.75         6\n          20       0.73      0.97      0.83        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.52      0.92      0.67        12\n          25       0.33      0.92      0.49       102\n          26       0.56      0.87      0.68        39\n          27       0.75      0.92      0.83        39\n          28       0.62      0.83      0.71        12\n          29       0.33      1.00      0.50         1\n          30       0.20      1.00      0.33         1\n          31       0.11      0.78      0.19         9\n          32       0.78      0.79      0.79        68\n          33       0.14      1.00      0.25         1\n          34       0.60      0.74      0.66        34\n          35       0.65      0.84      0.73        31\n          36       1.00      1.00      1.00         5\n          37       0.20      1.00      0.33         2\n          38       0.58      0.92      0.71        12\n          39       0.67      0.86      0.75         7\n          40       0.37      0.90      0.53        49\n          41       0.03      0.50      0.05        12\n          42       0.32      0.85      0.46        27\n          43       0.04      1.00      0.08         9\n          44       0.23      1.00      0.38         3\n          45       0.26      0.53      0.34        19\n          46       0.91      0.88      0.90       656\n          47       0.62      1.00      0.77         5\n          48       0.03      0.71      0.06         7\n          49       0.83      1.00      0.91         5\n          50       0.00      0.00      0.00         1\n          51       0.75      0.86      0.80         7\n          52       0.02      0.20      0.03         5\n          54       0.50      0.83      0.62         6\n          55       0.00      1.00      0.00         0\n          56       0.83      0.83      0.83         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.25      0.93      0.39        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.22      1.00      0.36         2\n          72       0.12      0.75      0.21         4\n          73       0.82      0.90      0.86       567\n          75       0.68      1.00      0.81        13\n          76       0.08      0.83      0.15         6\n          77       0.21      1.00      0.34        11\n          78       0.24      0.98      0.38        53\n          79       0.23      1.00      0.38         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.50      0.67         2\n          83       0.67      1.00      0.80         2\n          84       0.38      0.50      0.43         6\n          85       0.00      1.00      0.00         0\n          86       0.22      0.71      0.33         7\n          87       0.75      1.00      0.86        27\n          88       0.39      1.00      0.56         7\n          89       0.09      1.00      0.17         2\n          90       0.42      0.92      0.58        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.33      1.00      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.56      0.75      0.64        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       0.14      0.50      0.22         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.05      0.50      0.09         2\n         102       0.07      1.00      0.13         6\n         103       0.22      0.45      0.30        20\n         104       0.38      0.88      0.53       110\n         105       0.33      1.00      0.50         2\n         106       0.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.67      1.00      0.80         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.23      1.00      0.37         5\n         116       0.66      0.93      0.77       132\n         117       0.18      1.00      0.31         2\n         118       0.30      1.00      0.46         8\n         119       0.99      0.47      0.64      5640\n\n    accuracy                           0.67     10056\n   macro avg       0.34      0.86      0.39     10056\nweighted avg       0.90      0.67      0.72     10056\n']",1000,50,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_128","{'name': 'sequential_5', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_3'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_3', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 128, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9373252391815186,0.25090619921684265,0.9390963912010193,0.22345906496047974,2227.0,2344.5,721.0,2574.5,8773.0,0.4576429336231346,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.12      1.00      0.21         2\n           2       0.69      0.90      0.78        20\n           3       0.66      0.97      0.78       101\n           4       0.10      1.00      0.19         8\n           5       0.21      1.00      0.34         5\n           6       0.00      0.00      0.00         1\n           7       0.29      0.80      0.42        10\n           8       0.40      0.67      0.50         3\n           9       0.29      0.67      0.40         3\n          11       0.33      0.67      0.44         3\n          12       0.00      1.00      0.00         0\n          13       0.29      0.57      0.38         7\n          14       0.60      0.60      0.60         5\n          15       0.69      0.82      0.75        11\n          16       0.35      0.82      0.49        11\n          17       0.32      0.74      0.44        46\n          18       0.41      0.97      0.58        33\n          19       0.33      1.00      0.50         6\n          20       0.80      1.00      0.89        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.61      0.92      0.73        12\n          25       0.71      0.95      0.82       102\n          26       0.56      0.87      0.68        39\n          27       0.53      0.95      0.68        39\n          28       0.19      0.83      0.30        12\n          29       0.17      1.00      0.29         1\n          30       0.00      0.00      0.00         1\n          31       0.16      0.89      0.27         9\n          32       0.64      0.85      0.73        68\n          33       0.25      1.00      0.40         1\n          34       0.74      0.76      0.75        34\n          35       0.10      0.84      0.17        31\n          36       1.00      1.00      1.00         5\n          37       0.09      1.00      0.16         2\n          38       0.58      0.92      0.71        12\n          39       0.86      0.86      0.86         7\n          40       0.61      0.92      0.73        49\n          41       0.08      0.67      0.14        12\n          42       0.70      0.85      0.77        27\n          43       0.05      1.00      0.09         9\n          44       0.30      1.00      0.46         3\n          45       0.31      0.63      0.41        19\n          46       0.92      0.92      0.92       656\n          47       0.31      0.80      0.44         5\n          48       0.08      0.86      0.14         7\n          49       0.71      1.00      0.83         5\n          50       0.03      1.00      0.06         1\n          51       0.75      0.86      0.80         7\n          52       0.10      0.40      0.16         5\n          54       0.50      0.67      0.57         6\n          55       0.00      1.00      0.00         0\n          56       0.25      0.83      0.38         6\n          57       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.30      1.00      0.47        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.33      1.00      0.50         2\n          72       0.10      0.75      0.18         4\n          73       0.89      0.90      0.90       567\n          75       0.75      0.92      0.83        13\n          76       0.09      1.00      0.16         6\n          77       0.31      1.00      0.48        11\n          78       0.49      0.98      0.65        53\n          79       0.30      1.00      0.46         6\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.25      1.00      0.40         2\n          84       0.45      0.83      0.59         6\n          86       0.17      0.57      0.26         7\n          87       0.82      1.00      0.90        27\n          88       0.44      1.00      0.61         7\n          89       0.10      1.00      0.17         2\n          90       0.44      0.92      0.59        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.61      0.79      0.69        24\n          97       0.25      1.00      0.40         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.07      1.00      0.13         6\n         103       0.19      0.60      0.29        20\n         104       0.41      0.94      0.57       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.29      1.00      0.45         5\n         116       0.70      0.96      0.81       132\n         117       0.40      1.00      0.57         2\n         118       0.31      1.00      0.47         8\n         119       0.99      0.61      0.76      5640\n\n    accuracy                           0.76     10056\n   macro avg       0.36      0.86      0.41     10056\nweighted avg       0.92      0.76      0.80     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.22      1.00      0.36         2\n           2       0.86      0.90      0.88        20\n           3       0.84      0.98      0.90       101\n           4       0.18      1.00      0.31         8\n           5       0.14      1.00      0.24         5\n           6       0.00      0.00      0.00         1\n           7       0.35      0.90      0.50        10\n           8       0.25      0.67      0.36         3\n           9       0.20      0.67      0.31         3\n          11       0.60      1.00      0.75         3\n          12       0.00      1.00      0.00         0\n          13       0.44      0.57      0.50         7\n          14       0.36      0.80      0.50         5\n          15       0.75      0.82      0.78        11\n          16       0.27      0.82      0.41        11\n          17       0.26      0.74      0.39        46\n          18       0.65      0.94      0.77        33\n          19       0.67      1.00      0.80         6\n          20       0.97      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.58      0.92      0.71        12\n          25       0.77      0.93      0.84       102\n          26       0.60      0.85      0.70        39\n          27       0.65      0.92      0.77        39\n          28       0.38      0.75      0.50        12\n          29       0.11      1.00      0.20         1\n          30       0.00      0.00      0.00         1\n          31       0.18      0.67      0.29         9\n          32       0.66      0.87      0.75        68\n          33       0.00      0.00      0.00         1\n          34       0.79      0.79      0.79        34\n          35       0.67      0.90      0.77        31\n          36       0.71      1.00      0.83         5\n          37       0.33      1.00      0.50         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.47      0.92      0.62        49\n          41       0.16      0.58      0.25        12\n          42       0.59      0.85      0.70        27\n          43       0.09      1.00      0.16         9\n          44       0.43      1.00      0.60         3\n          45       0.21      0.84      0.34        19\n          46       0.93      0.90      0.92       656\n          47       1.00      1.00      1.00         5\n          48       0.07      0.71      0.13         7\n          49       0.83      1.00      0.91         5\n          50       0.02      1.00      0.04         1\n          51       0.67      0.86      0.75         7\n          52       0.03      0.20      0.05         5\n          54       0.42      0.83      0.56         6\n          55       0.00      1.00      0.00         0\n          56       0.71      0.83      0.77         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.89      0.97      0.93        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.33      1.00      0.49        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.10      0.75      0.18         4\n          73       0.85      0.89      0.87       567\n          74       0.00      1.00      0.00         0\n          75       0.76      1.00      0.87        13\n          76       0.07      0.83      0.12         6\n          77       0.55      1.00      0.71        11\n          78       0.45      0.98      0.62        53\n          79       0.46      1.00      0.63         6\n          80       0.00      1.00      0.00         0\n          81       1.00      1.00      1.00         2\n          83       0.50      1.00      0.67         2\n          84       0.80      0.67      0.73         6\n          86       0.24      0.71      0.36         7\n          87       0.75      1.00      0.86        27\n          88       0.64      1.00      0.78         7\n          89       0.09      1.00      0.16         2\n          90       0.61      0.92      0.73        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.33      1.00      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.60      0.88      0.71        24\n          97       0.20      1.00      0.33         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.06      0.83      0.11         6\n         103       0.31      0.75      0.44        20\n         104       0.64      0.93      0.76       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.25      1.00      0.40         5\n         116       0.71      0.98      0.82       132\n         117       0.50      1.00      0.67         2\n         118       0.32      1.00      0.48         8\n         119       0.99      0.71      0.83      5640\n\n    accuracy                           0.81     10056\n   macro avg       0.40      0.86      0.46     10056\nweighted avg       0.93      0.81      0.85     10056\n']",1000,128,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_256","{'name': 'sequential_8', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_5'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_5', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 256, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_5', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9533179998397827,0.2664068639278412,0.950899064540863,0.23318490386009216,2350.5,1643.0,597.5,1828.5,7851.5,0.48163653589435235,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.67      0.90      0.77        20\n           3       0.65      0.98      0.78       101\n           4       0.09      1.00      0.17         8\n           5       0.16      1.00      0.28         5\n           6       0.00      0.00      0.00         1\n           7       0.43      1.00      0.61        10\n           8       0.33      1.00      0.50         3\n           9       0.29      0.67      0.40         3\n          11       0.33      0.67      0.44         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.71      0.59         7\n          14       0.75      0.60      0.67         5\n          15       0.67      0.91      0.77        11\n          16       0.29      0.82      0.43        11\n          17       0.29      0.78      0.42        46\n          18       0.89      0.97      0.93        33\n          19       0.60      1.00      0.75         6\n          20       0.92      1.00      0.96        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.55      1.00      0.71        12\n          25       0.81      0.97      0.88       102\n          26       0.86      0.92      0.89        39\n          27       0.86      0.97      0.92        39\n          28       0.50      0.83      0.62        12\n          29       0.11      1.00      0.20         1\n          30       0.00      0.00      0.00         1\n          31       0.44      0.78      0.56         9\n          32       0.76      0.85      0.81        68\n          33       0.25      1.00      0.40         1\n          34       0.73      0.79      0.76        34\n          35       0.72      0.90      0.80        31\n          36       1.00      1.00      1.00         5\n          37       0.50      1.00      0.67         2\n          38       0.80      1.00      0.89        12\n          39       0.86      0.86      0.86         7\n          40       0.69      0.88      0.77        49\n          41       0.22      0.67      0.33        12\n          42       0.79      0.85      0.82        27\n          43       0.09      1.00      0.16         9\n          44       0.27      1.00      0.43         3\n          45       0.38      0.79      0.51        19\n          46       0.92      0.92      0.92       656\n          47       0.83      1.00      0.91         5\n          48       0.14      0.71      0.23         7\n          49       0.71      1.00      0.83         5\n          50       0.04      1.00      0.07         1\n          51       0.86      0.86      0.86         7\n          52       0.10      0.20      0.13         5\n          53       0.00      1.00      0.00         0\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.42      0.83      0.56         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       0.00      0.00      0.00         1\n          66       0.40      1.00      0.57        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.11      0.75      0.19         4\n          73       0.86      0.92      0.89       567\n          75       0.87      1.00      0.93        13\n          76       0.15      1.00      0.26         6\n          77       0.42      1.00      0.59        11\n          78       0.43      0.98      0.59        53\n          79       0.38      1.00      0.55         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.50      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.50      1.00      0.67         2\n          84       0.56      0.83      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.44      1.00      0.61         7\n          87       0.73      1.00      0.84        27\n          88       0.41      1.00      0.58         7\n          89       0.50      1.00      0.67         2\n          90       0.46      0.92      0.61        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.68      0.88      0.76        24\n          96       0.00      1.00      0.00         0\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.18      1.00      0.31         6\n         103       0.33      0.60      0.43        20\n         104       0.66      0.93      0.77       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.36      1.00      0.53         5\n         116       0.67      0.98      0.79       132\n         117       0.33      1.00      0.50         2\n         118       0.35      1.00      0.52         8\n         119       0.99      0.78      0.87      5640\n\n    accuracy                           0.85     10056\n   macro avg       0.43      0.87      0.49     10056\nweighted avg       0.93      0.85      0.88     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.75      0.90      0.82        20\n           3       0.91      0.99      0.95       101\n           4       0.24      1.00      0.38         8\n           5       0.25      1.00      0.40         5\n           6       1.00      0.00      0.00         1\n           7       0.48      1.00      0.65        10\n           8       0.38      1.00      0.55         3\n           9       0.20      0.67      0.31         3\n          11       0.50      1.00      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.67      0.86      0.75         7\n          14       1.00      0.80      0.89         5\n          15       0.77      0.91      0.83        11\n          16       0.28      0.73      0.40        11\n          17       0.35      0.78      0.48        46\n          18       0.87      1.00      0.93        33\n          19       0.75      1.00      0.86         6\n          20       0.55      1.00      0.71        61\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.50      0.92      0.65        12\n          25       0.85      0.98      0.91       102\n          26       0.82      0.92      0.87        39\n          27       0.78      0.92      0.85        39\n          28       0.50      0.83      0.62        12\n          29       0.20      1.00      0.33         1\n          30       0.00      0.00      0.00         1\n          31       0.33      0.78      0.47         9\n          32       0.79      0.81      0.80        68\n          33       0.00      0.00      0.00         1\n          34       0.70      0.82      0.76        34\n          35       0.66      0.87      0.75        31\n          36       1.00      1.00      1.00         5\n          37       0.67      1.00      0.80         2\n          38       1.00      0.92      0.96        12\n          39       0.43      0.86      0.57         7\n          40       0.69      0.86      0.76        49\n          41       0.16      0.75      0.26        12\n          42       0.53      0.85      0.66        27\n          43       0.05      1.00      0.09         9\n          44       0.50      1.00      0.67         3\n          45       0.33      0.74      0.45        19\n          46       0.92      0.93      0.92       656\n          47       1.00      1.00      1.00         5\n          48       0.12      0.86      0.20         7\n          49       0.83      1.00      0.91         5\n          50       0.06      1.00      0.11         1\n          51       0.67      0.86      0.75         7\n          52       0.12      0.20      0.15         5\n          53       0.00      1.00      0.00         0\n          54       0.56      0.83      0.67         6\n          55       0.00      1.00      0.00         0\n          56       0.45      0.83      0.59         6\n          58       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.39      0.93      0.55        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.16      0.75      0.26         4\n          73       0.88      0.94      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.17      1.00      0.29         6\n          77       0.31      1.00      0.47        11\n          78       0.51      0.98      0.68        53\n          79       0.38      1.00      0.55         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.62      0.83      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.26      0.86      0.40         7\n          87       0.93      1.00      0.96        27\n          88       0.58      1.00      0.74         7\n          89       0.22      1.00      0.36         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       1.00      0.50      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.83      0.80        24\n          96       0.00      1.00      0.00         0\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.05      0.67      0.10         6\n         103       0.25      0.60      0.35        20\n         104       0.66      0.95      0.78       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.67      0.98      0.80       132\n         117       0.67      1.00      0.80         2\n         118       0.29      1.00      0.44         8\n         119       0.99      0.77      0.87      5640\n\n    accuracy                           0.85     10056\n   macro avg       0.45      0.87      0.49     10056\nweighted avg       0.93      0.85      0.88     10056\n']",1000,256,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_512","{'name': 'sequential_10', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_6'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_6', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 512, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 512)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_6', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9740231037139893,0.2829620838165283,0.9677671194076538,0.2457452416419983,2499.5,1077.5,448.5,1215.0,5981.0,0.5564878193479058,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.80      1.00      0.89        20\n           3       0.91      0.96      0.93       101\n           4       0.62      1.00      0.76         8\n           5       0.29      1.00      0.45         5\n           6       0.00      0.00      0.00         1\n           7       0.56      1.00      0.71        10\n           8       0.38      1.00      0.55         3\n           9       0.40      0.67      0.50         3\n          11       0.33      1.00      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.57      0.53         7\n          14       0.60      0.60      0.60         5\n          15       0.91      0.91      0.91        11\n          16       0.28      0.91      0.43        11\n          17       0.27      0.74      0.40        46\n          18       0.91      0.97      0.94        33\n          19       0.83      0.83      0.83         6\n          20       0.97      1.00      0.98        61\n          24       0.55      0.92      0.69        12\n          25       0.96      0.97      0.97       102\n          26       0.88      0.92      0.90        39\n          27       0.83      0.97      0.89        39\n          28       0.45      0.83      0.59        12\n          29       0.17      1.00      0.29         1\n          30       1.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.68      0.87      0.76        68\n          33       0.00      0.00      0.00         1\n          34       0.81      0.85      0.83        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       1.00      1.00      1.00         2\n          38       1.00      0.92      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.79      0.92      0.85        49\n          41       0.44      0.67      0.53        12\n          42       0.83      0.89      0.86        27\n          43       0.12      1.00      0.22         9\n          44       1.00      1.00      1.00         3\n          45       0.38      0.74      0.50        19\n          46       0.94      0.94      0.94       656\n          47       0.71      1.00      0.83         5\n          48       0.35      0.86      0.50         7\n          49       0.71      1.00      0.83         5\n          50       0.03      1.00      0.06         1\n          51       0.86      0.86      0.86         7\n          52       0.06      0.20      0.09         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.45      0.83      0.59         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.45      0.93      0.60        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.20      0.75      0.32         4\n          73       0.90      0.95      0.92       567\n          75       0.81      1.00      0.90        13\n          76       0.26      0.83      0.40         6\n          77       0.52      1.00      0.69        11\n          78       0.58      0.98      0.73        53\n          79       0.50      1.00      0.67         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.60      1.00      0.75         6\n          86       0.36      0.71      0.48         7\n          87       0.87      1.00      0.93        27\n          88       0.54      1.00      0.70         7\n          89       0.50      1.00      0.67         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      0.50      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.82      0.96      0.88        24\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.17      0.83      0.29         6\n         103       0.62      0.80      0.70        20\n         104       0.88      0.95      0.91       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.79      0.98      0.88       132\n         117       0.50      1.00      0.67         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.87      0.92      5640\n\n    accuracy                           0.91     10056\n   macro avg       0.54      0.85      0.57     10056\nweighted avg       0.95      0.91      0.92     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.69      0.90      0.78        20\n           3       0.94      0.96      0.95       101\n           4       0.38      1.00      0.55         8\n           5       0.24      1.00      0.38         5\n           6       0.00      0.00      0.00         1\n           7       0.47      0.90      0.62        10\n           8       0.60      1.00      0.75         3\n           9       0.25      0.67      0.36         3\n          10       0.00      1.00      0.00         0\n          11       0.29      0.67      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.67      0.57      0.62         7\n          14       0.83      1.00      0.91         5\n          15       0.45      0.91      0.61        11\n          16       0.23      1.00      0.38        11\n          17       0.35      0.78      0.48        46\n          18       0.86      0.97      0.91        33\n          19       1.00      1.00      1.00         6\n          20       0.95      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          24       0.50      0.92      0.65        12\n          25       0.96      0.98      0.97       102\n          26       0.80      0.90      0.84        39\n          27       0.81      0.97      0.88        39\n          28       0.67      0.83      0.74        12\n          29       0.20      1.00      0.33         1\n          30       0.00      0.00      0.00         1\n          31       0.78      0.78      0.78         9\n          32       0.85      0.82      0.84        68\n          33       1.00      1.00      1.00         1\n          34       0.90      0.79      0.84        34\n          35       0.76      0.90      0.82        31\n          36       1.00      1.00      1.00         5\n          37       0.13      1.00      0.24         2\n          38       0.61      0.92      0.73        12\n          39       0.86      0.86      0.86         7\n          40       0.66      0.88      0.75        49\n          41       0.43      0.50      0.46        12\n          42       0.82      0.85      0.84        27\n          43       0.11      1.00      0.20         9\n          44       0.60      1.00      0.75         3\n          45       0.43      0.79      0.56        19\n          46       0.95      0.94      0.95       656\n          47       0.67      0.80      0.73         5\n          48       0.64      1.00      0.78         7\n          49       0.71      1.00      0.83         5\n          50       0.00      0.00      0.00         1\n          51       0.75      0.86      0.80         7\n          52       0.06      0.20      0.09         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.71      0.83      0.77         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.47      1.00      0.64        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.92      0.95      0.94       567\n          74       0.00      1.00      0.00         0\n          75       0.59      1.00      0.74        13\n          76       0.15      0.83      0.26         6\n          77       0.58      1.00      0.73        11\n          78       0.51      0.98      0.68        53\n          79       0.43      1.00      0.60         6\n          81       0.50      1.00      0.67         2\n          83       0.67      1.00      0.80         2\n          84       0.55      1.00      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.35      0.86      0.50         7\n          87       0.75      1.00      0.86        27\n          88       0.37      1.00      0.54         7\n          89       1.00      1.00      1.00         2\n          90       0.69      0.92      0.79        12\n          91       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.67      0.92      0.77        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.31      0.83      0.45         6\n         103       0.53      0.85      0.65        20\n         104       0.80      0.95      0.87       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.67      1.00      0.80         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.33      1.00      0.50         5\n         116       0.84      0.98      0.91       132\n         117       0.67      1.00      0.80         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.50      0.87      0.55     10056\nweighted avg       0.95      0.90      0.92     10056\n']",1000,512,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_1024","{'name': 'sequential_14', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_9'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_9', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_9', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9828720092773438,0.2889518737792969,0.9753768444061279,0.2547328472137451,2581.5,803.0,366.5,915.5,4978.0,0.5929831440395132,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.86      0.90      0.88        20\n           3       0.96      0.97      0.97       101\n           4       0.25      1.00      0.40         8\n           5       0.27      0.80      0.40         5\n           6       0.00      0.00      0.00         1\n           7       0.77      1.00      0.87        10\n           8       0.50      1.00      0.67         3\n           9       0.12      0.67      0.21         3\n          11       0.75      1.00      0.86         3\n          12       0.00      1.00      0.00         0\n          13       0.41      1.00      0.58         7\n          14       0.80      0.80      0.80         5\n          15       0.59      0.91      0.71        11\n          16       0.36      0.91      0.51        11\n          17       0.35      0.83      0.49        46\n          18       0.85      1.00      0.92        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          23       0.00      1.00      0.00         0\n          24       0.52      0.92      0.67        12\n          25       0.96      0.98      0.97       102\n          26       0.90      0.90      0.90        39\n          27       0.86      0.97      0.92        39\n          28       0.62      0.83      0.71        12\n          29       0.50      1.00      0.67         1\n          30       0.25      1.00      0.40         1\n          31       0.75      1.00      0.86         9\n          32       0.87      0.88      0.88        68\n          33       0.00      0.00      0.00         1\n          34       0.79      0.88      0.83        34\n          35       0.74      0.90      0.81        31\n          36       1.00      1.00      1.00         5\n          37       0.67      1.00      0.80         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.82      0.86      0.84        49\n          41       0.50      0.50      0.50        12\n          42       0.85      0.85      0.85        27\n          43       0.13      1.00      0.23         9\n          44       1.00      1.00      1.00         3\n          45       0.34      0.74      0.47        19\n          46       0.94      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.60      0.86      0.71         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.12      0.20      0.15         5\n          54       0.80      0.67      0.73         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.70      1.00      0.82        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      1.00      0.42         4\n          73       0.92      0.95      0.93       567\n          75       0.72      1.00      0.84        13\n          76       0.24      0.83      0.37         6\n          77       0.31      1.00      0.47        11\n          78       0.62      0.98      0.76        53\n          79       0.67      1.00      0.80         6\n          80       0.00      1.00      0.00         0\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.50      1.00      0.67         2\n          84       0.50      1.00      0.67         6\n          86       0.23      0.86      0.36         7\n          87       0.87      1.00      0.93        27\n          88       0.50      1.00      0.67         7\n          89       0.67      1.00      0.80         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.72      0.88      0.79        24\n          97       0.33      1.00      0.50         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.33      0.83      0.48         6\n         103       0.58      0.75      0.65        20\n         104       0.94      0.95      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.90      0.98      0.94        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.87      0.99      0.93       132\n         117       0.67      1.00      0.80         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.89      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.56      0.87      0.60     10056\nweighted avg       0.95      0.92      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.83      1.00      0.91        20\n           3       0.99      0.96      0.97       101\n           4       0.62      1.00      0.76         8\n           5       0.16      1.00      0.27         5\n           6       0.00      0.00      0.00         1\n           7       0.56      1.00      0.71        10\n           8       1.00      0.67      0.80         3\n           9       0.67      0.67      0.67         3\n          11       0.50      0.67      0.57         3\n          12       0.00      1.00      0.00         0\n          13       0.35      0.86      0.50         7\n          14       0.83      1.00      0.91         5\n          15       0.56      0.91      0.69        11\n          16       0.38      1.00      0.55        11\n          17       0.36      0.78      0.49        46\n          18       0.80      0.97      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.91      1.00      0.95        61\n          24       0.69      0.92      0.79        12\n          25       0.97      0.98      0.98       102\n          26       0.89      0.87      0.88        39\n          27       0.90      0.97      0.94        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.75      1.00      0.86         9\n          32       0.85      0.85      0.85        68\n          33       1.00      1.00      1.00         1\n          34       1.00      0.82      0.90        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.73      0.94      0.82        49\n          41       0.67      0.50      0.57        12\n          42       0.83      0.89      0.86        27\n          43       0.20      1.00      0.34         9\n          44       1.00      1.00      1.00         3\n          45       0.44      0.79      0.57        19\n          46       0.95      0.94      0.94       656\n          47       1.00      0.80      0.89         5\n          48       0.75      0.86      0.80         7\n          49       1.00      0.80      0.89         5\n          50       0.03      1.00      0.06         1\n          51       0.86      0.86      0.86         7\n          52       0.12      0.20      0.15         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.85      0.98      0.91        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.67      1.00      0.80        14\n          68       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      0.75      0.38         4\n          73       0.93      0.95      0.94       567\n          75       0.87      1.00      0.93        13\n          76       0.33      1.00      0.50         6\n          77       0.55      1.00      0.71        11\n          78       0.60      0.98      0.74        53\n          79       0.50      1.00      0.67         6\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.46      1.00      0.63         6\n          85       0.00      1.00      0.00         0\n          86       0.46      0.86      0.60         7\n          87       1.00      1.00      1.00        27\n          88       0.64      1.00      0.78         7\n          89       0.50      1.00      0.67         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          93       1.00      0.50      0.67         2\n          95       0.83      0.83      0.83        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.28      0.83      0.42         6\n         103       0.68      0.75      0.71        20\n         104       0.95      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.92      0.98      0.95       132\n         117       0.50      1.00      0.67         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.58      0.87      0.61     10056\nweighted avg       0.96      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_2048","{'name': 'sequential_17', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_11'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_11', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 2048, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 2048)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_11', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9864808320999146,0.2977116107940674,0.980563223361969,0.26041561365127563,2614.5,593.0,333.5,688.5,4182.5,0.6289747872642832,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.87      1.00      0.93        20\n           3       0.99      0.97      0.98       101\n           4       0.32      1.00      0.48         8\n           5       0.33      1.00      0.50         5\n           6       0.00      0.00      0.00         1\n           7       0.64      0.90      0.75        10\n           8       1.00      0.67      0.80         3\n           9       0.50      0.67      0.57         3\n          11       0.50      0.67      0.57         3\n          12       0.00      1.00      0.00         0\n          13       0.67      0.86      0.75         7\n          14       1.00      1.00      1.00         5\n          15       0.77      0.91      0.83        11\n          16       0.44      1.00      0.61        11\n          17       0.36      0.80      0.50        46\n          18       0.94      0.97      0.96        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.98      0.96      0.97       102\n          26       0.92      0.85      0.88        39\n          27       0.88      0.97      0.93        39\n          28       0.91      0.83      0.87        12\n          29       0.33      1.00      0.50         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.87      0.87      0.87        68\n          33       1.00      0.00      0.00         1\n          34       0.94      0.85      0.89        34\n          35       0.73      0.87      0.79        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      0.92      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.88      0.92      0.90        49\n          41       0.67      0.50      0.57        12\n          42       0.74      0.85      0.79        27\n          43       0.17      1.00      0.29         9\n          44       0.75      1.00      0.86         3\n          45       0.54      0.74      0.62        19\n          46       0.94      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.70      1.00      0.82         7\n          49       1.00      1.00      1.00         5\n          50       0.17      1.00      0.29         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.62      0.83      0.71         6\n          58       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.58      1.00      0.74        14\n          68       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      0.75      0.40         4\n          73       0.93      0.95      0.94       567\n          75       0.93      1.00      0.96        13\n          76       0.40      1.00      0.57         6\n          77       0.50      1.00      0.67        11\n          78       0.70      0.98      0.82        53\n          79       0.50      1.00      0.67         6\n          81       1.00      1.00      1.00         2\n          83       0.50      1.00      0.67         2\n          84       0.60      1.00      0.75         6\n          86       0.55      0.86      0.67         7\n          87       0.90      1.00      0.95        27\n          88       0.54      1.00      0.70         7\n          89       0.50      1.00      0.67         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.96      0.92      0.94        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.23      0.83      0.36         6\n         103       0.56      0.75      0.64        20\n         104       0.96      0.97      0.97       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.93      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.40      1.00      0.57         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.61      0.87      0.63     10056\nweighted avg       0.96      0.94      0.95     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.95      0.90      0.92        20\n           3       0.96      0.99      0.98       101\n           4       0.62      1.00      0.76         8\n           5       0.50      1.00      0.67         5\n           6       0.00      0.00      0.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      1.00      1.00         3\n           9       0.40      0.67      0.50         3\n          11       0.75      1.00      0.86         3\n          13       0.55      0.86      0.67         7\n          14       1.00      1.00      1.00         5\n          15       0.71      0.91      0.80        11\n          16       0.38      1.00      0.55        11\n          17       0.51      0.80      0.62        46\n          18       0.94      0.97      0.96        33\n          19       0.83      0.83      0.83         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.98      0.96      0.97       102\n          26       0.92      0.90      0.91        39\n          27       0.88      0.95      0.91        39\n          28       0.92      0.92      0.92        12\n          29       0.12      1.00      0.22         1\n          30       0.00      0.00      0.00         1\n          31       0.73      0.89      0.80         9\n          32       0.87      0.88      0.88        68\n          33       1.00      1.00      1.00         1\n          34       0.97      0.85      0.91        34\n          35       0.76      0.90      0.82        31\n          36       1.00      1.00      1.00         5\n          37       0.40      1.00      0.57         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.92      0.92        49\n          41       1.00      0.42      0.59        12\n          42       0.82      0.85      0.84        27\n          43       0.26      1.00      0.42         9\n          44       0.75      1.00      0.86         3\n          45       0.41      0.74      0.53        19\n          46       0.96      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       1.00      0.86      0.92         7\n          49       1.00      1.00      1.00         5\n          50       0.10      1.00      0.18         1\n          51       0.86      0.86      0.86         7\n          52       0.33      0.20      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          60       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.70      1.00      0.82        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.18      0.75      0.29         4\n          73       0.93      0.95      0.94       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.33      0.83      0.48         6\n          77       0.92      1.00      0.96        11\n          78       0.76      0.98      0.86        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.50      0.67         2\n          83       1.00      0.00      0.00         2\n          84       0.50      1.00      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.46      0.86      0.60         7\n          87       1.00      1.00      1.00        27\n          88       0.54      1.00      0.70         7\n          89       1.00      1.00      1.00         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.88      0.92      0.90        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.42      0.83      0.56         6\n         103       0.68      0.75      0.71        20\n         104       0.93      0.95      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.93      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.94      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.64      0.87      0.65     10056\nweighted avg       0.96      0.95      0.95     10056\n']",1000,2048,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_3072","{'name': 'sequential_19', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_12'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_12', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 3072, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 3072)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_12', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_12', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9858623743057251,0.293657124042511,0.9805633425712585,0.25802138447761536,2608.0,563.5,340.0,648.0,4056.0,0.628960167028896,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.87      1.00      0.93        20\n           3       0.98      0.97      0.98       101\n           4       0.44      1.00      0.62         8\n           5       0.42      1.00      0.59         5\n           6       0.00      0.00      0.00         1\n           7       0.69      0.90      0.78        10\n           8       0.75      1.00      0.86         3\n           9       0.67      0.67      0.67         3\n          11       0.50      0.67      0.57         3\n          13       0.44      0.57      0.50         7\n          14       1.00      0.60      0.75         5\n          15       1.00      0.82      0.90        11\n          16       0.46      1.00      0.63        11\n          17       0.44      0.78      0.56        46\n          18       0.80      1.00      0.89        33\n          19       0.83      0.83      0.83         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.96      0.99      0.98       102\n          26       0.94      0.87      0.91        39\n          27       0.90      0.97      0.94        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.80      0.89      0.84         9\n          32       0.89      0.85      0.87        68\n          33       0.33      1.00      0.50         1\n          34       0.94      0.88      0.91        34\n          35       0.93      0.87      0.90        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.79      0.86      0.82        49\n          41       0.67      0.50      0.57        12\n          42       0.72      0.85      0.78        27\n          43       0.27      1.00      0.43         9\n          44       0.43      1.00      0.60         3\n          45       0.54      0.79      0.64        19\n          46       0.96      0.95      0.95       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       0.83      1.00      0.91         5\n          50       0.25      1.00      0.40         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.61      1.00      0.76        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.30      0.75      0.43         4\n          73       0.94      0.95      0.95       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.46      1.00      0.63         6\n          77       0.85      1.00      0.92        11\n          78       0.69      0.98      0.81        53\n          79       0.67      1.00      0.80         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          83       1.00      0.50      0.67         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.40      0.86      0.55         7\n          87       1.00      1.00      1.00        27\n          88       0.58      1.00      0.74         7\n          89       0.67      1.00      0.80         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.96      0.92        24\n          97       0.33      1.00      0.50         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.36      0.83      0.50         6\n         103       0.73      0.80      0.76        20\n         104       0.93      0.95      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.62      1.00      0.77         5\n         116       0.87      0.98      0.92       132\n         117       1.00      1.00      1.00         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.94      0.97      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.60      0.87      0.62     10056\nweighted avg       0.96      0.95      0.95     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.90      0.90      0.90        20\n           3       0.97      0.98      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.27      0.80      0.40         5\n           6       0.00      0.00      0.00         1\n           7       0.59      1.00      0.74        10\n           8       1.00      1.00      1.00         3\n           9       0.60      1.00      0.75         3\n          11       0.29      0.67      0.40         3\n          13       0.70      1.00      0.82         7\n          14       1.00      1.00      1.00         5\n          15       0.77      0.91      0.83        11\n          16       0.48      1.00      0.65        11\n          17       0.47      0.76      0.58        46\n          18       0.89      0.94      0.91        33\n          19       0.83      0.83      0.83         6\n          20       0.98      1.00      0.99        61\n          22       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.96      0.98      0.97       102\n          26       0.94      0.87      0.91        39\n          27       0.88      0.97      0.93        39\n          28       0.83      0.83      0.83        12\n          29       0.50      1.00      0.67         1\n          30       0.00      0.00      0.00         1\n          31       0.75      1.00      0.86         9\n          32       0.92      0.85      0.89        68\n          33       1.00      0.00      0.00         1\n          34       0.97      0.85      0.91        34\n          35       0.87      0.87      0.87        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.79      0.90      0.84        49\n          41       0.62      0.42      0.50        12\n          42       0.85      0.85      0.85        27\n          43       0.19      1.00      0.32         9\n          44       1.00      1.00      1.00         3\n          45       0.45      0.74      0.56        19\n          46       0.96      0.95      0.95       656\n          47       0.83      1.00      0.91         5\n          48       0.38      0.86      0.52         7\n          49       1.00      1.00      1.00         5\n          50       0.14      1.00      0.25         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.70      1.00      0.82        14\n          68       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      0.75      0.38         4\n          73       0.93      0.95      0.94       567\n          75       1.00      1.00      1.00        13\n          76       0.46      1.00      0.63         6\n          77       0.73      1.00      0.85        11\n          78       0.64      0.98      0.78        53\n          79       0.75      1.00      0.86         6\n          81       1.00      0.50      0.67         2\n          83       0.50      1.00      0.67         2\n          84       0.67      1.00      0.80         6\n          86       0.46      0.86      0.60         7\n          87       1.00      1.00      1.00        27\n          88       0.64      1.00      0.78         7\n          89       0.67      1.00      0.80         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.81      0.92      0.86        24\n          96       0.00      1.00      0.00         0\n          97       0.33      1.00      0.50         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.42      0.83      0.56         6\n         103       0.68      0.75      0.71        20\n         104       0.95      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         113       0.00      1.00      0.00         0\n         115       0.33      1.00      0.50         5\n         116       0.95      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.64      0.86      0.65     10056\nweighted avg       0.96      0.95      0.95     10056\n']",1000,3072,10,32,False,False,False,,post,accuracy,macro,True,False,5

"EMB_4096","{'name': 'sequential_23', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_15'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_15', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 4096, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_7', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 4096)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_15', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9881758689880371,0.29714587330818176,0.9821869134902954,0.26092684268951416,2618.0,506.0,330.0,583.0,3894.5,0.638761226966625,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.80      1.00      0.89        20\n           3       0.99      0.96      0.97       101\n           4       0.73      1.00      0.84         8\n           5       0.33      1.00      0.50         5\n           6       0.00      0.00      0.00         1\n           7       0.77      1.00      0.87        10\n           8       0.50      0.67      0.57         3\n           9       0.29      0.67      0.40         3\n          11       0.50      1.00      0.67         3\n          13       0.75      0.86      0.80         7\n          14       0.83      1.00      0.91         5\n          15       0.79      1.00      0.88        11\n          16       0.39      1.00      0.56        11\n          17       0.58      0.76      0.66        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.98      0.97      0.98       102\n          26       0.92      0.87      0.89        39\n          27       0.86      0.97      0.92        39\n          28       0.83      0.83      0.83        12\n          29       0.50      1.00      0.67         1\n          30       0.33      1.00      0.50         1\n          31       0.62      0.89      0.73         9\n          32       0.88      0.85      0.87        68\n          33       1.00      0.00      0.00         1\n          34       0.96      0.79      0.87        34\n          35       0.87      0.87      0.87        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.75      0.90      0.81        49\n          41       0.83      0.42      0.56        12\n          42       0.82      0.85      0.84        27\n          43       0.25      1.00      0.40         9\n          44       1.00      1.00      1.00         3\n          45       0.62      0.79      0.70        19\n          46       0.95      0.95      0.95       656\n          47       0.83      1.00      0.91         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.20      1.00      0.33         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.20      0.22         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.78      1.00      0.88        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.11      1.00      0.21         4\n          73       0.95      0.92      0.93       567\n          75       0.93      1.00      0.96        13\n          76       0.60      1.00      0.75         6\n          77       0.65      1.00      0.79        11\n          78       0.79      0.98      0.87        53\n          79       0.67      1.00      0.80         6\n          81       1.00      1.00      1.00         2\n          83       0.50      1.00      0.67         2\n          84       0.55      1.00      0.71         6\n          86       0.46      0.86      0.60         7\n          87       1.00      1.00      1.00        27\n          88       0.70      1.00      0.82         7\n          89       0.67      1.00      0.80         2\n          90       0.65      0.92      0.76        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.88      0.92      0.90        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.45      0.83      0.59         6\n         103       0.76      0.80      0.78        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.93      0.98      0.95       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.94      0.97      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.63      0.89      0.65     10056\nweighted avg       0.97      0.95      0.96     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.90      0.90      0.90        20\n           3       0.96      0.98      0.97       101\n           4       0.38      1.00      0.55         8\n           5       0.27      0.80      0.40         5\n           6       1.00      0.00      0.00         1\n           7       0.62      1.00      0.77        10\n           8       1.00      1.00      1.00         3\n           9       1.00      0.67      0.80         3\n          11       0.67      0.67      0.67         3\n          13       0.75      0.86      0.80         7\n          14       1.00      0.80      0.89         5\n          15       0.83      0.91      0.87        11\n          16       0.79      1.00      0.88        11\n          17       0.48      0.78      0.60        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.98      0.96      0.97       102\n          26       0.83      0.90      0.86        39\n          27       0.90      0.97      0.94        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.20      1.00      0.33         1\n          31       0.73      0.89      0.80         9\n          32       0.95      0.84      0.89        68\n          33       0.00      0.00      0.00         1\n          34       1.00      0.85      0.92        34\n          35       0.85      0.94      0.89        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.84      0.88      0.86        49\n          41       1.00      0.42      0.59        12\n          42       0.77      0.85      0.81        27\n          43       0.29      1.00      0.45         9\n          44       1.00      1.00      1.00         3\n          45       0.61      0.74      0.67        19\n          46       0.96      0.95      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.75      0.86      0.80         7\n          49       1.00      1.00      1.00         5\n          50       0.17      1.00      0.29         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.20      0.20         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.94      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.70      1.00      0.82        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.40      1.00      0.57         4\n          73       0.94      0.96      0.95       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.55      1.00      0.71         6\n          77       0.85      1.00      0.92        11\n          78       0.71      0.98      0.83        53\n          79       0.67      1.00      0.80         6\n          81       1.00      0.50      0.67         2\n          83       0.00      0.00      0.00         2\n          84       0.55      1.00      0.71         6\n          86       0.44      1.00      0.61         7\n          87       1.00      1.00      1.00        27\n          88       0.88      1.00      0.93         7\n          89       0.50      1.00      0.67         2\n          90       0.85      0.92      0.88        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.88      0.96      0.92        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.21      0.83      0.33         6\n         103       0.77      0.85      0.81        20\n         104       0.95      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.97      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.93      0.98      0.95       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.65      0.87      0.65     10056\nweighted avg       0.97      0.95      0.96     10056\n']",1000,4096,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
6144,"{'name': 'sequential_2', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_1'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_1', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 6144, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 6144)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_1', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9885489344596863,0.2928241789340973,0.9828898906707764,0.26315006613731384,2628.0,483.5,320.0,556.0,3695.0,0.6668514813416794,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.90      0.90      0.90        20\n           3       0.97      0.98      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.38      1.00      0.56         5\n           6       0.00      0.00      0.00         1\n           7       0.83      1.00      0.91        10\n           8       0.75      1.00      0.86         3\n           9       0.75      1.00      0.86         3\n          11       0.25      0.67      0.36         3\n          13       0.56      0.71      0.62         7\n          14       0.80      0.80      0.80         5\n          15       0.91      0.91      0.91        11\n          16       0.46      1.00      0.63        11\n          17       0.55      0.76      0.64        46\n          18       0.89      0.94      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.99      0.98      0.99       102\n          26       0.94      0.85      0.89        39\n          27       0.93      0.97      0.95        39\n          28       0.92      0.92      0.92        12\n          29       0.50      1.00      0.67         1\n          30       0.25      1.00      0.40         1\n          31       0.78      0.78      0.78         9\n          32       0.94      0.87      0.90        68\n          33       0.50      1.00      0.67         1\n          34       1.00      0.85      0.92        34\n          35       0.90      0.87      0.89        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.74      0.94      0.83        49\n          41       1.00      0.42      0.59        12\n          42       0.74      0.85      0.79        27\n          43       0.22      1.00      0.36         9\n          44       1.00      1.00      1.00         3\n          45       0.71      0.79      0.75        19\n          46       0.96      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.86      0.86      0.86         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.86      0.86      0.86         7\n          52       0.33      0.20      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.74      1.00      0.85        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.13      1.00      0.23         4\n          73       0.95      0.92      0.94       567\n          75       0.93      1.00      0.96        13\n          76       0.55      1.00      0.71         6\n          77       0.38      1.00      0.55        11\n          78       0.72      0.98      0.83        53\n          79       0.75      1.00      0.86         6\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.50      1.00      0.67         2\n          84       0.86      1.00      0.92         6\n          86       0.43      0.86      0.57         7\n          87       1.00      1.00      1.00        27\n          88       0.88      1.00      0.93         7\n          89       0.67      1.00      0.80         2\n          90       1.00      0.92      0.96        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.76      0.92      0.83        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.36      0.83      0.50         6\n         103       0.81      0.85      0.83        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.62      1.00      0.77         5\n         116       0.95      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.80      1.00      0.89         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.67      0.89      0.68     10056\nweighted avg       0.97      0.95      0.96     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.98      0.98      0.98       101\n           4       0.67      1.00      0.80         8\n           5       0.36      1.00      0.53         5\n           6       1.00      0.00      0.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      1.00      1.00         3\n           9       1.00      0.67      0.80         3\n          11       0.50      0.67      0.57         3\n          13       0.88      1.00      0.93         7\n          14       1.00      0.60      0.75         5\n          15       0.58      1.00      0.73        11\n          16       0.41      1.00      0.58        11\n          17       0.65      0.78      0.71        46\n          18       0.97      0.88      0.92        33\n          19       0.83      0.83      0.83         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.99      0.98      0.99       102\n          26       0.95      0.90      0.92        39\n          27       0.81      0.97      0.88        39\n          28       0.92      0.92      0.92        12\n          29       0.50      1.00      0.67         1\n          30       0.00      0.00      0.00         1\n          31       0.73      0.89      0.80         9\n          32       0.90      0.88      0.89        68\n          33       0.00      0.00      0.00         1\n          34       0.96      0.79      0.87        34\n          35       0.90      0.90      0.90        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       0.92      1.00      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.85      0.94      0.89        49\n          41       1.00      0.50      0.67        12\n          42       0.85      0.85      0.85        27\n          43       0.20      1.00      0.34         9\n          44       1.00      1.00      1.00         3\n          45       0.62      0.79      0.70        19\n          46       0.95      0.95      0.95       656\n          47       0.83      1.00      0.91         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.20      0.20         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.78      1.00      0.88        14\n          68       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.24      1.00      0.38         4\n          73       0.94      0.94      0.94       567\n          75       1.00      1.00      1.00        13\n          76       0.71      0.83      0.77         6\n          77       0.92      1.00      0.96        11\n          78       0.71      0.98      0.83        53\n          79       0.55      1.00      0.71         6\n          80       0.00      1.00      0.00         0\n          81       1.00      1.00      1.00         2\n          83       1.00      0.50      0.67         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.54      1.00      0.70         7\n          87       0.96      1.00      0.98        27\n          88       0.78      1.00      0.88         7\n          89       0.67      1.00      0.80         2\n          90       1.00      0.92      0.96        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.88      0.88      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.38      0.83      0.53         6\n         103       0.72      0.90      0.80        20\n         104       0.94      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.92      0.98      0.95        58\n         113       0.00      1.00      0.00         0\n         115       0.83      1.00      0.91         5\n         116       0.94      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.67      0.87      0.67     10056\nweighted avg       0.97      0.96      0.96     10056\n']",1000,6144,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
8192,"{'name': 'sequential_4', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_2'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_2', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 8192, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 8192)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_2', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9903245568275452,0.30269837379455566,0.9840774536132812,0.2664452791213989,2627.0,440.0,321.0,501.5,3450.0,0.6726483341380913,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.87      1.00      0.93        20\n           3       0.99      0.97      0.98       101\n           4       0.89      1.00      0.94         8\n           5       0.36      1.00      0.53         5\n           6       0.00      0.00      0.00         1\n           7       0.77      1.00      0.87        10\n           8       0.75      1.00      0.86         3\n           9       0.67      0.67      0.67         3\n          11       0.50      1.00      0.67         3\n          13       1.00      0.86      0.92         7\n          14       0.83      1.00      0.91         5\n          15       0.62      0.91      0.74        11\n          16       0.61      1.00      0.76        11\n          17       0.62      0.78      0.69        46\n          18       0.84      0.97      0.90        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.99      0.98      0.99       102\n          26       0.94      0.85      0.89        39\n          27       0.90      0.97      0.94        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.69      1.00      0.82         9\n          32       0.94      0.87      0.90        68\n          33       1.00      1.00      1.00         1\n          34       1.00      0.85      0.92        34\n          35       0.90      0.87      0.89        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.86      0.90      0.88        49\n          41       1.00      0.42      0.59        12\n          42       0.83      0.93      0.88        27\n          43       0.27      1.00      0.43         9\n          44       0.38      1.00      0.55         3\n          45       0.70      0.84      0.76        19\n          46       0.97      0.95      0.96       656\n          47       1.00      0.80      0.89         5\n          48       1.00      0.86      0.92         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.12      0.20      0.15         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.82      1.00      0.90        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.33      0.75      0.46         4\n          73       0.95      0.96      0.96       567\n          75       0.93      1.00      0.96        13\n          76       0.50      1.00      0.67         6\n          77       0.79      1.00      0.88        11\n          78       0.76      0.98      0.86        53\n          79       0.86      1.00      0.92         6\n          81       1.00      1.00      1.00         2\n          83       1.00      0.00      0.00         2\n          84       0.50      1.00      0.67         6\n          86       0.18      0.86      0.30         7\n          87       1.00      1.00      1.00        27\n          88       0.88      1.00      0.93         7\n          89       1.00      1.00      1.00         2\n          90       0.92      0.92      0.92        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       1.00      0.96      0.98        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.29      0.83      0.43         6\n         103       0.82      0.90      0.86        20\n         104       0.96      0.98      0.97       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.98      0.98      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.62      1.00      0.76         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.69      0.88      0.70     10056\nweighted avg       0.97      0.96      0.96     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.83      1.00      0.91        20\n           3       0.99      0.97      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.36      1.00      0.53         5\n           6       1.00      0.00      0.00         1\n           7       0.83      1.00      0.91        10\n           8       1.00      1.00      1.00         3\n           9       1.00      1.00      1.00         3\n          11       1.00      0.67      0.80         3\n          13       0.86      0.86      0.86         7\n          14       1.00      0.60      0.75         5\n          15       0.77      0.91      0.83        11\n          16       0.56      0.91      0.69        11\n          17       0.48      0.76      0.59        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          24       0.73      0.92      0.81        12\n          25       0.99      0.98      0.99       102\n          26       0.95      0.90      0.92        39\n          27       0.93      0.97      0.95        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.75      1.00      0.86         9\n          32       0.92      0.84      0.88        68\n          33       0.25      1.00      0.40         1\n          34       0.88      0.82      0.85        34\n          35       0.77      0.87      0.82        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.90      0.91        49\n          41       0.83      0.42      0.56        12\n          42       0.88      0.85      0.87        27\n          43       0.23      1.00      0.37         9\n          44       1.00      1.00      1.00         3\n          45       0.52      0.74      0.61        19\n          46       0.96      0.94      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.75      0.86      0.80         7\n          49       0.83      1.00      0.91         5\n          50       0.17      1.00      0.29         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.67      1.00      0.80        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.09      1.00      0.16         4\n          73       0.96      0.90      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.46      1.00      0.63         6\n          77       0.92      1.00      0.96        11\n          78       0.81      0.98      0.89        53\n          79       0.86      1.00      0.92         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.50      0.67         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.56      0.83      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.88      1.00      0.93         7\n          89       0.67      1.00      0.80         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      0.50      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.92      0.90        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.36      0.83      0.50         6\n         103       0.75      0.75      0.75        20\n         104       0.93      0.97      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.97      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.89      1.00      0.94         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.67      0.88      0.65     10056\nweighted avg       0.97      0.95      0.96     10056\n']",1000,8192,10,32,False,False,False,,post,accuracy,macro,True,False,5
