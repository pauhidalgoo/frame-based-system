architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_100_1b_2h_4ff,"{'name': 'sequential_22', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_13'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_1', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 100, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_1', 'embed_dim': 100, 'num_heads': 2, 'ff_dim': 400, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 100)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_10', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_22', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 100)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 100)}}], 'build_input_shape': (None, 46)}",0.9644808173179626,0.27084487676620483,0.9567155838012695,0.22923146188259125,1979.0,1203.0,969.0,934.0,7946.0,0.5481270005739756,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       1.00      1.00      1.00         8\n           5       0.29      0.80      0.42         5\n           6       1.00      1.00      1.00         1\n           7       0.54      0.70      0.61        10\n           8       0.75      1.00      0.86         3\n           9       0.75      1.00      0.86         3\n          11       1.00      0.33      0.50         3\n          13       0.56      0.71      0.62         7\n          14       1.00      0.20      0.33         5\n          15       0.73      0.73      0.73        11\n          16       0.67      0.91      0.77        11\n          17       0.88      0.65      0.75        46\n          18       0.78      0.85      0.81        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.73      0.92      0.81        12\n          25       0.96      0.94      0.95       102\n          26       0.93      0.64      0.76        39\n          27       0.90      0.95      0.93        39\n          28       0.62      0.83      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.69      1.00      0.82         9\n          32       0.91      0.74      0.81        68\n          33       0.00      0.00      0.00         1\n          34       0.80      0.71      0.75        34\n          35       0.75      0.77      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.85      0.92      0.88        49\n          41       1.00      0.42      0.59        12\n          42       0.85      0.85      0.85        27\n          43       0.04      0.78      0.07         9\n          44       1.00      0.33      0.50         3\n          45       0.20      0.68      0.31        19\n          46       0.75      0.69      0.72       656\n          47       0.56      1.00      0.71         5\n          48       0.36      0.71      0.48         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.36      0.86      0.51        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.08      0.75      0.15         4\n          73       0.71      0.59      0.64       567\n          75       1.00      0.77      0.87        13\n          76       0.12      1.00      0.21         6\n          77       0.83      0.45      0.59        11\n          78       0.58      0.98      0.73        53\n          79       0.50      0.83      0.62         6\n          81       0.50      0.50      0.50         2\n          83       0.00      0.00      0.00         2\n          84       0.56      0.83      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.17      0.86      0.28         7\n          87       0.96      0.89      0.92        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.18         2\n          90       0.56      0.75      0.64        12\n          91       0.00      1.00      0.00         0\n          93       0.17      0.50      0.25         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.83      0.80        24\n          96       0.00      1.00      0.00         0\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.03      0.50      0.05         6\n         103       0.61      0.70      0.65        20\n         104       0.67      0.81      0.73       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.44      0.80      0.57         5\n         116       0.81      0.69      0.75       132\n         117       1.00      1.00      1.00         2\n         118       0.50      0.88      0.64         8\n         119       0.99      0.89      0.94      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.58      0.78      0.57     10056\nweighted avg       0.93      0.87      0.89     10056\n'],1000,100,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_2h_4ff,"{'name': 'sequential_28', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_17'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_2', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_2', 'embed_dim': 200, 'num_heads': 2, 'ff_dim': 800, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_11', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_31', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9706537127494812,0.2790900468826294,0.9639135003089905,0.23358860611915588,2127.0,995.0,821.0,751.0,7110.0,0.5386165045813013,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       1.00      0.88      0.93         8\n           5       0.33      1.00      0.50         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       0.75      1.00      0.86         3\n           9       0.75      1.00      0.86         3\n          11       1.00      0.33      0.50         3\n          13       0.60      0.86      0.71         7\n          14       1.00      0.20      0.33         5\n          15       0.69      0.82      0.75        11\n          16       0.73      1.00      0.85        11\n          17       0.76      0.76      0.76        46\n          18       0.79      0.94      0.86        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.73      0.92      0.81        12\n          25       0.98      0.94      0.96       102\n          26       0.96      0.67      0.79        39\n          27       0.90      0.95      0.93        39\n          28       0.59      0.83      0.69        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.64      1.00      0.78         9\n          32       0.96      0.72      0.82        68\n          33       0.00      0.00      0.00         1\n          34       0.84      0.76      0.80        34\n          35       0.77      0.87      0.82        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.90      0.90        49\n          41       0.83      0.42      0.56        12\n          42       0.88      0.85      0.87        27\n          43       0.06      1.00      0.12         9\n          44       0.50      0.33      0.40         3\n          45       0.44      0.63      0.52        19\n          46       0.83      0.68      0.75       656\n          47       1.00      0.80      0.89         5\n          48       0.27      0.57      0.36         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.20      0.20         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.19      0.86      0.32        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.20      0.75      0.32         4\n          73       0.72      0.74      0.73       567\n          75       0.93      1.00      0.96        13\n          76       0.16      1.00      0.27         6\n          77       0.55      0.55      0.55        11\n          78       0.58      0.98      0.73        53\n          79       0.46      1.00      0.63         6\n          81       1.00      0.00      0.00         2\n          83       1.00      0.00      0.00         2\n          84       0.55      1.00      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.50      0.71      0.59         7\n          87       0.96      0.81      0.88        27\n          88       0.33      1.00      0.50         7\n          89       0.11      1.00      0.19         2\n          90       0.62      0.83      0.71        12\n          92       0.00      1.00      0.00         0\n          93       0.17      0.50      0.25         2\n          94       0.00      1.00      0.00         0\n          95       0.75      0.88      0.81        24\n          96       0.00      1.00      0.00         0\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.05      0.50      0.10         6\n         103       0.75      0.60      0.67        20\n         104       0.66      0.85      0.74       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.28      1.00      0.43         5\n         116       0.85      0.61      0.71       132\n         117       1.00      1.00      1.00         2\n         118       0.44      0.50      0.47         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.61      0.78      0.59     10056\nweighted avg       0.94      0.90      0.91     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_4h_4ff,"{'name': 'sequential_34', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_21'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_3', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_3', 'embed_dim': 200, 'num_heads': 4, 'ff_dim': 800, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_12', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_40', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9725995659828186,0.2811715602874756,0.9646406173706055,0.23544465005397797,2157.0,1004.0,791.0,802.0,7234.0,0.5658953911384762,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       0.80      1.00      0.89         8\n           5       0.44      0.80      0.57         5\n           6       1.00      0.00      0.00         1\n           7       0.64      0.90      0.75        10\n           8       1.00      1.00      1.00         3\n           9       0.75      1.00      0.86         3\n          11       1.00      0.33      0.50         3\n          13       0.78      1.00      0.88         7\n          14       1.00      0.20      0.33         5\n          15       0.77      0.91      0.83        11\n          16       0.73      1.00      0.85        11\n          17       0.70      0.83      0.76        46\n          18       0.79      0.94      0.86        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.98      0.95      0.97       102\n          26       0.93      0.64      0.76        39\n          27       0.90      0.97      0.94        39\n          28       0.67      0.83      0.74        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.57      0.89      0.70         9\n          32       0.93      0.78      0.85        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.79      0.84        34\n          35       0.74      0.84      0.79        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.76      0.90      0.82        49\n          41       1.00      0.42      0.59        12\n          42       0.88      0.85      0.87        27\n          43       0.05      1.00      0.10         9\n          44       1.00      0.33      0.50         3\n          45       0.50      0.58      0.54        19\n          46       0.81      0.71      0.76       656\n          47       1.00      0.60      0.75         5\n          48       0.33      0.71      0.45         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.20      0.22         5\n          54       0.83      0.83      0.83         6\n          56       1.00      0.83      0.91         6\n          62       0.90      0.98      0.94        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.34      0.79      0.48        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.30      0.75      0.43         4\n          73       0.73      0.77      0.75       567\n          75       0.93      1.00      0.96        13\n          76       0.33      1.00      0.50         6\n          77       0.62      0.45      0.53        11\n          78       0.58      0.98      0.73        53\n          79       0.45      0.83      0.59         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.45      0.83      0.59         6\n          85       0.00      1.00      0.00         0\n          86       0.24      0.71      0.36         7\n          87       0.96      0.93      0.94        27\n          88       0.32      1.00      0.48         7\n          89       0.11      1.00      0.19         2\n          90       0.59      0.83      0.69        12\n          91       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          95       0.70      0.88      0.78        24\n          96       0.00      1.00      0.00         0\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.11      0.50      0.18         6\n         103       0.79      0.55      0.65        20\n         104       0.74      0.76      0.75       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.80      0.80      0.80       132\n         117       1.00      1.00      1.00         2\n         118       0.46      0.75      0.57         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.62      0.77      0.60     10056\nweighted avg       0.94      0.90      0.91     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_4h_4ff,"{'name': 'sequential_2', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block', 'embed_dim': 200, 'num_heads': 1, 'ff_dim': 800, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9695020914077759,0.2771340012550354,0.9645920991897583,0.23203861713409424,2074.0,953.0,874.0,728.0,7323.0,0.5376172233388072,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      0.90      0.92        20\n           3       0.97      0.99      0.98       101\n           4       1.00      0.88      0.93         8\n           5       0.40      0.40      0.40         5\n           6       1.00      0.00      0.00         1\n           7       0.75      0.60      0.67        10\n           8       0.60      1.00      0.75         3\n           9       1.00      1.00      1.00         3\n          11       1.00      0.33      0.50         3\n          13       0.78      1.00      0.88         7\n          14       0.33      0.20      0.25         5\n          15       0.71      0.91      0.80        11\n          16       0.77      0.91      0.83        11\n          17       0.82      0.80      0.81        46\n          18       0.79      0.91      0.85        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.85      0.92      0.88        12\n          25       0.95      0.98      0.97       102\n          26       0.96      0.69      0.81        39\n          27       0.93      0.95      0.94        39\n          28       0.67      0.83      0.74        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.98      0.75      0.85        68\n          33       0.00      0.00      0.00         1\n          34       0.89      0.74      0.81        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.86      0.90      0.88        49\n          41       1.00      0.42      0.59        12\n          42       0.88      0.85      0.87        27\n          43       0.05      0.56      0.10         9\n          44       0.50      0.33      0.40         3\n          45       0.47      0.74      0.57        19\n          46       0.80      0.64      0.71       656\n          47       0.42      1.00      0.59         5\n          48       0.22      0.57      0.32         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.20      0.20         5\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.32      0.86      0.46        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.17      1.00      0.29         4\n          73       0.67      0.73      0.70       567\n          75       1.00      0.69      0.82        13\n          76       0.21      1.00      0.34         6\n          77       0.54      0.64      0.58        11\n          78       0.59      0.98      0.74        53\n          79       0.71      0.83      0.77         6\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.50      1.00      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.17      0.71      0.28         7\n          87       0.96      0.96      0.96        27\n          88       0.35      1.00      0.52         7\n          89       0.10      1.00      0.18         2\n          90       0.67      0.83      0.74        12\n          93       0.25      1.00      0.40         2\n          94       0.00      1.00      0.00         0\n          95       0.66      0.79      0.72        24\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.04      0.50      0.07         6\n         103       0.68      0.85      0.76        20\n         104       0.78      0.65      0.71       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.67      0.80      0.73         5\n         116       0.75      0.88      0.81       132\n         117       1.00      1.00      1.00         2\n         118       0.43      0.38      0.40         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.59      0.74      0.57     10056\nweighted avg       0.93      0.90      0.91     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_8h_4ff,"{'name': 'sequential_12', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_6'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_2', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_2', 'embed_dim': 200, 'num_heads': 8, 'ff_dim': 800, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_2', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9719702005386353,0.2820458710193634,0.9642770886421204,0.23504190146923065,2111.0,977.0,837.0,738.0,7110.0,0.546859616967146,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       1.00      0.88      0.93         8\n           5       0.50      0.80      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.53      0.80      0.64        10\n           8       0.75      1.00      0.86         3\n           9       0.75      1.00      0.86         3\n          11       1.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.58      1.00      0.74         7\n          14       1.00      0.20      0.33         5\n          15       0.61      1.00      0.76        11\n          16       0.69      1.00      0.81        11\n          17       0.66      0.80      0.73        46\n          18       0.82      0.97      0.89        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.92      0.92      0.92        12\n          25       0.96      0.93      0.95       102\n          26       0.94      0.82      0.88        39\n          27       0.93      0.95      0.94        39\n          28       0.50      0.92      0.65        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.96      0.65      0.77        68\n          33       0.00      0.00      0.00         1\n          34       0.89      0.71      0.79        34\n          35       0.77      0.77      0.77        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.88      0.89        49\n          41       1.00      0.42      0.59        12\n          42       0.85      0.85      0.85        27\n          43       0.11      0.78      0.19         9\n          44       0.33      0.33      0.33         3\n          45       0.43      0.53      0.48        19\n          46       0.78      0.67      0.72       656\n          47       0.80      0.80      0.80         5\n          48       0.15      0.29      0.20         7\n          49       1.00      1.00      1.00         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.50      0.20      0.29         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.25      0.93      0.40        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.19      0.75      0.30         4\n          73       0.69      0.71      0.70       567\n          75       1.00      0.92      0.96        13\n          76       0.27      1.00      0.43         6\n          77       0.26      1.00      0.42        11\n          78       0.58      0.98      0.73        53\n          79       0.83      0.83      0.83         6\n          81       0.00      0.00      0.00         2\n          83       1.00      0.00      0.00         2\n          84       0.40      1.00      0.57         6\n          85       0.00      1.00      0.00         0\n          86       0.17      0.71      0.27         7\n          87       0.96      1.00      0.98        27\n          88       0.32      0.86      0.46         7\n          89       0.10      1.00      0.18         2\n          90       1.00      0.83      0.91        12\n          93       0.18      1.00      0.31         2\n          95       0.73      0.79      0.76        24\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.05      0.50      0.09         6\n         103       0.64      0.70      0.67        20\n         104       0.73      0.68      0.70       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.76      0.83      0.79       132\n         117       1.00      1.00      1.00         2\n         118       1.00      0.38      0.55         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.62      0.77      0.58     10056\nweighted avg       0.93      0.89      0.91     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_8h_2ff,"{'name': 'sequential_18', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_10'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_3', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_3', 'embed_dim': 200, 'num_heads': 8, 'ff_dim': 400, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_27', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9755368828773499,0.28714337944984436,0.9665794372558594,0.23895052075386047,2222.0,917.0,726.0,730.0,6852.0,0.5641243780053643,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       1.00      0.88      0.93         8\n           5       0.38      1.00      0.56         5\n           6       1.00      0.00      0.00         1\n           7       0.64      0.90      0.75        10\n           8       0.75      1.00      0.86         3\n           9       1.00      1.00      1.00         3\n          11       1.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.71      0.71      0.71         7\n          14       0.50      0.20      0.29         5\n          15       0.61      1.00      0.76        11\n          16       0.58      1.00      0.73        11\n          17       0.90      0.80      0.85        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.65      0.92      0.76        12\n          25       0.98      0.96      0.97       102\n          26       0.97      0.85      0.90        39\n          27       0.90      0.97      0.94        39\n          28       0.71      0.83      0.77        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.95      0.82      0.88        68\n          33       0.00      0.00      0.00         1\n          34       0.92      0.68      0.78        34\n          35       0.83      0.77      0.80        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.88      0.89        49\n          41       1.00      0.42      0.59        12\n          42       0.96      0.85      0.90        27\n          43       0.05      0.89      0.10         9\n          44       0.50      0.33      0.40         3\n          45       0.38      0.68      0.49        19\n          46       0.77      0.80      0.78       656\n          47       0.83      1.00      0.91         5\n          48       0.45      0.71      0.56         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.27      0.93      0.41        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.44      1.00      0.62         4\n          73       0.79      0.68      0.73       567\n          75       1.00      0.92      0.96        13\n          76       0.67      1.00      0.80         6\n          77       0.64      0.82      0.72        11\n          78       0.58      0.98      0.73        53\n          79       0.43      1.00      0.60         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.42      0.83      0.56         6\n          85       0.00      1.00      0.00         0\n          86       0.23      0.86      0.36         7\n          87       1.00      1.00      1.00        27\n          88       0.35      1.00      0.52         7\n          89       0.10      1.00      0.18         2\n          90       0.83      0.83      0.83        12\n          93       0.50      1.00      0.67         2\n          95       0.69      0.83      0.75        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.07      0.50      0.12         6\n         103       0.80      0.60      0.69        20\n         104       0.70      0.84      0.76       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.84      0.73      0.78       132\n         117       1.00      1.00      1.00         2\n         118       0.71      0.62      0.67         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.63      0.80      0.61     10056\nweighted avg       0.94      0.90      0.92     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_200_1b_4h_2ff,"{'name': 'sequential_24', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_14'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_4', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 200, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_4', 'embed_dim': 200, 'num_heads': 4, 'ff_dim': 400, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 200)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_4', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_36', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 200)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 200)}}], 'build_input_shape': (None, 46)}",0.9717305302619934,0.27937737107276917,0.9560855031013489,0.23341906070709229,2107.0,1168.0,841.0,1074.0,7684.0,0.5249276011800919,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      0.95      0.95        20\n           3       0.98      0.99      0.99       101\n           4       1.00      1.00      1.00         8\n           5       0.33      1.00      0.50         5\n           6       1.00      0.00      0.00         1\n           7       0.67      1.00      0.80        10\n           8       0.60      1.00      0.75         3\n           9       1.00      1.00      1.00         3\n          11       0.50      0.33      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.70      1.00      0.82         7\n          14       0.50      0.20      0.29         5\n          15       0.77      0.91      0.83        11\n          16       0.50      1.00      0.67        11\n          17       0.82      0.78      0.80        46\n          18       0.81      0.91      0.86        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.99      0.95      0.97       102\n          26       0.94      0.77      0.85        39\n          27       0.90      0.97      0.94        39\n          28       0.38      0.83      0.53        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.87      0.79      0.83        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.76      0.83        34\n          35       0.93      0.84      0.88        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.74      0.88      0.80        49\n          41       1.00      0.42      0.59        12\n          42       0.86      0.89      0.87        27\n          43       0.04      0.89      0.08         9\n          44       1.00      0.33      0.50         3\n          45       0.48      0.58      0.52        19\n          46       0.78      0.73      0.75       656\n          47       0.83      1.00      0.91         5\n          48       0.36      0.71      0.48         7\n          49       1.00      1.00      1.00         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.52      0.79      0.63        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.20      1.00      0.33         4\n          73       0.72      0.72      0.72       567\n          75       1.00      0.92      0.96        13\n          76       0.26      1.00      0.41         6\n          77       0.67      0.36      0.47        11\n          78       0.58      0.98      0.73        53\n          79       0.33      1.00      0.50         6\n          81       1.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.55      1.00      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.10      0.86      0.18         7\n          87       0.96      0.96      0.96        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.18         2\n          90       0.73      0.67      0.70        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.25      0.50      0.33         2\n          94       0.00      1.00      0.00         0\n          95       0.80      0.83      0.82        24\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.02      0.50      0.03         6\n         103       0.85      0.55      0.67        20\n         104       0.70      0.80      0.75       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.82      0.73      0.78       132\n         117       1.00      1.00      1.00         2\n         118       0.47      0.88      0.61         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.58      0.78      0.55     10056\nweighted avg       0.93      0.87      0.90     10056\n'],1000,200,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_512_1b_4h_2ff,"{'name': 'sequential_34', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_18'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_6', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 512, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_6', 'embed_dim': 512, 'num_heads': 4, 'ff_dim': 1024, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 512)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_6', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_50', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 512)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 512)}}], 'build_input_shape': (None, 46)}",0.8250316977500916,0.20864008367061615,0.17941448092460632,0.15863001346588135,1232.0,2025.0,1716.0,3293.0,9600.0,0.38282921083347055,['              precision    recall  f1-score   support\n\n           0       0.93      0.02      0.03      1702\n           1       0.40      1.00      0.57         2\n           2       0.68      0.95      0.79        20\n           3       0.99      0.93      0.96       101\n           4       0.64      0.88      0.74         8\n           5       0.33      0.20      0.25         5\n           6       1.00      0.00      0.00         1\n           7       1.00      0.00      0.00        10\n           8       1.00      0.00      0.00         3\n           9       1.00      0.00      0.00         3\n          10       0.00      1.00      0.00         0\n          11       1.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       1.00      0.14      0.25         7\n          14       1.00      0.00      0.00         5\n          15       0.80      0.36      0.50        11\n          16       0.42      0.91      0.57        11\n          17       0.71      0.11      0.19        46\n          18       0.96      0.79      0.87        33\n          19       0.80      0.67      0.73         6\n          20       1.00      1.00      1.00        61\n          24       0.89      0.67      0.76        12\n          25       0.77      0.52      0.62       102\n          26       0.77      0.44      0.56        39\n          27       0.79      0.79      0.79        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       0.86      0.63      0.73        68\n          33       0.00      0.00      0.00         1\n          34       0.81      0.62      0.70        34\n          35       0.73      0.52      0.60        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       0.77      0.83      0.80        12\n          39       1.00      0.71      0.83         7\n          40       0.82      0.92      0.87        49\n          41       1.00      0.33      0.50        12\n          42       1.00      0.81      0.90        27\n          43       0.04      0.89      0.08         9\n          44       0.00      0.00      0.00         3\n          45       0.33      0.26      0.29        19\n          46       0.61      0.74      0.67       656\n          47       0.03      0.60      0.06         5\n          48       0.20      0.86      0.32         7\n          49       1.00      0.60      0.75         5\n          50       0.00      1.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.11      0.20      0.14         5\n          54       0.83      0.83      0.83         6\n          56       1.00      0.67      0.80         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.12      0.43      0.19        14\n          71       0.20      0.50      0.29         2\n          72       0.17      1.00      0.30         4\n          73       0.64      0.33      0.43       567\n          75       0.05      0.77      0.09        13\n          76       0.29      0.33      0.31         6\n          77       0.57      0.73      0.64        11\n          78       0.59      0.92      0.72        53\n          79       0.00      0.00      0.00         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       1.00      0.67      0.80         6\n          85       0.00      1.00      0.00         0\n          86       0.00      0.00      0.00         7\n          87       1.00      0.89      0.94        27\n          88       0.58      1.00      0.74         7\n          89       0.25      0.50      0.33         2\n          90       0.12      0.58      0.20        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      0.50      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.64      0.88      0.74        24\n          96       0.00      1.00      0.00         0\n          97       0.33      1.00      0.50         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.00      0.00      0.00         6\n         103       1.00      0.00      0.00        20\n         104       0.58      0.69      0.63       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.88      0.91      0.90        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.03      1.00      0.06         5\n         116       0.68      0.48      0.56       132\n         117       0.00      0.00      0.00         2\n         118       0.24      0.62      0.34         8\n         119       0.97      0.60      0.74      5640\n\n    accuracy                           0.50     10056\n   macro avg       0.53      0.59      0.38     10056\nweighted avg       0.89      0.50      0.59     10056\n'],1000,512,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_512_1b_1h_2ff,"{'name': 'sequential_40', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_22'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_7', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 512, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_7', 'embed_dim': 512, 'num_heads': 1, 'ff_dim': 1024, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 512)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_7', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_59', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 512)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 512)}}], 'build_input_shape': (None, 46)}",0.9342919588088989,0.24855375289916992,0.9346855878829956,0.20828761160373688,1819.0,1595.0,1129.0,1688.0,8231.0,0.4211220744727376,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.78      0.90      0.84        20\n           3       0.97      0.98      0.98       101\n           4       0.44      1.00      0.62         8\n           5       0.27      0.60      0.38         5\n           6       1.00      0.00      0.00         1\n           7       0.33      1.00      0.50        10\n           8       0.23      1.00      0.38         3\n           9       0.27      1.00      0.43         3\n          10       0.00      1.00      0.00         0\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.32      0.86      0.46         7\n          14       0.00      0.00      0.00         5\n          15       0.25      0.73      0.37        11\n          16       0.48      0.91      0.62        11\n          17       0.51      0.80      0.63        46\n          18       0.91      0.97      0.94        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       1.00      0.73      0.84       102\n          26       0.93      0.64      0.76        39\n          27       0.92      0.85      0.88        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.54      0.78      0.64         9\n          32       0.71      0.66      0.69        68\n          33       0.00      0.00      0.00         1\n          34       0.59      0.56      0.58        34\n          35       0.90      0.61      0.73        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      0.92      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.73      0.92      0.81        49\n          41       0.83      0.42      0.56        12\n          42       0.68      0.85      0.75        27\n          43       0.04      0.78      0.07         9\n          44       1.00      0.33      0.50         3\n          45       0.50      0.37      0.42        19\n          46       0.78      0.62      0.69       656\n          47       1.00      0.00      0.00         5\n          48       0.10      0.86      0.18         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       1.00      0.00      0.00         3\n          64       0.00      0.00      0.00         1\n          66       0.16      1.00      0.28        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.27      0.75      0.40         4\n          73       0.69      0.69      0.69       567\n          74       0.00      1.00      0.00         0\n          75       0.90      0.69      0.78        13\n          76       0.20      1.00      0.33         6\n          77       0.33      0.45      0.38        11\n          78       0.59      0.98      0.74        53\n          79       0.33      0.83      0.48         6\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.22      0.67      0.33         6\n          85       0.00      1.00      0.00         0\n          86       0.08      0.71      0.14         7\n          87       0.96      1.00      0.98        27\n          88       0.37      1.00      0.54         7\n          89       0.08      1.00      0.15         2\n          90       0.53      0.75      0.62        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.12      0.50      0.20         2\n          94       0.00      1.00      0.00         0\n          95       0.76      0.67      0.71        24\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.05      0.50      0.09         2\n         102       0.01      0.67      0.03         6\n         103       1.00      0.25      0.40        20\n         104       0.74      0.53      0.62       110\n         105       0.20      0.50      0.29         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.28      1.00      0.43         5\n         116       0.70      0.80      0.75       132\n         117       0.67      1.00      0.80         2\n         118       0.16      1.00      0.27         8\n         119       0.99      0.79      0.88      5640\n\n    accuracy                           0.81     10056\n   macro avg       0.47      0.74      0.43     10056\nweighted avg       0.92      0.81      0.85     10056\n'],1000,512,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_256_1b_1h_2ff,"{'name': 'sequential_46', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_26'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_8', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 256, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_8', 'embed_dim': 256, 'num_heads': 1, 'ff_dim': 512, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 256)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_8', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_68', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9716290235519409,0.2834447920322418,0.9625564813613892,0.23843823373317719,2141.0,1029.0,807.0,880.0,7440.0,0.5546329987034649,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       0.67      1.00      0.80         8\n           5       0.50      1.00      0.67         5\n           6       1.00      1.00      1.00         1\n           7       0.62      0.50      0.56        10\n           8       1.00      1.00      1.00         3\n           9       1.00      1.00      1.00         3\n          11       0.50      0.33      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.70      1.00      0.82         7\n          14       1.00      0.20      0.33         5\n          15       0.79      1.00      0.88        11\n          16       0.73      1.00      0.85        11\n          17       0.76      0.85      0.80        46\n          18       0.83      0.91      0.87        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.94      0.97      0.96       102\n          26       0.97      0.74      0.84        39\n          27       0.90      0.97      0.94        39\n          28       0.50      0.83      0.62        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.92      0.68      0.78        68\n          33       0.00      0.00      0.00         1\n          34       0.93      0.79      0.86        34\n          35       0.77      0.87      0.82        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.81      0.88      0.84        49\n          41       1.00      0.42      0.59        12\n          42       0.85      0.85      0.85        27\n          43       0.05      1.00      0.09         9\n          44       1.00      0.33      0.50         3\n          45       0.57      0.68      0.62        19\n          46       0.80      0.68      0.74       656\n          47       0.67      0.40      0.50         5\n          48       0.33      0.57      0.42         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.20      0.22         5\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.67      0.67      0.67         3\n          64       1.00      0.00      0.00         1\n          66       0.46      0.93      0.62        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.50      1.00      0.67         4\n          73       0.71      0.75      0.73       567\n          75       0.81      1.00      0.90        13\n          76       0.14      1.00      0.25         6\n          77       0.62      0.45      0.53        11\n          78       0.58      0.98      0.73        53\n          79       0.71      0.83      0.77         6\n          81       1.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.67      1.00      0.80         6\n          85       0.00      1.00      0.00         0\n          86       0.17      0.71      0.27         7\n          87       0.96      0.96      0.96        27\n          88       0.44      1.00      0.61         7\n          89       0.10      1.00      0.18         2\n          90       0.59      0.83      0.69        12\n          92       0.00      1.00      0.00         0\n          93       0.20      1.00      0.33         2\n          95       0.85      0.96      0.90        24\n          96       0.00      1.00      0.00         0\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.02      0.33      0.04         6\n         103       0.73      0.80      0.76        20\n         104       0.80      0.72      0.76       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         114       0.00      1.00      0.00         0\n         115       0.44      0.80      0.57         5\n         116       0.78      0.88      0.83       132\n         117       0.67      1.00      0.80         2\n         118       0.50      0.75      0.60         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.61      0.78      0.60     10056\nweighted avg       0.94      0.89      0.91     10056\n'],1000,256,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_256_1b_1h_1ff,"{'name': 'sequential_54', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_30'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_9', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 256, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_10', 'embed_dim': 256, 'num_heads': 1, 'ff_dim': 256, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 256)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_9', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_79', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9723594188690186,0.2847606837749481,0.9648101925849915,0.2351524978876114,2178.0,926.0,770.0,779.0,6713.0,0.5666046746613972,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       0.89      1.00      0.94         8\n           5       0.44      0.80      0.57         5\n           6       1.00      1.00      1.00         1\n           7       0.57      0.80      0.67        10\n           8       0.60      1.00      0.75         3\n           9       0.60      1.00      0.75         3\n          10       0.00      1.00      0.00         0\n          11       1.00      0.33      0.50         3\n          13       0.70      1.00      0.82         7\n          14       0.50      0.20      0.29         5\n          15       0.53      0.91      0.67        11\n          16       0.55      1.00      0.71        11\n          17       0.84      0.78      0.81        46\n          18       0.78      0.94      0.85        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.85      0.92      0.88        12\n          25       0.97      0.94      0.96       102\n          26       0.96      0.64      0.77        39\n          27       0.92      0.92      0.92        39\n          28       0.56      0.83      0.67        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.95      0.81      0.87        68\n          33       0.00      0.00      0.00         1\n          34       0.88      0.65      0.75        34\n          35       0.83      0.81      0.82        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.90      0.90        49\n          41       1.00      0.42      0.59        12\n          42       0.83      0.89      0.86        27\n          43       0.06      0.89      0.12         9\n          44       1.00      0.33      0.50         3\n          45       0.55      0.63      0.59        19\n          46       0.78      0.77      0.77       656\n          47       1.00      0.80      0.89         5\n          48       0.62      0.71      0.67         7\n          49       1.00      1.00      1.00         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.37      0.93      0.53        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.25      0.75      0.38         4\n          73       0.75      0.71      0.73       567\n          75       1.00      1.00      1.00        13\n          76       0.67      1.00      0.80         6\n          77       1.00      0.36      0.53        11\n          78       0.60      0.98      0.75        53\n          79       0.50      0.83      0.62         6\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.31      0.83      0.45         6\n          85       0.00      1.00      0.00         0\n          86       0.28      0.71      0.40         7\n          87       0.96      0.85      0.90        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.18         2\n          90       0.60      0.75      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.33      0.50      0.40         2\n          94       0.00      1.00      0.00         0\n          95       0.70      0.67      0.68        24\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.03      0.50      0.06         6\n         103       0.81      0.65      0.72        20\n         104       0.68      0.83      0.75       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.82      0.70      0.76       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.59      0.78      0.58     10056\nweighted avg       0.94      0.90      0.91     10056\n'],1000,256,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_100_1b_1h_1ff,"{'name': 'sequential_60', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_34'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding_10', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 100, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block_11', 'embed_dim': 100, 'num_heads': 1, 'ff_dim': 100, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 100)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_10', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_88', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 100)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 100)}}], 'build_input_shape': (None, 46)}",0.9657228589057922,0.2703210115432739,0.9552614092826843,0.23068808019161224,1992.0,1202.0,956.0,1020.0,7908.0,0.5418257712352242,['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       0.47      1.00      0.64         8\n           5       0.22      0.80      0.35         5\n           6       1.00      1.00      1.00         1\n           7       0.56      1.00      0.71        10\n           8       0.75      1.00      0.86         3\n           9       0.60      1.00      0.75         3\n          11       1.00      0.33      0.50         3\n          13       0.71      0.71      0.71         7\n          14       1.00      0.20      0.33         5\n          15       0.58      1.00      0.73        11\n          16       0.69      1.00      0.81        11\n          17       0.62      0.70      0.65        46\n          18       0.76      0.94      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.92      0.95       102\n          26       0.93      0.67      0.78        39\n          27       0.90      0.92      0.91        39\n          28       0.58      0.92      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.56      1.00      0.72         9\n          32       0.92      0.69      0.79        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.76      0.83        34\n          35       0.76      0.81      0.78        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.88      0.91        49\n          41       1.00      0.42      0.59        12\n          42       0.92      0.85      0.88        27\n          43       0.04      0.89      0.08         9\n          44       1.00      0.33      0.50         3\n          45       0.46      0.68      0.55        19\n          46       0.75      0.65      0.70       656\n          47       0.83      1.00      0.91         5\n          48       0.25      0.57      0.35         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.20      0.20         5\n          54       0.83      0.83      0.83         6\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.36      0.86      0.51        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.25      1.00      0.40         4\n          73       0.67      0.68      0.67       567\n          75       1.00      0.92      0.96        13\n          76       0.35      1.00      0.52         6\n          77       0.64      0.64      0.64        11\n          78       0.58      0.98      0.73        53\n          79       0.43      1.00      0.60         6\n          81       1.00      0.50      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.60      1.00      0.75         6\n          85       0.00      1.00      0.00         0\n          86       0.13      0.71      0.22         7\n          87       0.96      0.81      0.88        27\n          88       0.32      1.00      0.48         7\n          89       0.11      1.00      0.19         2\n          90       0.60      0.75      0.67        12\n          91       0.00      1.00      0.00         0\n          93       0.11      0.50      0.18         2\n          94       0.00      1.00      0.00         0\n          95       0.78      0.88      0.82        24\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.02      0.50      0.03         6\n         103       0.62      0.65      0.63        20\n         104       0.65      0.78      0.71       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.40      0.80      0.53         5\n         116       0.79      0.65      0.71       132\n         117       0.67      1.00      0.80         2\n         118       0.50      0.50      0.50         8\n         119       0.99      0.87      0.93      5640\n\n    accuracy                           0.86     10056\n   macro avg       0.58      0.77      0.57     10056\nweighted avg       0.93      0.86      0.89     10056\n'],1000,100,10,32,False,False,False,,post,accuracy,macro,True,False,5
architecture_name,summary,best_model_training_acc,best_model_training_f1,best_model_validation_acc,best_model_validation_f1,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
Transformer_256_1b_2h_1ff_lr,"{'name': 'sequential_2', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None}, {'module': 'transformer_prof', 'class_name': 'TokenAndPositionEmbedding', 'config': {'name': 'token_and_position_embedding', 'maxlen': 46, 'vocab_size': 1001, 'embed_dim': 256, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TokenAndPositionEmbedding', 'build_config': {'input_shape': (None, 46)}}, {'module': 'transformer_prof', 'class_name': 'TransformerBlock', 'config': {'name': 'transformer_block', 'embed_dim': 256, 'num_heads': 2, 'ff_dim': 256, 'trainable': True, 'dtype': 'float32'}, 'registered_name': 'TransformerBlock', 'build_config': {'input_shape': (None, 46, 256)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.940524160861969,0.2214290350675583,0.9350491166114807,0.19603225588798523,1577.0,1915.0,1371.0,1414.0,9323.0,0.38651692699573137,['              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.98      0.99       101\n           4       0.40      1.00      0.57         8\n           5       0.02      0.20      0.04         5\n           6       0.00      0.00      0.00         1\n           7       0.22      0.50      0.30        10\n           8       0.20      0.33      0.25         3\n           9       0.33      1.00      0.50         3\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.29      0.29      0.29         7\n          14       0.00      0.00      0.00         5\n          15       0.38      0.45      0.42        11\n          16       0.40      0.91      0.56        11\n          17       0.45      0.57      0.50        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          24       0.71      0.83      0.77        12\n          25       0.93      0.76      0.84       102\n          26       0.96      0.62      0.75        39\n          27       0.67      0.79      0.73        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.44      0.47         9\n          32       0.91      0.44      0.59        68\n          33       0.00      0.00      0.00         1\n          34       0.67      0.29      0.41        34\n          35       0.91      0.68      0.78        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.00      0.00         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.84      0.87        49\n          41       0.83      0.42      0.56        12\n          42       0.85      0.81      0.83        27\n          43       0.03      0.67      0.06         9\n          44       1.00      0.00      0.00         3\n          45       0.07      0.26      0.11        19\n          46       0.70      0.52      0.60       656\n          47       0.44      0.80      0.57         5\n          48       0.11      0.57      0.19         7\n          49       1.00      1.00      1.00         5\n          50       0.12      1.00      0.22         1\n          51       0.86      0.86      0.86         7\n          52       0.11      0.20      0.14         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.97      0.93        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.07      0.79      0.12        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.21      0.75      0.33         4\n          73       0.68      0.52      0.59       567\n          75       1.00      0.62      0.76        13\n          76       0.22      1.00      0.36         6\n          77       0.38      0.45      0.42        11\n          78       0.59      0.98      0.74        53\n          79       0.27      0.67      0.38         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.14      0.33      0.20         6\n          85       0.00      1.00      0.00         0\n          86       0.27      0.57      0.36         7\n          87       0.91      0.74      0.82        27\n          88       0.33      1.00      0.50         7\n          89       0.08      1.00      0.15         2\n          90       0.58      0.58      0.58        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.07      0.50      0.12         2\n          94       0.00      1.00      0.00         0\n          95       0.69      0.38      0.49        24\n          97       0.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.01      0.33      0.02         6\n         103       0.70      0.35      0.47        20\n         104       0.69      0.62      0.65       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.80      0.65      0.72       132\n         117       0.40      1.00      0.57         2\n         118       0.33      0.50      0.40         8\n         119       0.99      0.82      0.90      5640\n\n    accuracy                           0.80     10056\n   macro avg       0.43      0.69      0.41     10056\nweighted avg       0.92      0.80      0.85     10056\n'],1000,256,10,32,False,False,False,,post,accuracy,macro,True,False,5
