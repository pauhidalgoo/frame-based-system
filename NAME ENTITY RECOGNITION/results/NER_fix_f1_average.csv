architecture_name,summary,average_training_acc,average_training_f1,average_training_loss,average_val_acc,average_val_f1,average_val_loss,average_tp,average_fp,average_fn,average_errors,average_tokens,sci_f1,reports,vocab_size,embedding_dim,epochs,batch_size,lemmatize,stem,remove_stopwords,custom_stopwords,padding,selection_metric,f1_type,use_sample_weights,early_stopping,early_stopping_patience
SimpleRNN_64,"{'name': 'sequential_1', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9773281514644623,0.2847747951745987,0.04969291016459465,0.9726746082305908,0.25192227959632874,0.16402567923069,2562.5,838.0,385.5,963.5,5099.0,0.600035775161951,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.86      0.90      0.88        20\n           3       0.96      0.97      0.97       101\n           4       0.24      1.00      0.38         8\n           5       0.25      1.00      0.40         5\n           6       0.00      0.00      0.00         1\n           7       0.48      1.00      0.65        10\n           8       0.75      1.00      0.86         3\n           9       0.38      1.00      0.55         3\n          11       0.60      1.00      0.75         3\n          13       0.62      0.71      0.67         7\n          14       1.00      1.00      1.00         5\n          15       0.91      0.91      0.91        11\n          16       0.41      1.00      0.58        11\n          17       0.32      0.80      0.46        46\n          18       0.82      1.00      0.90        33\n          19       0.83      0.83      0.83         6\n          20       0.97      1.00      0.98        61\n          24       0.52      0.92      0.67        12\n          25       0.95      0.98      0.97       102\n          26       0.90      0.92      0.91        39\n          27       0.81      0.97      0.88        39\n          28       0.77      0.83      0.80        12\n          29       0.33      1.00      0.50         1\n          30       1.00      0.00      0.00         1\n          31       0.75      1.00      0.86         9\n          32       0.74      0.87      0.80        68\n          33       1.00      0.00      0.00         1\n          34       0.90      0.82      0.86        34\n          35       0.74      0.90      0.81        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       1.00      0.92      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.88      0.86      0.87        49\n          41       0.30      0.58      0.40        12\n          42       0.77      0.85      0.81        27\n          43       0.16      1.00      0.27         9\n          44       0.60      1.00      0.75         3\n          45       0.41      0.79      0.54        19\n          46       0.96      0.94      0.95       656\n          47       1.00      1.00      1.00         5\n          48       0.86      0.86      0.86         7\n          49       1.00      1.00      1.00         5\n          50       0.17      1.00      0.29         1\n          51       0.86      0.86      0.86         7\n          52       0.18      0.40      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.83      0.83      0.83         6\n          62       0.94      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.41      1.00      0.58        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.33      0.75      0.46         4\n          73       0.93      0.95      0.94       567\n          75       1.00      1.00      1.00        13\n          76       0.40      1.00      0.57         6\n          77       0.69      1.00      0.81        11\n          78       0.60      0.98      0.75        53\n          79       0.67      1.00      0.80         6\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.50      1.00      0.67         2\n          84       0.86      1.00      0.92         6\n          86       0.40      0.86      0.55         7\n          87       0.93      1.00      0.96        27\n          88       0.64      1.00      0.78         7\n          89       1.00      1.00      1.00         2\n          90       0.61      0.92      0.73        12\n          91       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.29      1.00      0.44         6\n         103       0.41      0.75      0.53        20\n         104       0.88      0.95      0.91       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.83      0.98      0.90       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.62      0.87      0.65     10056\nweighted avg       0.95      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.78      0.90      0.84        20\n           3       0.84      0.97      0.90       101\n           4       0.35      1.00      0.52         8\n           5       0.20      1.00      0.33         5\n           6       1.00      0.00      0.00         1\n           7       0.47      0.90      0.62        10\n           8       0.75      1.00      0.86         3\n           9       0.33      0.67      0.44         3\n          11       0.38      1.00      0.55         3\n          12       0.00      1.00      0.00         0\n          13       0.45      0.71      0.56         7\n          14       0.50      0.80      0.62         5\n          15       0.75      0.82      0.78        11\n          16       0.41      1.00      0.58        11\n          17       0.37      0.78      0.50        46\n          18       0.89      0.94      0.91        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.61      0.92      0.73        12\n          25       0.98      0.96      0.97       102\n          26       0.87      0.85      0.86        39\n          27       0.90      0.97      0.94        39\n          28       0.92      0.92      0.92        12\n          29       0.20      1.00      0.33         1\n          30       0.50      1.00      0.67         1\n          31       0.50      0.89      0.64         9\n          32       0.70      0.81      0.75        68\n          33       0.50      1.00      0.67         1\n          34       0.82      0.82      0.82        34\n          35       0.72      0.90      0.80        31\n          36       1.00      1.00      1.00         5\n          37       0.40      1.00      0.57         2\n          38       1.00      0.92      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.75      0.90      0.81        49\n          41       0.50      0.50      0.50        12\n          42       0.66      0.85      0.74        27\n          43       0.19      1.00      0.32         9\n          44       1.00      1.00      1.00         3\n          45       0.42      0.74      0.54        19\n          46       0.93      0.95      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.09      1.00      0.17         1\n          51       0.86      0.86      0.86         7\n          52       0.06      0.20      0.09         5\n          54       0.62      0.83      0.71         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.95      0.93        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.50      1.00      0.67        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.93      0.95      0.94       567\n          75       0.93      1.00      0.96        13\n          76       0.40      1.00      0.57         6\n          77       0.73      1.00      0.85        11\n          78       0.58      0.98      0.73        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          82       0.00      1.00      0.00         0\n          83       0.40      1.00      0.57         2\n          84       1.00      0.67      0.80         6\n          85       0.00      1.00      0.00         0\n          86       0.50      1.00      0.67         7\n          87       0.96      1.00      0.98        27\n          88       0.70      1.00      0.82         7\n          89       1.00      1.00      1.00         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.96      0.85        24\n          97       0.25      1.00      0.40         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.28      0.83      0.42         6\n         103       0.75      0.75      0.75        20\n         104       0.85      0.97      0.91       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.87      0.97      0.92       132\n         117       0.67      1.00      0.80         2\n         118       0.36      1.00      0.53         8\n         119       0.99      0.89      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.56      0.89      0.59     10056\nweighted avg       0.95      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

SimpleRNN_128,"{'name': 'sequential_5', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_3'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_3', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9799881875514984,0.2873043119907379,0.04704132303595543,0.9738621115684509,0.2540725916624069,0.15989936143159866,2568.0,784.0,380.0,894.0,4879.5,0.5797079436825681,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.69      0.90      0.78        20\n           3       0.96      0.98      0.97       101\n           4       0.17      1.00      0.30         8\n           5       0.24      1.00      0.38         5\n           6       1.00      0.00      0.00         1\n           7       0.64      0.90      0.75        10\n           8       0.60      1.00      0.75         3\n           9       0.33      0.67      0.44         3\n          11       0.60      1.00      0.75         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.71      0.59         7\n          14       0.71      1.00      0.83         5\n          15       0.83      0.91      0.87        11\n          16       0.27      0.91      0.42        11\n          17       0.49      0.74      0.59        46\n          18       0.89      0.97      0.93        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          23       0.00      1.00      0.00         0\n          24       0.55      0.92      0.69        12\n          25       0.95      0.98      0.97       102\n          26       0.87      0.85      0.86        39\n          27       0.79      0.97      0.87        39\n          28       0.85      0.92      0.88        12\n          29       0.14      1.00      0.25         1\n          30       0.33      1.00      0.50         1\n          31       0.47      0.89      0.62         9\n          32       0.89      0.82      0.85        68\n          33       0.25      1.00      0.40         1\n          34       1.00      0.88      0.94        34\n          35       0.71      0.87      0.78        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.70      0.90      0.79        49\n          41       0.50      0.50      0.50        12\n          42       0.70      0.85      0.77        27\n          43       0.22      0.89      0.35         9\n          44       1.00      1.00      1.00         3\n          45       0.42      0.74      0.54        19\n          46       0.94      0.95      0.95       656\n          47       0.83      1.00      0.91         5\n          48       0.42      0.71      0.53         7\n          49       1.00      1.00      1.00         5\n          50       0.11      1.00      0.20         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.40      0.27         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.46      0.93      0.62        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.91      0.95      0.93       567\n          75       0.81      1.00      0.90        13\n          76       0.30      1.00      0.46         6\n          77       0.69      1.00      0.81        11\n          78       0.65      0.98      0.78        53\n          79       0.60      1.00      0.75         6\n          81       0.50      1.00      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.43      1.00      0.60         6\n          86       0.41      1.00      0.58         7\n          87       0.90      1.00      0.95        27\n          88       0.58      1.00      0.74         7\n          89       1.00      1.00      1.00         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.17      0.83      0.28         6\n         103       0.60      0.75      0.67        20\n         104       0.91      0.95      0.93       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.36      1.00      0.53         5\n         116       0.86      0.98      0.92       132\n         117       0.67      1.00      0.80         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.55      0.90      0.59     10056\nweighted avg       0.95      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.82      0.90      0.86        20\n           3       0.92      0.96      0.94       101\n           4       0.33      1.00      0.50         8\n           5       0.19      1.00      0.31         5\n           6       0.00      0.00      0.00         1\n           7       0.59      1.00      0.74        10\n           8       0.50      0.67      0.57         3\n           9       0.50      0.67      0.57         3\n          11       0.50      1.00      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.60      0.86      0.71         7\n          14       0.67      0.80      0.73         5\n          15       0.62      0.91      0.74        11\n          16       0.27      1.00      0.42        11\n          17       0.35      0.76      0.48        46\n          18       0.84      0.97      0.90        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          24       0.61      0.92      0.73        12\n          25       0.97      0.98      0.98       102\n          26       0.89      0.87      0.88        39\n          27       0.86      0.97      0.92        39\n          28       0.83      0.83      0.83        12\n          29       0.33      1.00      0.50         1\n          30       1.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.89      0.85      0.87        68\n          33       0.50      1.00      0.67         1\n          34       0.93      0.82      0.88        34\n          35       0.87      0.87      0.87        31\n          36       0.83      1.00      0.91         5\n          37       1.00      1.00      1.00         2\n          38       0.92      1.00      0.96        12\n          39       0.86      0.86      0.86         7\n          40       0.81      0.88      0.84        49\n          41       0.67      0.50      0.57        12\n          42       0.80      0.89      0.84        27\n          43       0.18      1.00      0.31         9\n          44       1.00      1.00      1.00         3\n          45       0.37      0.79      0.50        19\n          46       0.96      0.94      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.12      1.00      0.22         1\n          51       0.86      0.86      0.86         7\n          52       0.06      0.20      0.10         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.83      0.83      0.83         6\n          58       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.74      1.00      0.85        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.93      0.96      0.94       567\n          75       0.93      1.00      0.96        13\n          76       0.29      0.83      0.43         6\n          77       0.52      1.00      0.69        11\n          78       0.58      0.98      0.73        53\n          79       0.75      1.00      0.86         6\n          80       0.00      1.00      0.00         0\n          81       1.00      1.00      1.00         2\n          82       0.00      1.00      0.00         0\n          83       0.50      0.50      0.50         2\n          84       0.43      1.00      0.60         6\n          85       0.00      1.00      0.00         0\n          86       0.35      0.86      0.50         7\n          87       0.87      1.00      0.93        27\n          88       0.54      1.00      0.70         7\n          89       1.00      1.00      1.00         2\n          90       0.61      0.92      0.73        12\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.17      0.83      0.28         6\n         103       0.71      0.85      0.77        20\n         104       0.92      0.95      0.93       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         110       0.00      1.00      0.00         0\n         111       0.93      0.98      0.96        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.89      0.98      0.94       132\n         117       1.00      1.00      1.00         2\n         118       0.32      1.00      0.48         8\n         119       0.99      0.90      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.58      0.88      0.61     10056\nweighted avg       0.96      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

SimpleRNN_32,"{'name': 'sequential_8', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_5'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_5', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_5', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.976784348487854,0.2824609726667404,0.048225175589323044,0.9716324210166931,0.24929974973201752,0.17494945973157883,2553.5,833.0,394.5,990.0,5121.5,0.5675414265280591,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.90      0.90      0.90        20\n           3       0.93      0.98      0.95       101\n           4       0.33      1.00      0.50         8\n           5       0.28      1.00      0.43         5\n           6       0.00      0.00      0.00         1\n           7       0.60      0.90      0.72        10\n           8       0.50      1.00      0.67         3\n           9       0.60      1.00      0.75         3\n          11       0.38      1.00      0.55         3\n          12       0.00      1.00      0.00         0\n          13       0.33      0.86      0.48         7\n          14       0.75      0.60      0.67         5\n          15       0.53      0.91      0.67        11\n          16       0.28      1.00      0.43        11\n          17       0.33      0.76      0.46        46\n          18       0.89      0.97      0.93        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          23       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.94      0.98      0.96       102\n          26       0.89      0.85      0.87        39\n          27       0.88      0.97      0.93        39\n          28       0.41      0.92      0.56        12\n          29       0.50      1.00      0.67         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.89      0.84      0.86        68\n          33       0.33      1.00      0.50         1\n          34       0.71      0.85      0.77        34\n          35       0.65      0.90      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      1.00      1.00         2\n          38       0.92      0.92      0.92        12\n          39       0.86      0.86      0.86         7\n          40       0.62      0.88      0.73        49\n          41       1.00      0.50      0.67        12\n          42       0.80      0.89      0.84        27\n          43       0.18      1.00      0.31         9\n          44       0.75      1.00      0.86         3\n          45       0.58      0.74      0.65        19\n          46       0.93      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.46      0.86      0.60         7\n          49       1.00      0.80      0.89         5\n          50       0.06      1.00      0.12         1\n          51       0.75      0.86      0.80         7\n          52       0.33      0.40      0.36         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          62       0.89      0.97      0.93        64\n          63       1.00      1.00      1.00         3\n          64       0.00      0.00      0.00         1\n          66       0.54      0.93      0.68        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.30      0.75      0.43         4\n          73       0.91      0.95      0.93       567\n          75       0.93      1.00      0.96        13\n          76       0.35      1.00      0.52         6\n          77       0.61      1.00      0.76        11\n          78       0.57      0.98      0.72        53\n          79       0.55      1.00      0.71         6\n          81       0.50      1.00      0.67         2\n          83       0.67      1.00      0.80         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.43      0.86      0.57         7\n          87       0.87      1.00      0.93        27\n          88       0.70      1.00      0.82         7\n          89       0.50      1.00      0.67         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.96      0.90        24\n          96       0.00      1.00      0.00         0\n          97       0.25      1.00      0.40         1\n          98       1.00      0.50      0.67         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.13      0.83      0.23         6\n         103       0.54      0.75      0.62        20\n         104       0.84      0.97      0.90       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.83      0.98      0.90       132\n         117       0.67      1.00      0.80         2\n         118       0.40      1.00      0.57         8\n         119       0.99      0.88      0.93      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.54      0.89      0.59     10056\nweighted avg       0.95      0.92      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.90      0.90      0.90        20\n           3       0.97      0.98      0.98       101\n           4       0.32      1.00      0.48         8\n           5       0.26      1.00      0.42         5\n           6       0.00      0.00      0.00         1\n           7       0.69      0.90      0.78        10\n           8       0.60      1.00      0.75         3\n           9       0.40      0.67      0.50         3\n          11       0.50      0.67      0.57         3\n          13       0.46      0.86      0.60         7\n          14       0.75      0.60      0.67         5\n          15       0.69      0.82      0.75        11\n          16       0.32      1.00      0.49        11\n          17       0.38      0.78      0.51        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       0.85      1.00      0.92        61\n          22       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.94      0.98      0.96       102\n          26       0.92      0.85      0.88        39\n          27       0.90      0.97      0.94        39\n          28       0.79      0.92      0.85        12\n          29       0.25      1.00      0.40         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.91      0.85      0.88        68\n          33       0.25      1.00      0.40         1\n          34       0.80      0.82      0.81        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.67      0.86      0.75         7\n          40       0.92      0.90      0.91        49\n          41       0.38      0.42      0.40        12\n          42       0.75      0.89      0.81        27\n          43       0.20      1.00      0.33         9\n          44       0.38      1.00      0.55         3\n          45       0.45      0.74      0.56        19\n          46       0.94      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.83      1.00      0.91         5\n          50       0.11      1.00      0.20         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.20      0.22         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.83      0.83      0.83         6\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.46      0.93      0.62        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.23      0.75      0.35         4\n          73       0.92      0.95      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.55      1.00      0.71         6\n          77       0.42      1.00      0.59        11\n          78       0.60      0.98      0.75        53\n          79       0.43      1.00      0.60         6\n          81       1.00      0.50      0.67         2\n          83       1.00      0.50      0.67         2\n          84       0.62      0.83      0.71         6\n          86       0.29      0.86      0.43         7\n          87       0.93      1.00      0.96        27\n          88       0.50      1.00      0.67         7\n          89       0.33      1.00      0.50         2\n          90       0.65      0.92      0.76        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.73      1.00      0.84        24\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.30      1.00      0.46         6\n         103       0.62      0.75      0.68        20\n         104       0.88      0.95      0.91       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.95      0.98      0.97        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.85      0.98      0.91       132\n         117       1.00      1.00      1.00         2\n         118       0.22      1.00      0.36         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.56      0.87      0.60     10056\nweighted avg       0.95      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

SimpleRNN_256,"{'name': 'sequential_10', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_6'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_6', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'SimpleRNN', 'config': {'name': 'simple_rnn_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_6', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9862782061100006,0.2944732755422592,0.026526923291385174,0.9800543487071991,0.2585807144641876,0.12261278182268143,2619.5,575.0,328.5,664.5,4141.0,0.6430592847585304,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.83      1.00      0.91        20\n           3       0.98      0.96      0.97       101\n           4       0.62      1.00      0.76         8\n           5       0.31      1.00      0.48         5\n           6       1.00      0.00      0.00         1\n           7       0.75      0.90      0.82        10\n           8       0.75      1.00      0.86         3\n           9       0.33      0.67      0.44         3\n          11       0.50      1.00      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.62      0.71      0.67         7\n          14       1.00      0.80      0.89         5\n          15       0.91      0.91      0.91        11\n          16       0.67      0.91      0.77        11\n          17       0.51      0.78      0.62        46\n          18       0.89      0.97      0.93        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.97      0.98      0.98       102\n          26       0.95      0.92      0.94        39\n          27       0.81      0.97      0.88        39\n          28       0.91      0.83      0.87        12\n          29       0.14      1.00      0.25         1\n          30       0.50      1.00      0.67         1\n          31       0.70      0.78      0.74         9\n          32       0.91      0.85      0.88        68\n          33       0.50      1.00      0.67         1\n          34       0.88      0.85      0.87        34\n          35       0.70      0.90      0.79        31\n          36       1.00      1.00      1.00         5\n          37       0.67      1.00      0.80         2\n          38       0.92      0.92      0.92        12\n          39       0.86      0.86      0.86         7\n          40       0.80      0.88      0.83        49\n          41       0.67      0.50      0.57        12\n          42       0.82      0.85      0.84        27\n          43       0.20      1.00      0.33         9\n          44       1.00      1.00      1.00         3\n          45       0.41      0.74      0.53        19\n          46       0.96      0.95      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.71      0.71      0.71         7\n          49       1.00      1.00      1.00         5\n          50       0.08      1.00      0.14         1\n          51       0.86      0.86      0.86         7\n          52       0.09      0.20      0.12         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.61      1.00      0.76        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.30      0.75      0.43         4\n          73       0.94      0.95      0.95       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.43      1.00      0.60         6\n          77       0.48      1.00      0.65        11\n          78       0.60      0.98      0.75        53\n          79       0.55      1.00      0.71         6\n          81       1.00      1.00      1.00         2\n          83       1.00      1.00      1.00         2\n          84       0.67      1.00      0.80         6\n          86       0.56      0.71      0.62         7\n          87       0.96      1.00      0.98        27\n          88       0.64      1.00      0.78         7\n          89       0.67      1.00      0.80         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.76      0.92      0.83        24\n          96       0.00      1.00      0.00         0\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.21      1.00      0.35         6\n         103       0.71      0.75      0.73        20\n         104       0.94      0.97      0.96       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.95      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.63      0.89      0.66     10056\nweighted avg       0.96      0.95      0.95     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.87      1.00      0.93        20\n           3       0.98      0.97      0.98       101\n           4       0.36      1.00      0.53         8\n           5       0.26      1.00      0.42         5\n           6       0.00      0.00      0.00         1\n           7       0.71      1.00      0.83        10\n           8       1.00      1.00      1.00         3\n           9       0.33      0.67      0.44         3\n          11       0.43      1.00      0.60         3\n          12       0.00      1.00      0.00         0\n          13       0.86      0.86      0.86         7\n          14       0.80      0.80      0.80         5\n          15       0.71      0.91      0.80        11\n          16       0.58      1.00      0.73        11\n          17       0.48      0.76      0.59        46\n          18       0.89      0.97      0.93        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          24       0.69      0.92      0.79        12\n          25       0.99      0.98      0.99       102\n          26       0.92      0.90      0.91        39\n          27       0.88      0.97      0.93        39\n          28       0.85      0.92      0.88        12\n          29       0.14      1.00      0.25         1\n          30       0.00      0.00      0.00         1\n          31       0.40      0.89      0.55         9\n          32       0.89      0.85      0.87        68\n          33       0.50      1.00      0.67         1\n          34       0.90      0.82      0.86        34\n          35       0.75      0.87      0.81        31\n          36       1.00      1.00      1.00         5\n          37       1.00      1.00      1.00         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.83      0.90      0.86        49\n          41       0.60      0.50      0.55        12\n          42       0.77      0.85      0.81        27\n          43       0.23      1.00      0.37         9\n          44       1.00      1.00      1.00         3\n          45       0.45      0.74      0.56        19\n          46       0.97      0.95      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.70      1.00      0.82         7\n          49       1.00      1.00      1.00         5\n          50       0.10      1.00      0.18         1\n          51       0.86      0.86      0.86         7\n          52       0.33      0.20      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      0.83      0.91         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.61      1.00      0.76        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.21      0.75      0.33         4\n          73       0.93      0.95      0.94       567\n          75       0.93      1.00      0.96        13\n          76       0.75      1.00      0.86         6\n          77       0.50      1.00      0.67        11\n          78       0.62      0.98      0.76        53\n          79       0.60      1.00      0.75         6\n          81       1.00      1.00      1.00         2\n          83       0.67      1.00      0.80         2\n          84       0.75      1.00      0.86         6\n          86       0.38      0.86      0.52         7\n          87       0.87      1.00      0.93        27\n          88       0.70      1.00      0.82         7\n          89       1.00      1.00      1.00         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.81      0.92      0.86        24\n          97       0.20      1.00      0.33         1\n          98       1.00      0.50      0.67         2\n         101       0.00      0.00      0.00         2\n         102       0.29      0.83      0.43         6\n         103       0.64      0.70      0.67        20\n         104       0.92      0.96      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.67      1.00      0.80         2\n         111       0.92      0.98      0.95        58\n         112       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.65      0.88      0.68     10056\nweighted avg       0.96      0.95      0.95     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_16,"{'name': 'sequential_14', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_9'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_9', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_9', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 16)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 16)}}], 'build_input_shape': (None, 46)}",0.8894758820533752,0.21000777184963226,1.4531803727149963,0.8841064870357513,0.18382854014635086,0.6621054708957672,2252.5,4118.0,695.5,4166.0,10073.0,0.40263363431200616,"['              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1702\n           1       0.03      1.00      0.07         2\n           2       0.18      0.90      0.30        20\n           3       0.88      0.98      0.93       101\n           4       0.05      1.00      0.09         8\n           5       0.04      0.80      0.08         5\n           6       1.00      1.00      1.00         1\n           7       0.20      1.00      0.34        10\n           8       0.50      0.67      0.57         3\n           9       0.07      0.67      0.13         3\n          11       0.25      0.33      0.29         3\n          13       0.29      0.71      0.42         7\n          14       0.25      0.40      0.31         5\n          15       0.57      0.73      0.64        11\n          16       0.12      0.91      0.21        11\n          17       0.42      0.50      0.46        46\n          18       0.47      1.00      0.64        33\n          19       0.46      1.00      0.63         6\n          20       0.72      0.98      0.83        61\n          24       0.41      0.92      0.56        12\n          25       0.73      0.89      0.81       102\n          26       0.67      0.72      0.69        39\n          27       0.18      0.74      0.29        39\n          28       0.48      0.83      0.61        12\n          29       0.50      1.00      0.67         1\n          30       0.06      1.00      0.12         1\n          31       0.29      0.67      0.40         9\n          32       0.74      0.59      0.66        68\n          33       0.01      1.00      0.03         1\n          34       0.74      0.68      0.71        34\n          35       0.48      0.71      0.57        31\n          36       0.02      1.00      0.03         5\n          37       0.33      0.50      0.40         2\n          38       0.17      1.00      0.29        12\n          39       0.55      0.86      0.67         7\n          40       0.68      0.90      0.77        49\n          41       0.08      0.42      0.14        12\n          42       0.49      0.85      0.62        27\n          43       0.03      0.89      0.05         9\n          44       1.00      0.00      0.00         3\n          45       0.08      0.74      0.15        19\n          46       0.45      0.81      0.57       656\n          47       0.15      0.60      0.24         5\n          48       0.19      0.71      0.29         7\n          49       0.50      1.00      0.67         5\n          50       1.00      0.00      0.00         1\n          51       0.40      0.86      0.55         7\n          52       0.05      0.20      0.08         5\n          54       0.67      1.00      0.80         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          62       0.80      0.95      0.87        64\n          63       0.25      0.67      0.36         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.46        14\n          67       0.00      1.00      0.00         0\n          71       0.29      1.00      0.44         2\n          72       0.08      0.75      0.15         4\n          73       0.44      0.84      0.57       567\n          74       0.00      1.00      0.00         0\n          75       0.30      1.00      0.46        13\n          76       0.31      0.67      0.42         6\n          77       0.50      0.64      0.56        11\n          78       0.66      0.98      0.79        53\n          79       0.17      1.00      0.29         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.62      0.83      0.71         6\n          86       0.07      1.00      0.13         7\n          87       0.60      1.00      0.75        27\n          88       0.25      1.00      0.40         7\n          89       1.00      0.50      0.67         2\n          90       0.53      0.83      0.65        12\n          92       0.00      1.00      0.00         0\n          93       0.25      0.50      0.33         2\n          94       0.00      1.00      0.00         0\n          95       0.75      0.88      0.81        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n         100       0.00      1.00      0.00         0\n         101       0.07      0.50      0.12         2\n         102       0.24      0.83      0.37         6\n         103       0.00      0.00      0.00        20\n         104       0.81      0.90      0.85       110\n         105       0.20      1.00      0.33         2\n         106       0.00      0.00      0.00         1\n         108       0.29      1.00      0.44         2\n         111       0.70      0.98      0.82        58\n         112       0.00      1.00      0.00         0\n         115       0.24      1.00      0.38         5\n         116       0.73      0.98      0.83       132\n         117       0.50      1.00      0.67         2\n         118       0.45      0.62      0.53         8\n         119       0.99      0.40      0.57      5640\n\n    accuracy                           0.62     10056\n   macro avg       0.38      0.77      0.41     10056\nweighted avg       0.86      0.62      0.65     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      1702\n           1       0.11      1.00      0.19         2\n           2       0.42      1.00      0.59        20\n           3       0.84      0.97      0.90       101\n           4       0.18      1.00      0.31         8\n           5       0.09      0.60      0.16         5\n           6       1.00      0.00      0.00         1\n           7       0.24      0.80      0.37        10\n           8       0.38      1.00      0.55         3\n           9       0.07      1.00      0.14         3\n          11       0.20      0.33      0.25         3\n          13       0.30      0.43      0.35         7\n          14       0.31      0.80      0.44         5\n          15       0.19      0.73      0.30        11\n          16       0.29      0.91      0.43        11\n          17       0.14      0.63      0.23        46\n          18       0.59      1.00      0.74        33\n          19       0.86      1.00      0.92         6\n          20       0.27      1.00      0.43        61\n          21       0.00      1.00      0.00         0\n          24       0.58      0.92      0.71        12\n          25       0.91      0.84      0.87       102\n          26       0.52      0.59      0.55        39\n          27       0.29      0.92      0.44        39\n          28       0.18      0.83      0.30        12\n          29       0.50      1.00      0.67         1\n          30       0.17      1.00      0.29         1\n          31       0.06      0.44      0.11         9\n          32       0.81      0.69      0.75        68\n          33       0.07      1.00      0.12         1\n          34       0.85      0.68      0.75        34\n          35       0.42      0.87      0.56        31\n          36       0.28      1.00      0.43         5\n          37       0.17      0.50      0.25         2\n          38       0.23      1.00      0.37        12\n          39       0.11      0.86      0.20         7\n          40       0.72      0.86      0.79        49\n          41       0.06      0.42      0.11        12\n          42       0.68      0.85      0.75        27\n          43       0.04      0.67      0.07         9\n          44       0.33      0.33      0.33         3\n          45       0.09      0.68      0.16        19\n          46       0.44      0.86      0.58       656\n          47       0.21      0.60      0.32         5\n          48       0.00      0.00      0.00         7\n          49       0.08      0.80      0.15         5\n          50       0.33      1.00      0.50         1\n          51       0.41      1.00      0.58         7\n          52       0.04      0.20      0.07         5\n          54       0.50      1.00      0.67         6\n          56       0.03      1.00      0.06         6\n          57       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.55      0.98      0.71        64\n          63       1.00      0.67      0.80         3\n          64       0.00      0.00      0.00         1\n          66       0.28      1.00      0.44        14\n          70       0.00      1.00      0.00         0\n          71       0.25      1.00      0.40         2\n          72       0.15      0.75      0.25         4\n          73       0.45      0.83      0.58       567\n          74       0.00      1.00      0.00         0\n          75       0.50      1.00      0.67        13\n          76       0.38      1.00      0.55         6\n          77       0.43      0.27      0.33        11\n          78       0.31      0.98      0.47        53\n          79       0.31      0.83      0.45         6\n          81       0.33      0.50      0.40         2\n          83       0.00      0.00      0.00         2\n          84       0.80      0.67      0.73         6\n          86       0.09      0.29      0.14         7\n          87       0.71      1.00      0.83        27\n          88       0.21      1.00      0.35         7\n          89       0.11      1.00      0.19         2\n          90       0.38      0.92      0.54        12\n          92       0.00      1.00      0.00         0\n          93       0.20      0.50      0.29         2\n          95       0.64      0.96      0.77        24\n          97       1.00      1.00      1.00         1\n          98       0.10      0.50      0.17         2\n         100       0.00      1.00      0.00         0\n         101       0.25      0.50      0.33         2\n         102       0.03      0.33      0.05         6\n         103       0.55      0.55      0.55        20\n         104       0.78      0.89      0.83       110\n         105       1.00      0.00      0.00         2\n         106       1.00      0.00      0.00         1\n         108       0.20      1.00      0.33         2\n         111       0.63      0.98      0.77        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.36      1.00      0.53         5\n         116       0.87      0.88      0.87       132\n         117       0.29      1.00      0.44         2\n         118       0.29      1.00      0.44         8\n         119       0.99      0.43      0.60      5640\n\n    accuracy                           0.63     10056\n   macro avg       0.35      0.77      0.39     10056\nweighted avg       0.86      0.63      0.67     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_32,"{'name': 'sequential_17', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_11'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_11', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_11', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9242035150527954,0.24348584562540054,0.5204623341560364,0.9179147481918335,0.21513894200325012,0.37157823145389557,2394.0,3024.5,554.0,3079.5,9982.0,0.48172300443112265,"['              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.15      1.00      0.27         2\n           2       0.57      1.00      0.73        20\n           3       0.96      0.97      0.97       101\n           4       0.07      1.00      0.14         8\n           5       0.11      0.80      0.19         5\n           6       1.00      0.00      0.00         1\n           7       0.50      1.00      0.67        10\n           8       0.40      0.67      0.50         3\n           9       0.08      0.67      0.14         3\n          11       0.40      0.67      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.46      0.86      0.60         7\n          14       0.33      0.80      0.47         5\n          15       0.85      1.00      0.92        11\n          16       0.13      1.00      0.23        11\n          17       0.16      0.63      0.26        46\n          18       0.76      0.97      0.85        33\n          19       0.62      0.83      0.71         6\n          20       0.90      1.00      0.95        61\n          23       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.90      0.93      0.91       102\n          26       0.67      0.85      0.75        39\n          27       0.23      0.92      0.36        39\n          28       0.67      0.83      0.74        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.29      0.56      0.38         9\n          32       0.79      0.78      0.79        68\n          33       0.04      1.00      0.08         1\n          34       0.86      0.74      0.79        34\n          35       0.68      0.90      0.78        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.32      1.00      0.48        12\n          39       0.86      0.86      0.86         7\n          40       0.66      0.86      0.74        49\n          41       0.21      0.42      0.28        12\n          42       0.68      0.85      0.75        27\n          43       0.08      0.89      0.15         9\n          44       0.75      1.00      0.86         3\n          45       0.11      0.79      0.19        19\n          46       0.49      0.89      0.63       656\n          47       0.67      0.80      0.73         5\n          48       0.54      1.00      0.70         7\n          49       0.45      1.00      0.62         5\n          50       0.03      1.00      0.07         1\n          51       0.46      0.86      0.60         7\n          52       0.05      0.20      0.08         5\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.38      1.00      0.55         6\n          62       0.88      0.98      0.93        64\n          63       0.50      0.67      0.57         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.27      1.00      0.43        14\n          70       0.00      1.00      0.00         0\n          71       0.33      1.00      0.50         2\n          72       0.06      0.75      0.11         4\n          73       0.43      0.91      0.59       567\n          74       0.00      1.00      0.00         0\n          75       0.59      1.00      0.74        13\n          76       0.50      1.00      0.67         6\n          77       0.48      0.91      0.62        11\n          78       0.63      0.98      0.76        53\n          79       0.38      0.83      0.53         6\n          80       0.00      1.00      0.00         0\n          81       0.29      1.00      0.44         2\n          83       0.00      0.00      0.00         2\n          84       1.00      0.83      0.91         6\n          86       0.50      0.71      0.59         7\n          87       0.66      1.00      0.79        27\n          88       0.23      1.00      0.37         7\n          89       1.00      1.00      1.00         2\n          90       0.56      0.83      0.67        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.18      1.00      0.31         2\n          94       0.00      1.00      0.00         0\n          95       0.68      0.96      0.79        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.28      0.83      0.42         6\n         103       0.59      0.80      0.68        20\n         104       0.88      0.96      0.92       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.81        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.31      1.00      0.48         5\n         116       0.85      0.98      0.91       132\n         117       0.67      1.00      0.80         2\n         118       0.33      1.00      0.50         8\n         119       0.99      0.54      0.70      5640\n\n    accuracy                           0.71     10056\n   macro avg       0.44      0.85      0.49     10056\nweighted avg       0.88      0.71      0.75     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.08      1.00      0.14         2\n           2       0.60      0.90      0.72        20\n           3       0.94      0.96      0.95       101\n           4       0.26      1.00      0.41         8\n           5       0.16      1.00      0.27         5\n           6       1.00      0.00      0.00         1\n           7       0.31      1.00      0.48        10\n           8       0.50      0.67      0.57         3\n           9       0.29      0.67      0.40         3\n          11       0.00      0.00      0.00         3\n          13       0.88      1.00      0.93         7\n          14       0.12      0.80      0.22         5\n          15       0.38      0.82      0.51        11\n          16       0.25      1.00      0.40        11\n          17       0.17      0.72      0.27        46\n          18       0.80      1.00      0.89        33\n          19       0.50      1.00      0.67         6\n          20       0.82      1.00      0.90        61\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.86      0.88      0.87       102\n          26       0.86      0.77      0.81        39\n          27       0.67      0.90      0.77        39\n          28       0.50      0.83      0.62        12\n          29       1.00      1.00      1.00         1\n          30       0.10      1.00      0.18         1\n          31       0.47      0.78      0.58         9\n          32       0.88      0.78      0.83        68\n          33       0.03      1.00      0.06         1\n          34       0.83      0.74      0.78        34\n          35       0.71      0.87      0.78        31\n          36       0.56      1.00      0.71         5\n          37       0.50      0.50      0.50         2\n          38       0.44      1.00      0.62        12\n          39       0.86      0.86      0.86         7\n          40       0.67      0.88      0.76        49\n          41       0.14      0.42      0.21        12\n          42       0.56      0.85      0.68        27\n          43       0.04      0.78      0.08         9\n          44       1.00      1.00      1.00         3\n          45       0.16      0.74      0.26        19\n          46       0.52      0.87      0.65       656\n          47       1.00      0.60      0.75         5\n          48       0.29      0.86      0.43         7\n          49       0.45      1.00      0.62         5\n          50       0.00      0.00      0.00         1\n          51       0.50      0.86      0.63         7\n          52       0.07      0.20      0.10         5\n          53       0.00      1.00      0.00         0\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.35      1.00      0.52         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.25      1.00      0.41        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.18      1.00      0.31         2\n          72       0.10      0.75      0.17         4\n          73       0.53      0.92      0.67       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.24      1.00      0.39         6\n          77       0.75      0.55      0.63        11\n          78       0.63      0.98      0.77        53\n          79       0.60      1.00      0.75         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.46      1.00      0.63         6\n          85       0.00      1.00      0.00         0\n          86       0.21      0.71      0.32         7\n          87       0.66      1.00      0.79        27\n          88       0.27      1.00      0.42         7\n          89       1.00      1.00      1.00         2\n          90       0.50      0.92      0.65        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          95       0.90      0.79      0.84        24\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.50      0.83      0.62         6\n         103       0.55      0.80      0.65        20\n         104       0.87      0.95      0.91       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       1.00      1.00      1.00         2\n         111       0.68      0.98      0.80        58\n         112       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.61      0.95      0.74       132\n         117       0.67      1.00      0.80         2\n         118       0.35      1.00      0.52         8\n         119       0.99      0.63      0.77      5640\n\n    accuracy                           0.76     10056\n   macro avg       0.43      0.83      0.48     10056\nweighted avg       0.89      0.76      0.79     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_64,"{'name': 'sequential_20', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_13'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_13', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_13', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9586887955665588,0.259812593460083,0.15567701309919357,0.9558916389942169,0.2328256368637085,0.20939139276742935,2504.5,1492.5,443.5,1589.5,7798.0,0.5596408269672277,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.22      1.00      0.36         2\n           2       0.67      1.00      0.80        20\n           3       0.92      0.97      0.95       101\n           4       0.23      1.00      0.37         8\n           5       0.16      1.00      0.28         5\n           6       1.00      1.00      1.00         1\n           7       0.40      1.00      0.57        10\n           8       1.00      0.67      0.80         3\n           9       0.06      0.67      0.11         3\n          11       0.40      0.67      0.50         3\n          13       0.88      1.00      0.93         7\n          14       0.17      0.20      0.18         5\n          15       0.75      0.82      0.78        11\n          16       0.37      0.91      0.53        11\n          17       0.23      0.72      0.35        46\n          18       0.79      0.94      0.86        33\n          19       0.86      1.00      0.92         6\n          20       0.79      1.00      0.88        61\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.70      0.98      0.82       102\n          26       0.94      0.77      0.85        39\n          27       0.77      0.95      0.85        39\n          28       0.77      0.83      0.80        12\n          29       0.50      1.00      0.67         1\n          30       0.17      1.00      0.29         1\n          31       0.44      0.78      0.56         9\n          32       0.81      0.81      0.81        68\n          33       0.14      1.00      0.25         1\n          34       0.90      0.79      0.84        34\n          35       0.85      0.90      0.88        31\n          36       0.62      1.00      0.77         5\n          37       0.25      0.50      0.33         2\n          38       0.43      1.00      0.60        12\n          39       0.86      0.86      0.86         7\n          40       0.85      0.90      0.87        49\n          41       0.56      0.42      0.48        12\n          42       0.70      0.85      0.77        27\n          43       0.08      0.78      0.15         9\n          44       0.75      1.00      0.86         3\n          45       0.36      0.79      0.49        19\n          46       0.83      0.91      0.87       656\n          47       1.00      0.80      0.89         5\n          48       0.47      1.00      0.64         7\n          49       0.62      1.00      0.77         5\n          50       0.07      1.00      0.12         1\n          51       0.46      0.86      0.60         7\n          52       0.09      0.20      0.12         5\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.75      1.00      0.86         3\n          64       0.00      0.00      0.00         1\n          66       0.23      1.00      0.37        14\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.05      0.75      0.09         4\n          73       0.77      0.88      0.82       567\n          74       0.00      1.00      0.00         0\n          75       0.72      1.00      0.84        13\n          76       0.30      1.00      0.46         6\n          77       0.73      1.00      0.85        11\n          78       0.57      0.98      0.72        53\n          79       0.86      1.00      0.92         6\n          80       0.00      1.00      0.00         0\n          81       0.20      0.50      0.29         2\n          83       0.00      0.00      0.00         2\n          84       0.46      1.00      0.63         6\n          86       0.47      1.00      0.64         7\n          87       0.71      1.00      0.83        27\n          88       0.25      1.00      0.40         7\n          89       0.10      1.00      0.17         2\n          90       0.52      0.92      0.67        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.85      0.92      0.88        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.33      0.67      0.44         6\n         103       0.59      0.85      0.69        20\n         104       0.90      0.96      0.93       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.81        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.92      0.98      0.95       132\n         117       0.40      1.00      0.57         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.81      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.50      0.87      0.57     10056\nweighted avg       0.93      0.87      0.89     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.17      1.00      0.29         2\n           2       0.60      0.90      0.72        20\n           3       0.89      0.97      0.93       101\n           4       0.80      1.00      0.89         8\n           5       0.11      0.80      0.20         5\n           6       1.00      0.00      0.00         1\n           7       0.53      1.00      0.69        10\n           8       1.00      0.67      0.80         3\n           9       0.14      0.67      0.24         3\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.46      0.86      0.60         7\n          14       0.30      0.60      0.40         5\n          15       0.83      0.91      0.87        11\n          16       0.58      1.00      0.73        11\n          17       0.22      0.78      0.34        46\n          18       0.79      0.94      0.86        33\n          19       0.75      1.00      0.86         6\n          20       0.84      1.00      0.91        61\n          24       0.69      0.92      0.79        12\n          25       0.88      0.97      0.93       102\n          26       0.77      0.87      0.82        39\n          27       0.63      0.97      0.77        39\n          28       0.71      0.83      0.77        12\n          29       1.00      1.00      1.00         1\n          30       0.12      1.00      0.22         1\n          31       0.38      0.67      0.48         9\n          32       0.78      0.79      0.79        68\n          33       0.25      1.00      0.40         1\n          34       0.96      0.76      0.85        34\n          35       0.75      0.87      0.81        31\n          36       0.56      1.00      0.71         5\n          37       0.20      0.50      0.29         2\n          38       0.43      1.00      0.60        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.90      0.90        49\n          41       0.33      0.42      0.37        12\n          42       0.74      0.85      0.79        27\n          43       0.11      0.89      0.19         9\n          44       0.75      1.00      0.86         3\n          45       0.29      0.84      0.43        19\n          46       0.90      0.91      0.91       656\n          47       1.00      0.80      0.89         5\n          48       0.38      0.71      0.50         7\n          49       0.56      1.00      0.71         5\n          50       0.00      0.00      0.00         1\n          51       0.60      0.86      0.71         7\n          52       0.08      0.20      0.11         5\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          58       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.28      1.00      0.44        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.12      0.75      0.20         4\n          73       0.66      0.94      0.78       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.50      1.00      0.67         6\n          77       0.92      1.00      0.96        11\n          78       0.57      0.98      0.72        53\n          79       0.67      1.00      0.80         6\n          81       0.25      0.50      0.33         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.60      1.00      0.75         6\n          86       0.54      1.00      0.70         7\n          87       0.66      1.00      0.79        27\n          88       0.30      1.00      0.47         7\n          89       0.11      1.00      0.19         2\n          90       0.55      0.92      0.69        12\n          93       0.25      0.50      0.33         2\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.17      0.50      0.25         2\n         102       0.23      1.00      0.38         6\n         103       0.57      0.80      0.67        20\n         104       0.91      0.96      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.78      0.98      0.87        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.90      0.98      0.94       132\n         117       0.50      1.00      0.67         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.81      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.52      0.84      0.56     10056\nweighted avg       0.93      0.87      0.89     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_128,"{'name': 'sequential_23', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_15'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_15', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_15', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9779332876205444,0.2756091058254242,0.06749775633215904,0.9722989797592163,0.24685068428516388,0.14179293811321259,2590.5,877.5,357.5,973.0,5661.5,0.6262512717632847,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.77      1.00      0.87        20\n           3       0.95      0.97      0.96       101\n           4       1.00      1.00      1.00         8\n           5       0.20      1.00      0.33         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       0.50      0.67      0.57         3\n           9       0.29      0.67      0.40         3\n          11       0.20      0.33      0.25         3\n          13       1.00      0.86      0.92         7\n          14       0.50      0.80      0.62         5\n          15       1.00      0.91      0.95        11\n          16       0.38      1.00      0.55        11\n          17       0.26      0.72      0.38        46\n          18       0.86      0.94      0.90        33\n          19       0.62      0.83      0.71         6\n          20       0.84      1.00      0.91        61\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.96      0.98      0.97       102\n          26       0.94      0.87      0.91        39\n          27       0.86      0.95      0.90        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.50      0.78      0.61         9\n          32       0.90      0.82      0.86        68\n          33       1.00      1.00      1.00         1\n          34       0.90      0.79      0.84        34\n          35       0.82      0.90      0.86        31\n          36       0.62      1.00      0.77         5\n          37       0.25      0.50      0.33         2\n          38       0.36      1.00      0.53        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.92      0.92        49\n          41       0.45      0.42      0.43        12\n          42       0.82      0.85      0.84        27\n          43       0.07      1.00      0.13         9\n          44       1.00      1.00      1.00         3\n          45       0.36      0.84      0.51        19\n          46       0.94      0.95      0.94       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.56      1.00      0.71         5\n          50       0.14      1.00      0.25         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.40      0.31         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.45      1.00      0.62        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.14      0.75      0.24         4\n          73       0.92      0.93      0.93       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.60      1.00      0.75         6\n          77       0.79      1.00      0.88        11\n          78       0.59      0.98      0.74        53\n          79       0.67      1.00      0.80         6\n          81       0.33      1.00      0.50         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.47      1.00      0.64         7\n          87       0.69      1.00      0.82        27\n          88       0.70      1.00      0.82         7\n          89       1.00      1.00      1.00         2\n          90       0.65      0.92      0.76        12\n          93       0.50      1.00      0.67         2\n          95       0.74      0.96      0.84        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.38      0.83      0.53         6\n         103       0.67      0.90      0.77        20\n         104       0.93      0.96      0.95       110\n         105       0.50      1.00      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.84      0.98      0.90        58\n         115       0.71      1.00      0.83         5\n         116       0.95      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.89      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.62      0.88      0.65     10056\nweighted avg       0.95      0.92      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.71      1.00      0.83        20\n           3       0.95      0.96      0.96       101\n           4       1.00      1.00      1.00         8\n           5       0.17      0.80      0.28         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       0.75      1.00      0.86         3\n           9       0.30      1.00      0.46         3\n          11       0.25      0.33      0.29         3\n          13       0.67      0.86      0.75         7\n          14       0.60      0.60      0.60         5\n          15       0.79      1.00      0.88        11\n          16       0.31      1.00      0.47        11\n          17       0.25      0.74      0.37        46\n          18       0.83      0.91      0.87        33\n          19       0.86      1.00      0.92         6\n          20       0.91      1.00      0.95        61\n          24       0.71      0.83      0.77        12\n          25       0.98      0.97      0.98       102\n          26       0.87      0.87      0.87        39\n          27       0.88      0.97      0.93        39\n          28       0.85      0.92      0.88        12\n          29       0.50      1.00      0.67         1\n          30       0.12      1.00      0.22         1\n          31       0.58      0.78      0.67         9\n          32       0.85      0.82      0.84        68\n          33       0.50      1.00      0.67         1\n          34       0.93      0.76      0.84        34\n          35       0.93      0.87      0.90        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.48      1.00      0.65        12\n          39       0.75      0.86      0.80         7\n          40       0.90      0.88      0.89        49\n          41       0.50      0.42      0.45        12\n          42       0.77      0.85      0.81        27\n          43       0.11      0.78      0.19         9\n          44       1.00      1.00      1.00         3\n          45       0.33      0.89      0.48        19\n          46       0.95      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.88      1.00      0.93         7\n          49       0.62      1.00      0.77         5\n          50       0.25      1.00      0.40         1\n          51       0.86      0.86      0.86         7\n          52       0.11      0.20      0.14         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.37      1.00      0.54        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.12      0.75      0.21         4\n          73       0.90      0.92      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.46      1.00      0.63         6\n          77       0.92      1.00      0.96        11\n          78       0.68      0.98      0.81        53\n          79       0.60      1.00      0.75         6\n          81       0.50      1.00      0.67         2\n          83       0.00      0.00      0.00         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.41      1.00      0.58         7\n          87       0.93      1.00      0.96        27\n          88       0.78      1.00      0.88         7\n          89       0.67      1.00      0.80         2\n          90       0.58      0.92      0.71        12\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.30      1.00      0.46         6\n         103       0.69      0.90      0.78        20\n         104       0.95      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         115       0.50      1.00      0.67         5\n         116       0.95      0.99      0.97       132\n         117       0.50      1.00      0.67         2\n         118       0.40      1.00      0.57         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.61      0.88      0.65     10056\nweighted avg       0.96      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_256,"{'name': 'sequential_25', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_16'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_16', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_16', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_16', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}], 'build_input_shape': (None, 46)}",0.9166258275508881,0.1697736158967018,0.8941794037818909,0.911747008562088,0.1574564129114151,0.3112867623567581,1031.0,2481.0,1917.0,1805.5,9827.0,0.3318275133728754,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.65      1.00      0.78        20\n           3       0.99      0.89      0.94       101\n           4       1.00      1.00      1.00         8\n           5       0.01      0.20      0.02         5\n           6       0.08      1.00      0.15         1\n           7       0.00      0.00      0.00        10\n           8       1.00      0.00      0.00         3\n           9       0.11      0.33      0.17         3\n          11       0.08      0.33      0.13         3\n          13       1.00      0.00      0.00         7\n          14       0.17      0.20      0.18         5\n          15       0.44      0.36      0.40        11\n          16       0.31      0.73      0.43        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.00      0.00      0.00        12\n          25       0.93      0.25      0.40       102\n          26       0.96      0.59      0.73        39\n          27       0.83      0.49      0.61        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       1.00      0.01      0.03        68\n          33       0.00      0.00      0.00         1\n          34       0.40      0.06      0.10        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.95      0.86      0.90        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.03      0.56      0.05         9\n          44       1.00      0.33      0.50         3\n          45       0.02      0.05      0.03        19\n          46       0.61      0.29      0.39       656\n          47       1.00      0.00      0.00         5\n          48       0.15      0.86      0.25         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.25      0.50      0.33         4\n          73       0.61      0.37      0.46       567\n          75       1.00      0.00      0.00        13\n          76       0.05      0.50      0.10         6\n          77       1.00      0.18      0.31        11\n          78       0.59      0.98      0.74        53\n          79       0.16      0.83      0.26         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       1.00      0.00      0.00         6\n          86       0.00      0.00      0.00         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          92       0.00      1.00      0.00         0\n          93       0.07      1.00      0.13         2\n          95       0.64      0.29      0.40        24\n          96       0.00      1.00      0.00         0\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.00      0.17      0.00         6\n         103       1.00      0.00      0.00        20\n         104       0.63      0.36      0.46       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.27      0.60      0.38         5\n         116       0.65      0.54      0.59       132\n         117       1.00      0.00      0.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.77      0.86      5640\n\n    accuracy                           0.72     10056\n   macro avg       0.50      0.62      0.36     10056\nweighted avg       0.91      0.72      0.79     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.65      1.00      0.78        20\n           3       0.99      0.89      0.94       101\n           4       1.00      1.00      1.00         8\n           5       0.00      0.00      0.00         5\n           6       0.08      1.00      0.15         1\n           7       0.04      0.10      0.06        10\n           8       0.33      0.33      0.33         3\n           9       0.11      0.33      0.17         3\n          10       0.00      1.00      0.00         0\n          11       0.12      0.33      0.18         3\n          13       1.00      0.00      0.00         7\n          14       1.00      0.00      0.00         5\n          15       1.00      0.00      0.00        11\n          16       0.31      0.73      0.43        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.00      0.00      0.00        12\n          25       0.93      0.14      0.24       102\n          26       0.96      0.56      0.71        39\n          27       0.83      0.49      0.61        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       1.00      0.01      0.03        68\n          33       0.00      0.00      0.00         1\n          34       0.56      0.26      0.36        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.95      0.86      0.90        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       1.00      0.33      0.50         3\n          45       0.02      0.05      0.03        19\n          46       0.61      0.28      0.38       656\n          47       1.00      0.00      0.00         5\n          48       0.09      0.43      0.14         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.05      0.75      0.10         4\n          73       0.56      0.36      0.44       567\n          75       1.00      0.00      0.00        13\n          76       0.10      1.00      0.18         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       1.00      0.00      0.00         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          86       0.00      0.00      0.00         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.71      0.42      0.53        12\n          92       0.00      1.00      0.00         0\n          93       0.07      1.00      0.13         2\n          95       0.00      0.00      0.00        24\n          96       0.00      1.00      0.00         0\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.01      0.83      0.02         6\n         103       0.52      0.55      0.54        20\n         104       0.62      0.42      0.50       110\n         105       1.00      0.00      0.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.67      0.69      0.68       132\n         117       0.50      1.00      0.67         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.71     10056\n   macro avg       0.49      0.61      0.35     10056\nweighted avg       0.90      0.71      0.78     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_16,"{'name': 'sequential_28', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_18'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_18', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_18', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_18', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 16)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 16)}}], 'build_input_shape': (None, 46)}",0.9345160722732544,0.21834462136030197,1.408233880996704,0.9285661280155182,0.19460710883140564,0.5034277886152267,2202.0,2330.0,746.0,2276.0,9307.5,0.41034991253872494,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.77      1.00      0.87        20\n           3       0.98      0.96      0.97       101\n           4       0.03      1.00      0.06         8\n           5       0.31      1.00      0.48         5\n           6       0.00      0.00      0.00         1\n           7       0.05      0.30      0.08        10\n           8       0.14      0.33      0.20         3\n           9       0.05      0.33      0.08         3\n          11       0.25      0.33      0.29         3\n          13       0.08      0.14      0.10         7\n          14       0.02      0.20      0.03         5\n          15       0.22      0.64      0.33        11\n          16       0.27      0.73      0.39        11\n          17       0.14      0.65      0.23        46\n          18       0.69      1.00      0.81        33\n          19       0.50      1.00      0.67         6\n          20       0.95      0.98      0.97        61\n          21       0.00      1.00      0.00         0\n          24       0.77      0.83      0.80        12\n          25       0.85      0.44      0.58       102\n          26       0.88      0.38      0.54        39\n          27       0.77      0.77      0.77        39\n          28       0.36      0.83      0.50        12\n          29       0.06      1.00      0.12         1\n          30       0.00      0.00      0.00         1\n          31       0.23      0.56      0.32         9\n          32       0.87      0.59      0.70        68\n          33       0.00      0.00      0.00         1\n          34       0.78      0.74      0.76        34\n          35       0.84      0.87      0.86        31\n          36       0.11      1.00      0.20         5\n          37       0.50      0.50      0.50         2\n          38       0.16      1.00      0.27        12\n          39       0.75      0.86      0.80         7\n          40       0.95      0.86      0.90        49\n          41       0.62      0.42      0.50        12\n          42       0.44      0.85      0.58        27\n          43       0.12      0.89      0.21         9\n          44       1.00      1.00      1.00         3\n          45       0.09      0.37      0.14        19\n          46       0.76      0.84      0.80       656\n          47       0.00      0.00      0.00         5\n          48       0.17      0.57      0.26         7\n          49       0.50      1.00      0.67         5\n          50       1.00      1.00      1.00         1\n          51       0.75      0.86      0.80         7\n          52       0.05      0.20      0.08         5\n          54       0.15      1.00      0.27         6\n          56       0.50      1.00      0.67         6\n          60       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.29      1.00      0.44        14\n          71       0.02      1.00      0.04         2\n          72       0.17      0.75      0.27         4\n          73       0.89      0.83      0.86       567\n          75       0.44      0.92      0.60        13\n          76       0.32      1.00      0.48         6\n          77       0.21      0.55      0.31        11\n          78       0.50      0.98      0.66        53\n          79       0.25      0.67      0.36         6\n          81       1.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.33      0.83      0.48         6\n          86       0.15      0.29      0.20         7\n          87       0.59      1.00      0.74        27\n          88       0.21      1.00      0.34         7\n          89       0.40      1.00      0.57         2\n          90       0.43      0.75      0.55        12\n          92       0.00      1.00      0.00         0\n          93       0.18      1.00      0.31         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.54      0.65        24\n          97       0.07      1.00      0.12         1\n          98       0.50      0.50      0.50         2\n         100       0.00      1.00      0.00         0\n         101       0.04      1.00      0.08         2\n         102       0.42      0.83      0.56         6\n         103       0.00      0.00      0.00        20\n         104       0.61      0.45      0.52       110\n         105       1.00      0.00      0.00         2\n         106       1.00      0.00      0.00         1\n         108       0.03      1.00      0.05         2\n         111       0.74      0.98      0.84        58\n         112       0.00      1.00      0.00         0\n         115       0.14      1.00      0.25         5\n         116       0.67      0.67      0.67       132\n         117       0.50      1.00      0.67         2\n         118       0.36      1.00      0.53         8\n         119       0.99      0.72      0.84      5640\n\n    accuracy                           0.78     10056\n   macro avg       0.42      0.71      0.42     10056\nweighted avg       0.92      0.78      0.83     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.05      1.00      0.10         2\n           2       0.83      1.00      0.91        20\n           3       0.86      0.97      0.91       101\n           4       0.89      1.00      0.94         8\n           5       0.10      0.80      0.17         5\n           6       0.00      0.00      0.00         1\n           7       0.09      0.60      0.15        10\n           8       0.25      1.00      0.40         3\n           9       0.06      0.33      0.10         3\n          11       0.25      0.33      0.29         3\n          13       0.00      0.00      0.00         7\n          14       0.02      0.20      0.03         5\n          15       0.22      0.45      0.29        11\n          16       0.35      1.00      0.52        11\n          17       0.13      0.72      0.22        46\n          18       0.75      1.00      0.86        33\n          19       0.33      1.00      0.50         6\n          20       0.98      1.00      0.99        61\n          24       0.79      0.92      0.85        12\n          25       0.89      0.39      0.54       102\n          26       0.96      0.56      0.71        39\n          27       0.82      0.72      0.77        39\n          28       0.42      0.83      0.56        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       0.84      0.40      0.54        68\n          33       0.25      1.00      0.40         1\n          34       0.67      0.53      0.59        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.44      1.00      0.62        12\n          39       0.43      0.86      0.57         7\n          40       0.93      0.86      0.89        49\n          41       0.33      0.42      0.37        12\n          42       0.34      0.85      0.49        27\n          43       0.03      0.67      0.06         9\n          44       0.75      1.00      0.86         3\n          45       0.33      0.53      0.41        19\n          46       0.47      0.91      0.62       656\n          47       0.75      0.60      0.67         5\n          48       0.56      0.71      0.62         7\n          49       0.83      1.00      0.91         5\n          50       0.50      1.00      0.67         1\n          51       0.75      0.86      0.80         7\n          52       0.05      0.20      0.07         5\n          54       0.60      1.00      0.75         6\n          56       0.21      1.00      0.34         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       0.50      0.33      0.40         3\n          64       1.00      0.00      0.00         1\n          66       0.15      0.93      0.26        14\n          67       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.10      0.75      0.17         4\n          73       0.85      0.83      0.84       567\n          75       0.80      0.92      0.86        13\n          76       0.56      0.83      0.67         6\n          77       1.00      0.55      0.71        11\n          78       0.62      0.98      0.76        53\n          79       0.80      0.67      0.73         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.20      0.83      0.32         6\n          85       0.00      1.00      0.00         0\n          86       0.16      0.43      0.23         7\n          87       0.96      1.00      0.98        27\n          88       0.20      1.00      0.33         7\n          89       0.09      1.00      0.16         2\n          90       0.79      0.92      0.85        12\n          92       0.00      1.00      0.00         0\n          93       0.07      1.00      0.12         2\n          94       0.00      1.00      0.00         0\n          95       0.55      0.25      0.34        24\n          97       0.33      1.00      0.50         1\n          98       0.01      0.50      0.02         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.42      0.83      0.56         6\n         103       0.60      0.60      0.60        20\n         104       0.64      0.94      0.76       110\n         105       1.00      0.50      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.25      0.80      0.38         5\n         116       0.88      0.63      0.73       132\n         117       0.33      1.00      0.50         2\n         118       0.62      1.00      0.76         8\n         119       0.99      0.67      0.80      5640\n\n    accuracy                           0.76     10056\n   macro avg       0.46      0.74      0.47     10056\nweighted avg       0.91      0.76      0.81     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_32,"{'name': 'sequential_31', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_20'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_20', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_20', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_20', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9601308703422546,0.2583707422018051,0.4691239297389984,0.9549949169158936,0.22938423603773117,0.26136815547943115,2427.0,1436.5,521.0,1484.0,7678.5,0.503145283700554,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.22      1.00      0.36         2\n           2       0.55      0.90      0.68        20\n           3       0.97      0.97      0.97       101\n           4       0.26      1.00      0.41         8\n           5       0.17      1.00      0.29         5\n           6       1.00      0.00      0.00         1\n           7       0.28      0.70      0.40        10\n           8       0.20      0.67      0.31         3\n           9       0.05      0.33      0.09         3\n          11       0.67      0.67      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.10      0.43      0.17         7\n          14       0.14      0.20      0.17         5\n          15       0.43      0.91      0.59        11\n          16       0.43      0.91      0.59        11\n          17       0.25      0.67      0.36        46\n          18       0.80      1.00      0.89        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.96      0.83      0.89       102\n          26       0.83      0.74      0.78        39\n          27       0.85      0.90      0.88        39\n          28       0.71      0.83      0.77        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.78      0.61         9\n          32       0.77      0.34      0.47        68\n          33       0.00      0.00      0.00         1\n          34       0.63      0.76      0.69        34\n          35       0.74      0.84      0.79        31\n          36       0.24      1.00      0.38         5\n          37       0.04      0.50      0.08         2\n          38       0.46      1.00      0.63        12\n          39       0.50      0.86      0.63         7\n          40       0.30      0.86      0.45        49\n          41       0.42      0.42      0.42        12\n          42       0.96      0.85      0.90        27\n          43       0.08      0.78      0.15         9\n          44       1.00      1.00      1.00         3\n          45       0.24      0.74      0.36        19\n          46       0.94      0.92      0.93       656\n          47       1.00      0.80      0.89         5\n          48       0.54      1.00      0.70         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.86      0.86      0.86         7\n          52       0.04      0.20      0.06         5\n          53       0.00      1.00      0.00         0\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.40      1.00      0.57         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.38      1.00      0.55         3\n          64       1.00      0.00      0.00         1\n          66       0.18      1.00      0.30        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.21      0.75      0.33         4\n          73       0.91      0.92      0.91       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.40      1.00      0.57         6\n          77       1.00      0.73      0.84        11\n          78       0.59      0.98      0.74        53\n          79       0.75      1.00      0.86         6\n          80       0.00      1.00      0.00         0\n          81       0.50      1.00      0.67         2\n          83       0.00      0.00      0.00         2\n          84       0.42      0.83      0.56         6\n          85       0.00      1.00      0.00         0\n          86       0.43      0.86      0.57         7\n          87       1.00      1.00      1.00        27\n          88       0.21      1.00      0.35         7\n          89       0.09      1.00      0.17         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.79      0.79        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.22      0.83      0.34         6\n         103       0.79      0.75      0.77        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.95      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.67      1.00      0.80         8\n         119       0.99      0.83      0.90      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.48      0.83      0.50     10056\nweighted avg       0.95      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.61      1.00      0.75        20\n           3       0.99      0.97      0.98       101\n           4       0.36      1.00      0.53         8\n           5       0.23      1.00      0.37         5\n           6       1.00      0.00      0.00         1\n           7       0.20      0.90      0.33        10\n           8       0.25      0.67      0.36         3\n           9       0.03      0.67      0.05         3\n          11       0.12      1.00      0.22         3\n          12       0.00      1.00      0.00         0\n          13       0.19      0.57      0.29         7\n          14       0.33      0.80      0.47         5\n          15       0.75      0.82      0.78        11\n          16       0.13      0.91      0.23        11\n          17       0.28      0.61      0.38        46\n          18       0.82      0.97      0.89        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.61      0.92      0.73        12\n          25       0.77      0.75      0.76       102\n          26       0.97      0.72      0.82        39\n          27       0.86      0.79      0.83        39\n          28       0.83      0.83      0.83        12\n          29       0.17      1.00      0.29         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.93      0.41      0.57        68\n          33       0.02      1.00      0.04         1\n          34       0.89      0.74      0.81        34\n          35       0.90      0.84      0.87        31\n          36       1.00      1.00      1.00         5\n          37       0.25      0.50      0.33         2\n          38       0.48      1.00      0.65        12\n          39       0.86      0.86      0.86         7\n          40       0.86      0.88      0.87        49\n          41       0.62      0.42      0.50        12\n          42       0.79      0.85      0.82        27\n          43       0.05      1.00      0.10         9\n          44       1.00      1.00      1.00         3\n          45       0.31      0.79      0.45        19\n          46       0.90      0.88      0.89       656\n          47       0.80      0.80      0.80         5\n          48       0.15      0.86      0.26         7\n          49       0.83      1.00      0.91         5\n          50       0.33      1.00      0.50         1\n          51       0.75      0.86      0.80         7\n          52       0.12      0.20      0.15         5\n          53       0.00      1.00      0.00         0\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.50      0.33      0.40         3\n          64       1.00      0.00      0.00         1\n          66       0.28      0.93      0.43        14\n          67       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.50      0.75      0.60         4\n          73       0.83      0.94      0.88       567\n          75       1.00      0.92      0.96        13\n          76       0.29      1.00      0.44         6\n          77       1.00      0.91      0.95        11\n          78       0.58      0.98      0.73        53\n          79       0.55      1.00      0.71         6\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.33      0.50      0.40         2\n          84       0.50      0.83      0.62         6\n          85       0.00      1.00      0.00         0\n          86       0.29      0.86      0.43         7\n          87       1.00      1.00      1.00        27\n          88       0.26      1.00      0.41         7\n          89       0.09      1.00      0.16         2\n          90       0.85      0.92      0.88        12\n          92       0.00      1.00      0.00         0\n          93       0.03      0.50      0.05         2\n          94       0.00      1.00      0.00         0\n          95       0.87      0.83      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.09      0.50      0.15         2\n         102       0.23      0.83      0.36         6\n         103       0.90      0.90      0.90        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.89      1.00      0.94         8\n         119       0.99      0.80      0.89      5640\n\n    accuracy                           0.86     10056\n   macro avg       0.49      0.85      0.50     10056\nweighted avg       0.95      0.86      0.89     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_64,"{'name': 'sequential_34', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_22'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_22', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_22', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_22', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.976397842168808,0.27858054637908936,0.1269826591014862,0.9703843891620636,0.24714233726263046,0.15615259855985641,2572.0,913.5,376.0,993.5,5905.5,0.5939886074531603,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.29      1.00      0.44         2\n           2       0.87      1.00      0.93        20\n           3       0.99      0.97      0.98       101\n           4       0.26      1.00      0.41         8\n           5       0.19      1.00      0.32         5\n           6       1.00      0.00      0.00         1\n           7       0.30      1.00      0.47        10\n           8       1.00      0.67      0.80         3\n           9       0.50      0.67      0.57         3\n          11       0.33      0.33      0.33         3\n          13       0.80      0.57      0.67         7\n          14       0.50      0.40      0.44         5\n          15       0.90      0.82      0.86        11\n          16       0.37      0.91      0.53        11\n          17       0.24      0.74      0.36        46\n          18       0.85      1.00      0.92        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.97      0.96      0.97       102\n          26       0.89      0.87      0.88        39\n          27       0.90      0.97      0.94        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.60      1.00      0.75         9\n          32       0.86      0.82      0.84        68\n          33       0.00      0.00      0.00         1\n          34       0.96      0.76      0.85        34\n          35       0.90      0.90      0.90        31\n          36       1.00      1.00      1.00         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.86      0.88      0.87        49\n          41       0.62      0.42      0.50        12\n          42       0.77      0.85      0.81        27\n          43       0.09      1.00      0.17         9\n          44       1.00      1.00      1.00         3\n          45       0.37      0.74      0.49        19\n          46       0.96      0.92      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       1.00      0.80      0.89         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.25      0.40      0.31         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.30      0.86      0.44        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.18      0.75      0.29         4\n          73       0.92      0.94      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.43      1.00      0.60         6\n          77       0.92      1.00      0.96        11\n          78       0.61      0.98      0.75        53\n          79       0.75      1.00      0.86         6\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       1.00      0.83      0.91         6\n          86       0.47      1.00      0.64         7\n          87       0.75      1.00      0.86        27\n          88       0.44      1.00      0.61         7\n          89       0.10      1.00      0.17         2\n          90       0.79      0.92      0.85        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.92      0.90        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.24      0.83      0.37         6\n         103       0.68      0.75      0.71        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         114       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.73      1.00      0.84         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.59      0.83      0.60     10056\nweighted avg       0.96      0.92      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.77      1.00      0.87        20\n           3       0.96      0.97      0.97       101\n           4       0.38      1.00      0.55         8\n           5       0.11      1.00      0.20         5\n           6       1.00      1.00      1.00         1\n           7       0.56      1.00      0.71        10\n           8       1.00      1.00      1.00         3\n           9       0.33      0.67      0.44         3\n          11       0.25      0.67      0.36         3\n          13       0.54      1.00      0.70         7\n          14       0.27      0.80      0.40         5\n          15       0.85      1.00      0.92        11\n          16       0.50      1.00      0.67        11\n          17       0.23      0.72      0.35        46\n          18       0.91      0.88      0.89        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.99      0.91      0.95       102\n          26       0.94      0.87      0.91        39\n          27       0.88      0.92      0.90        39\n          28       0.42      0.92      0.58        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.82      0.78      0.80        68\n          33       0.03      1.00      0.06         1\n          34       0.96      0.79      0.87        34\n          35       1.00      0.81      0.89        31\n          36       1.00      1.00      1.00         5\n          37       0.20      0.50      0.29         2\n          38       0.48      1.00      0.65        12\n          39       0.86      0.86      0.86         7\n          40       0.87      0.94      0.90        49\n          41       0.71      0.42      0.53        12\n          42       0.85      0.85      0.85        27\n          43       0.06      1.00      0.11         9\n          44       0.60      1.00      0.75         3\n          45       0.37      0.79      0.50        19\n          46       0.94      0.91      0.93       656\n          47       1.00      0.80      0.89         5\n          48       0.54      1.00      0.70         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.75      0.86      0.80         7\n          52       0.12      0.20      0.15         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.31      1.00      0.47        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.38      0.75      0.50         4\n          73       0.91      0.93      0.92       567\n          75       1.00      1.00      1.00        13\n          76       0.35      1.00      0.52         6\n          77       0.73      1.00      0.85        11\n          78       0.63      0.98      0.77        53\n          79       0.86      1.00      0.92         6\n          81       0.50      1.00      0.67         2\n          83       0.33      0.50      0.40         2\n          84       0.60      1.00      0.75         6\n          86       0.39      1.00      0.56         7\n          87       0.69      1.00      0.82        27\n          88       0.32      1.00      0.48         7\n          89       0.67      1.00      0.80         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.83      0.83      0.83        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.17      1.00      0.29         2\n         102       0.38      0.83      0.53         6\n         103       0.75      0.90      0.82        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.87      0.93      5640\n\n    accuracy                           0.91     10056\n   macro avg       0.58      0.89      0.62     10056\nweighted avg       0.96      0.91      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_128,"{'name': 'sequential_38', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_25'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_25', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_25', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_25', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.982928067445755,0.28715282678604126,0.053222766146063805,0.9773277044296265,0.25584155321121216,0.1203894205391407,2595.5,663.5,352.5,719.0,4735.5,0.660288134052044,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.77      1.00      0.87        20\n           3       0.99      0.96      0.97       101\n           4       0.89      1.00      0.94         8\n           5       0.21      1.00      0.34         5\n           6       1.00      1.00      1.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      0.67      0.80         3\n           9       0.17      0.67      0.27         3\n          11       0.12      0.67      0.21         3\n          13       0.88      1.00      0.93         7\n          14       0.36      0.80      0.50         5\n          15       1.00      0.91      0.95        11\n          16       0.42      1.00      0.59        11\n          17       0.29      0.72      0.42        46\n          18       0.84      0.97      0.90        33\n          19       0.75      1.00      0.86         6\n          20       1.00      1.00      1.00        61\n          24       1.00      0.92      0.96        12\n          25       0.99      0.98      0.99       102\n          26       0.92      0.90      0.91        39\n          27       0.90      0.97      0.94        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.60      1.00      0.75         9\n          32       0.95      0.84      0.89        68\n          33       1.00      1.00      1.00         1\n          34       1.00      0.88      0.94        34\n          35       0.93      0.84      0.88        31\n          36       1.00      1.00      1.00         5\n          37       0.14      0.50      0.22         2\n          38       0.50      1.00      0.67        12\n          39       0.75      0.86      0.80         7\n          40       0.91      0.88      0.90        49\n          41       0.71      0.42      0.53        12\n          42       0.62      0.85      0.72        27\n          43       0.20      0.89      0.32         9\n          44       1.00      1.00      1.00         3\n          45       0.54      0.79      0.64        19\n          46       0.96      0.94      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.70      1.00      0.82         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.86      0.86      0.86         7\n          52       0.12      0.20      0.15         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.47      1.00      0.64        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.05      1.00      0.10         4\n          73       0.93      0.84      0.88       567\n          75       0.93      1.00      0.96        13\n          76       0.35      1.00      0.52         6\n          77       0.52      1.00      0.69        11\n          78       0.70      0.98      0.82        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       0.50      1.00      0.67         2\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.54      1.00      0.70         7\n          89       0.67      1.00      0.80         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.96      0.96      0.96        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.46      1.00      0.63         6\n         103       0.77      0.85      0.81        20\n         104       0.95      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.93      0.98      0.96        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.62      1.00      0.76         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.64      0.88      0.67     10056\nweighted avg       0.96      0.93      0.95     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.87      1.00      0.93        20\n           3       0.99      0.97      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.18      1.00      0.30         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       1.00      1.00      1.00         3\n           9       0.15      0.67      0.25         3\n          11       0.43      1.00      0.60         3\n          13       0.88      1.00      0.93         7\n          14       0.36      0.80      0.50         5\n          15       0.83      0.91      0.87        11\n          16       0.55      1.00      0.71        11\n          17       0.30      0.74      0.42        46\n          18       0.84      0.97      0.90        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.99      0.98      0.99       102\n          26       0.97      0.85      0.90        39\n          27       0.90      0.97      0.94        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.20      1.00      0.33         1\n          31       0.67      0.89      0.76         9\n          32       0.93      0.84      0.88        68\n          33       0.50      1.00      0.67         1\n          34       0.93      0.76      0.84        34\n          35       0.90      0.84      0.87        31\n          36       1.00      1.00      1.00         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.91      0.88      0.90        49\n          41       0.83      0.42      0.56        12\n          42       0.88      0.85      0.87        27\n          43       0.17      1.00      0.29         9\n          44       1.00      1.00      1.00         3\n          45       0.50      0.79      0.61        19\n          46       0.95      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.88      1.00      0.93         7\n          49       0.83      1.00      0.91         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.06      0.20      0.10         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.57      0.93      0.70        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      0.75      0.38         4\n          73       0.95      0.95      0.95       567\n          75       1.00      1.00      1.00        13\n          76       0.42      0.83      0.56         6\n          77       0.92      1.00      0.96        11\n          78       0.73      0.98      0.84        53\n          79       0.75      1.00      0.86         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       1.00      1.00      1.00         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.50      0.83      0.62         6\n         103       0.74      0.85      0.79        20\n         104       0.94      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         115       0.71      1.00      0.83         5\n         116       0.98      0.98      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.64      0.90      0.68     10056\nweighted avg       0.97      0.94      0.95     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_256,"{'name': 'sequential_40', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_26'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_26', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_26', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_26', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}], 'build_input_shape': (None, 46)}",0.9197522103786469,0.17114140093326569,0.8974196314811707,0.9042097628116608,0.14554592221975327,0.31015175580978394,879.0,2615.0,2069.0,1897.0,9848.5,0.31711115975964066,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.65      1.00      0.78        20\n           3       0.99      0.89      0.94       101\n           4       1.00      1.00      1.00         8\n           5       0.03      0.20      0.05         5\n           6       1.00      0.00      0.00         1\n           7       0.00      0.00      0.00        10\n           8       1.00      0.00      0.00         3\n           9       0.11      0.33      0.17         3\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       1.00      0.00      0.00         7\n          14       1.00      0.00      0.00         5\n          15       0.43      0.55      0.48        11\n          16       0.31      0.73      0.43        11\n          17       0.01      0.04      0.01        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.93      0.25      0.40       102\n          26       1.00      0.33      0.50        39\n          27       0.83      0.49      0.61        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       1.00      0.01      0.03        68\n          33       0.00      0.00      0.00         1\n          34       0.67      0.18      0.28        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.31      0.46        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       1.00      0.00      0.00         3\n          45       0.02      0.05      0.03        19\n          46       0.61      0.28      0.38       656\n          47       1.00      0.00      0.00         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.14      1.00      0.25         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.25      0.50      0.33         4\n          73       0.62      0.26      0.37       567\n          75       1.00      0.00      0.00        13\n          76       0.09      0.83      0.16         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       0.20      1.00      0.33         6\n          80       0.00      1.00      0.00         0\n          81       0.05      0.50      0.09         2\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          86       0.00      0.00      0.00         7\n          87       1.00      0.00      0.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          93       0.07      1.00      0.13         2\n          95       0.00      0.00      0.00        24\n          96       0.00      1.00      0.00         0\n          97       0.04      1.00      0.07         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.01      0.83      0.02         6\n         103       1.00      0.00      0.00        20\n         104       0.55      0.15      0.24       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.33      0.40      0.36         5\n         116       0.66      0.58      0.61       132\n         117       0.67      1.00      0.80         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.70     10056\n   macro avg       0.48      0.62      0.35     10056\nweighted avg       0.91      0.70      0.77     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.65      1.00      0.78        20\n           3       0.99      0.89      0.94       101\n           4       1.00      1.00      1.00         8\n           5       0.00      0.00      0.00         5\n           6       1.00      0.00      0.00         1\n           7       0.10      0.30      0.15        10\n           8       1.00      0.00      0.00         3\n           9       0.11      0.33      0.17         3\n          10       0.00      1.00      0.00         0\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       1.00      0.00      0.00         7\n          14       0.00      0.00      0.00         5\n          15       0.43      0.27      0.33        11\n          16       0.08      0.09      0.09        11\n          17       0.01      0.04      0.02        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.93      0.25      0.40       102\n          26       1.00      0.33      0.50        39\n          27       0.83      0.49      0.61        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       1.00      0.01      0.03        68\n          33       0.00      0.00      0.00         1\n          34       0.40      0.06      0.10        34\n          35       0.50      0.23      0.31        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.31      0.46        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       0.67      0.67      0.67         3\n          45       0.02      0.05      0.03        19\n          46       0.67      0.18      0.28       656\n          47       0.24      1.00      0.38         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      0.50      0.50         2\n          72       0.23      0.75      0.35         4\n          73       0.62      0.26      0.37       567\n          75       1.00      0.00      0.00        13\n          76       0.09      0.83      0.16         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       0.14      0.67      0.24         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          85       0.00      1.00      0.00         0\n          86       0.00      0.00      0.00         7\n          87       1.00      0.00      0.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          92       0.00      1.00      0.00         0\n          93       1.00      0.00      0.00         2\n          95       0.00      0.00      0.00        24\n          97       0.04      1.00      0.07         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.01      0.50      0.03         2\n         102       0.01      0.67      0.02         6\n         103       1.00      0.00      0.00        20\n         104       0.55      0.21      0.30       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.65      0.54      0.59       132\n         117       0.67      1.00      0.80         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.69     10056\n   macro avg       0.46      0.60      0.33     10056\nweighted avg       0.91      0.69      0.76     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_256,"{'name': 'sequential_1', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9802975356578827,0.28064411878585815,0.04415805824100971,0.9748315811157227,0.2511018067598343,0.13609831035137177,2622.0,769.5,326.0,894.5,5278.0,0.634646893316481,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.72      0.90      0.80        20\n           3       0.95      0.97      0.96       101\n           4       1.00      1.00      1.00         8\n           5       0.26      1.00      0.42         5\n           6       1.00      0.00      0.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      1.00      1.00         3\n           9       0.33      1.00      0.50         3\n          11       0.40      0.67      0.50         3\n          13       0.88      1.00      0.93         7\n          14       0.60      0.60      0.60         5\n          15       0.77      0.91      0.83        11\n          16       0.73      1.00      0.85        11\n          17       0.27      0.74      0.39        46\n          18       0.85      1.00      0.92        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          22       0.00      1.00      0.00         0\n          24       0.71      0.83      0.77        12\n          25       0.96      0.98      0.97       102\n          26       0.95      0.90      0.92        39\n          27       0.93      0.95      0.94        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.46      0.67      0.55         9\n          32       0.95      0.82      0.88        68\n          33       1.00      1.00      1.00         1\n          34       0.93      0.82      0.88        34\n          35       0.80      0.90      0.85        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.48      1.00      0.65        12\n          39       0.75      0.86      0.80         7\n          40       0.95      0.86      0.90        49\n          41       0.56      0.42      0.48        12\n          42       0.77      0.85      0.81        27\n          43       0.26      1.00      0.42         9\n          44       1.00      1.00      1.00         3\n          45       0.56      0.79      0.65        19\n          46       0.93      0.94      0.93       656\n          47       1.00      0.80      0.89         5\n          48       0.88      1.00      0.93         7\n          49       0.50      1.00      0.67         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.67      1.00      0.80         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.41      0.93      0.57        14\n          71       1.00      1.00      1.00         2\n          72       0.12      0.75      0.21         4\n          73       0.88      0.95      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.43      1.00      0.60         6\n          77       0.85      1.00      0.92        11\n          78       0.65      0.98      0.78        53\n          79       0.46      1.00      0.63         6\n          81       0.67      1.00      0.80         2\n          83       0.00      0.00      0.00         2\n          84       0.86      1.00      0.92         6\n          86       0.43      0.86      0.57         7\n          87       0.90      1.00      0.95        27\n          88       0.70      1.00      0.82         7\n          89       1.00      1.00      1.00         2\n          90       0.69      0.92      0.79        12\n          91       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.74      0.96      0.84        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.35      1.00      0.52         6\n         103       0.73      0.80      0.76        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.84      0.98      0.90        58\n         115       0.42      1.00      0.59         5\n         116       0.94      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.64      0.87      0.66     10056\nweighted avg       0.95      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.77      1.00      0.87        20\n           3       0.97      0.97      0.97       101\n           4       0.80      1.00      0.89         8\n           5       0.23      1.00      0.37         5\n           6       1.00      1.00      1.00         1\n           7       0.77      1.00      0.87        10\n           8       0.75      1.00      0.86         3\n           9       0.33      1.00      0.50         3\n          11       0.43      1.00      0.60         3\n          12       0.00      1.00      0.00         0\n          13       0.88      1.00      0.93         7\n          14       0.44      0.80      0.57         5\n          15       1.00      0.91      0.95        11\n          16       0.52      1.00      0.69        11\n          17       0.30      0.76      0.43        46\n          18       0.86      0.91      0.88        33\n          19       0.83      0.83      0.83         6\n          20       0.85      1.00      0.92        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.97      0.98      0.98       102\n          26       0.95      0.90      0.92        39\n          27       0.90      0.95      0.93        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.60      0.67      0.63         9\n          32       0.88      0.84      0.86        68\n          33       1.00      1.00      1.00         1\n          34       1.00      0.85      0.92        34\n          35       0.90      0.90      0.90        31\n          36       0.62      1.00      0.77         5\n          37       0.25      0.50      0.33         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.90      0.92      0.91        49\n          41       0.62      0.42      0.50        12\n          42       0.77      0.85      0.81        27\n          43       0.12      1.00      0.21         9\n          44       0.60      1.00      0.75         3\n          45       0.39      0.84      0.53        19\n          46       0.95      0.95      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      0.86      0.92         7\n          49       0.71      1.00      0.83         5\n          50       0.06      1.00      0.11         1\n          51       0.60      0.86      0.71         7\n          52       0.11      0.20      0.14         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.75      1.00      0.86         6\n          62       0.93      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.42      1.00      0.60        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.50      0.75      0.60         4\n          73       0.88      0.96      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.46      1.00      0.63         6\n          77       0.46      1.00      0.63        11\n          78       0.68      0.98      0.81        53\n          79       0.67      1.00      0.80         6\n          81       0.40      1.00      0.57         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.58      1.00      0.74         7\n          87       0.77      1.00      0.87        27\n          88       0.58      1.00      0.74         7\n          89       0.67      1.00      0.80         2\n          90       0.65      0.92      0.76        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.96      0.85        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.17      0.50      0.25         2\n         102       0.23      0.83      0.36         6\n         103       0.62      0.90      0.73        20\n         104       0.94      0.97      0.96       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      0.60      0.55         5\n         116       0.95      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.58      0.90      0.62     10056\nweighted avg       0.95      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_256,"{'name': 'sequential_5', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_3'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_3', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9837755858898163,0.2888098508119583,0.0320755522698164,0.9780548214912415,0.25646474957466125,0.12006412073969841,2625.0,626.0,323.0,714.0,4485.0,0.6378712757403193,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.29      1.00      0.44         2\n           2       0.80      1.00      0.89        20\n           3       0.99      0.96      0.97       101\n           4       0.13      1.00      0.23         8\n           5       0.25      1.00      0.40         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       1.00      0.67      0.80         3\n           9       0.40      0.67      0.50         3\n          11       0.33      0.67      0.44         3\n          13       0.75      0.86      0.80         7\n          14       0.33      0.80      0.47         5\n          15       0.83      0.91      0.87        11\n          16       0.52      1.00      0.69        11\n          17       0.40      0.74      0.52        46\n          18       0.88      0.91      0.90        33\n          19       0.83      0.83      0.83         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.98      0.99       102\n          26       0.92      0.87      0.89        39\n          27       0.86      0.95      0.90        39\n          28       0.61      0.92      0.73        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.67      0.89      0.76         9\n          32       0.93      0.84      0.88        68\n          33       1.00      1.00      1.00         1\n          34       0.97      0.82      0.89        34\n          35       0.75      0.87      0.81        31\n          36       1.00      1.00      1.00         5\n          37       0.33      0.50      0.40         2\n          38       0.40      1.00      0.57        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.94      0.92        49\n          41       0.83      0.42      0.56        12\n          42       0.79      0.85      0.82        27\n          43       0.12      1.00      0.21         9\n          44       1.00      1.00      1.00         3\n          45       0.58      0.74      0.65        19\n          46       0.96      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.58      1.00      0.74         7\n          49       0.62      1.00      0.77         5\n          50       0.14      1.00      0.25         1\n          51       0.86      0.86      0.86         7\n          52       0.10      0.20      0.13         5\n          53       0.00      1.00      0.00         0\n          54       0.80      0.67      0.73         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.59      0.93      0.72        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.92      0.96      0.94       567\n          75       1.00      1.00      1.00        13\n          76       0.46      1.00      0.63         6\n          77       0.85      1.00      0.92        11\n          78       0.80      0.98      0.88        53\n          79       0.75      1.00      0.86         6\n          81       0.33      0.50      0.40         2\n          83       0.00      0.00      0.00         2\n          84       0.86      1.00      0.92         6\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.37      1.00      0.54         7\n          89       0.10      1.00      0.18         2\n          90       0.65      0.92      0.76        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.17      1.00      0.29         6\n         103       0.68      0.75      0.71        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.95      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.62      0.88      0.64     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      0.90      0.92        20\n           3       0.97      0.99      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.17      0.80      0.28         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       0.67      0.67      0.67         3\n           9       0.50      0.67      0.57         3\n          11       0.43      1.00      0.60         3\n          13       1.00      0.86      0.92         7\n          14       0.44      0.80      0.57         5\n          15       1.00      0.91      0.95        11\n          16       0.52      1.00      0.69        11\n          17       0.37      0.74      0.49        46\n          18       0.85      1.00      0.92        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.98      0.99       102\n          26       0.92      0.87      0.89        39\n          27       0.88      0.97      0.93        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.67      0.89      0.76         9\n          32       0.89      0.84      0.86        68\n          33       1.00      1.00      1.00         1\n          34       0.97      0.82      0.89        34\n          35       0.76      0.84      0.80        31\n          36       1.00      1.00      1.00         5\n          37       0.25      0.50      0.33         2\n          38       0.50      1.00      0.67        12\n          39       0.86      0.86      0.86         7\n          40       0.93      0.88      0.91        49\n          41       0.71      0.42      0.53        12\n          42       0.82      0.85      0.84        27\n          43       0.21      1.00      0.35         9\n          44       0.50      1.00      0.67         3\n          45       0.58      0.74      0.65        19\n          46       0.96      0.94      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.75      0.86      0.80         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.11      0.20      0.14         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.94      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.50      1.00      0.67        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.11      1.00      0.19         4\n          73       0.95      0.92      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.50      1.00      0.67         6\n          77       0.85      1.00      0.92        11\n          78       0.75      0.98      0.85        53\n          79       0.83      0.83      0.83         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.40      0.86      0.55         7\n          87       1.00      1.00      1.00        27\n          88       0.78      1.00      0.88         7\n          89       1.00      1.00      1.00         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.82      0.96      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.50      1.00      0.67         6\n         103       0.65      0.85      0.74        20\n         104       0.97      0.95      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.98      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.94      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.66      0.89      0.68     10056\nweighted avg       0.97      0.95      0.95     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_16,"{'name': 'sequential_8', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_5'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_5', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_5', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9264744818210602,0.2335314154624939,0.5692843496799469,0.9263971745967865,0.2086542546749115,0.41840480268001556,2470.5,2023.5,477.5,2096.5,8928.0,0.48179553688377685,"['              precision    recall  f1-score   support\n\n           0       0.99      0.86      0.92      1702\n           1       0.14      1.00      0.25         2\n           2       0.65      1.00      0.78        20\n           3       0.87      0.99      0.93       101\n           4       0.16      1.00      0.28         8\n           5       0.16      0.60      0.25         5\n           6       1.00      0.00      0.00         1\n           7       0.22      0.80      0.34        10\n           8       0.33      0.67      0.44         3\n           9       0.18      1.00      0.30         3\n          11       0.33      0.33      0.33         3\n          13       0.17      0.29      0.21         7\n          14       0.25      0.20      0.22         5\n          15       0.69      0.82      0.75        11\n          16       0.33      0.91      0.49        11\n          17       0.18      0.78      0.29        46\n          18       0.87      1.00      0.93        33\n          19       0.36      0.83      0.50         6\n          20       0.63      1.00      0.77        61\n          23       0.00      1.00      0.00         0\n          24       0.52      1.00      0.69        12\n          25       0.53      0.87      0.66       102\n          26       0.81      0.74      0.77        39\n          27       0.87      0.85      0.86        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.33      0.78      0.47         9\n          32       0.57      0.69      0.63        68\n          33       0.00      0.00      0.00         1\n          34       0.83      0.71      0.76        34\n          35       0.81      0.84      0.83        31\n          36       0.56      1.00      0.71         5\n          37       0.33      0.50      0.40         2\n          38       0.24      1.00      0.39        12\n          39       0.86      0.86      0.86         7\n          40       0.49      0.96      0.65        49\n          41       0.25      0.42      0.31        12\n          42       0.74      0.85      0.79        27\n          43       0.07      1.00      0.13         9\n          44       0.43      1.00      0.60         3\n          45       0.30      0.89      0.45        19\n          46       0.84      0.91      0.87       656\n          47       1.00      1.00      1.00         5\n          48       0.44      1.00      0.61         7\n          49       0.25      1.00      0.40         5\n          50       0.20      1.00      0.33         1\n          51       0.54      1.00      0.70         7\n          52       0.10      0.60      0.18         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.40      1.00      0.57         6\n          57       0.00      1.00      0.00         0\n          62       0.84      0.98      0.91        64\n          63       0.50      0.33      0.40         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.47        14\n          68       0.00      1.00      0.00         0\n          71       0.18      1.00      0.31         2\n          72       0.25      1.00      0.40         4\n          73       0.75      0.94      0.84       567\n          74       0.00      1.00      0.00         0\n          75       0.62      1.00      0.76        13\n          76       0.06      1.00      0.11         6\n          77       0.59      0.91      0.71        11\n          78       0.56      0.98      0.71        53\n          79       0.42      0.83      0.56         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.23      0.86      0.36         7\n          87       0.77      1.00      0.87        27\n          88       0.26      1.00      0.41         7\n          89       0.50      1.00      0.67         2\n          90       0.40      0.83      0.54        12\n          92       0.00      1.00      0.00         0\n          93       0.14      0.50      0.22         2\n          94       0.00      1.00      0.00         0\n          95       0.58      0.92      0.71        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.09      0.50      0.15         2\n         102       0.12      0.67      0.21         6\n         103       0.36      0.65      0.46        20\n         104       0.95      0.98      0.96       110\n         105       0.33      1.00      0.50         2\n         106       1.00      0.00      0.00         1\n         108       0.25      1.00      0.40         2\n         111       0.71      0.98      0.83        58\n         112       0.00      1.00      0.00         0\n         115       0.29      1.00      0.45         5\n         116       0.90      0.96      0.93       132\n         117       1.00      1.00      1.00         2\n         118       0.39      0.88      0.54         8\n         119       0.99      0.77      0.87      5640\n\n    accuracy                           0.82     10056\n   macro avg       0.43      0.82      0.47     10056\nweighted avg       0.92      0.82      0.86     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.87      0.93      1702\n           1       0.17      1.00      0.29         2\n           2       0.54      0.95      0.69        20\n           3       0.76      0.99      0.86       101\n           4       0.24      1.00      0.38         8\n           5       0.25      0.80      0.38         5\n           6       0.50      1.00      0.67         1\n           7       0.27      1.00      0.43        10\n           8       0.22      0.67      0.33         3\n           9       0.22      0.67      0.33         3\n          11       0.25      0.33      0.29         3\n          12       0.00      1.00      0.00         0\n          13       0.43      0.86      0.57         7\n          14       0.25      0.20      0.22         5\n          15       0.56      0.82      0.67        11\n          16       0.53      0.91      0.67        11\n          17       0.26      0.78      0.39        46\n          18       0.67      1.00      0.80        33\n          19       0.46      1.00      0.63         6\n          20       0.73      1.00      0.84        61\n          21       0.00      1.00      0.00         0\n          24       0.75      1.00      0.86        12\n          25       0.35      0.98      0.52       102\n          26       0.77      0.77      0.77        39\n          27       0.47      0.97      0.63        39\n          28       0.62      0.83      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.28      0.56      0.37         9\n          32       0.76      0.79      0.78        68\n          33       0.00      0.00      0.00         1\n          34       0.67      0.76      0.71        34\n          35       0.80      0.90      0.85        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.28      1.00      0.44        12\n          39       0.67      0.86      0.75         7\n          40       0.55      0.96      0.70        49\n          41       0.20      0.42      0.27        12\n          42       0.82      0.85      0.84        27\n          43       0.12      1.00      0.22         9\n          44       0.60      1.00      0.75         3\n          45       0.35      0.74      0.47        19\n          46       0.85      0.91      0.88       656\n          47       0.44      0.80      0.57         5\n          48       0.43      0.86      0.57         7\n          49       0.83      1.00      0.91         5\n          50       1.00      0.00      0.00         1\n          51       0.46      0.86      0.60         7\n          52       0.15      0.40      0.22         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.40      1.00      0.57         6\n          57       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.64      0.98      0.78        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.46        14\n          70       0.00      1.00      0.00         0\n          71       0.03      1.00      0.07         2\n          72       0.20      1.00      0.33         4\n          73       0.46      0.94      0.62       567\n          75       0.81      1.00      0.90        13\n          76       0.38      0.83      0.53         6\n          77       0.41      0.82      0.55        11\n          78       0.49      0.98      0.65        53\n          79       0.26      0.83      0.40         6\n          80       0.00      1.00      0.00         0\n          81       0.20      0.50      0.29         2\n          83       0.67      1.00      0.80         2\n          84       0.55      1.00      0.71         6\n          86       0.25      0.71      0.37         7\n          87       0.87      1.00      0.93        27\n          88       0.33      1.00      0.50         7\n          89       0.17      1.00      0.29         2\n          90       0.44      0.92      0.59        12\n          92       0.00      1.00      0.00         0\n          93       0.40      1.00      0.57         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.10      0.67      0.17         6\n         103       0.50      0.70      0.58        20\n         104       0.88      0.91      0.89       110\n         105       0.33      0.50      0.40         2\n         106       1.00      0.00      0.00         1\n         108       0.40      1.00      0.57         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.31      0.80      0.44         5\n         116       0.83      0.98      0.90       132\n         117       0.40      1.00      0.57         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.70      0.82      5640\n\n    accuracy                           0.78     10056\n   macro avg       0.44      0.82      0.49     10056\nweighted avg       0.90      0.78      0.81     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_32,"{'name': 'sequential_11', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_7'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_7', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_1', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_7', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9610130488872528,0.2566235661506653,0.14340820908546448,0.9587150812149048,0.23309358954429626,0.21562594175338745,2610.5,1020.0,337.5,1151.5,5994.0,0.589791513876057,"['              precision    recall  f1-score   support\n\n           0       0.99      0.91      0.95      1702\n           1       0.40      1.00      0.57         2\n           2       0.71      1.00      0.83        20\n           3       0.90      0.99      0.94       101\n           4       0.28      1.00      0.43         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.26      1.00      0.41        10\n           8       0.40      0.67      0.50         3\n           9       0.22      0.67      0.33         3\n          11       0.20      0.33      0.25         3\n          13       0.44      1.00      0.61         7\n          14       0.29      0.40      0.33         5\n          15       0.77      0.91      0.83        11\n          16       0.73      1.00      0.85        11\n          17       0.31      0.83      0.46        46\n          18       0.94      1.00      0.97        33\n          19       0.62      0.83      0.71         6\n          20       1.00      1.00      1.00        61\n          23       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.75      0.98      0.85       102\n          26       0.89      0.87      0.88        39\n          27       0.79      0.95      0.86        39\n          28       0.61      0.92      0.73        12\n          29       0.50      1.00      0.67         1\n          30       0.00      0.00      0.00         1\n          31       0.46      0.67      0.55         9\n          32       0.70      0.85      0.77        68\n          33       0.33      1.00      0.50         1\n          34       0.97      0.82      0.89        34\n          35       0.81      0.94      0.87        31\n          36       0.71      1.00      0.83         5\n          37       0.50      0.50      0.50         2\n          38       0.36      1.00      0.53        12\n          39       0.67      0.86      0.75         7\n          40       0.78      0.96      0.86        49\n          41       0.56      0.42      0.48        12\n          42       0.82      0.85      0.84        27\n          43       0.29      1.00      0.45         9\n          44       0.60      1.00      0.75         3\n          45       0.40      0.89      0.55        19\n          46       0.92      0.94      0.93       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.83      1.00      0.91         5\n          50       1.00      1.00      1.00         1\n          51       0.55      0.86      0.67         7\n          52       0.19      0.60      0.29         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.28      0.93      0.43        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.33      1.00      0.50         2\n          72       0.31      1.00      0.47         4\n          73       0.83      0.95      0.89       567\n          75       1.00      1.00      1.00        13\n          76       0.55      1.00      0.71         6\n          77       0.42      1.00      0.59        11\n          78       0.57      0.98      0.72        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.75      1.00      0.86         6\n          86       0.35      1.00      0.52         7\n          87       0.93      1.00      0.96        27\n          88       0.35      1.00      0.52         7\n          89       0.67      1.00      0.80         2\n          90       0.50      0.92      0.65        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.36      0.67      0.47         6\n         103       0.62      0.75      0.68        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.50      1.00      0.67         2\n         111       0.83      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.82      1.00      0.90       132\n         117       0.67      1.00      0.80         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.88      0.93      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.52      0.86      0.57     10056\nweighted avg       0.94      0.90      0.91     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      1702\n           1       0.29      1.00      0.44         2\n           2       0.74      1.00      0.85        20\n           3       0.84      0.99      0.91       101\n           4       0.35      1.00      0.52         8\n           5       0.31      1.00      0.48         5\n           6       1.00      1.00      1.00         1\n           7       0.48      1.00      0.65        10\n           8       0.33      1.00      0.50         3\n           9       0.60      1.00      0.75         3\n          11       0.43      1.00      0.60         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.86      0.63         7\n          14       0.50      0.80      0.62         5\n          15       0.79      1.00      0.88        11\n          16       0.48      1.00      0.65        11\n          17       0.22      0.80      0.34        46\n          18       0.97      1.00      0.99        33\n          19       0.86      1.00      0.92         6\n          20       0.64      1.00      0.78        61\n          24       0.79      0.92      0.85        12\n          25       0.87      0.97      0.92       102\n          26       0.86      0.92      0.89        39\n          27       0.84      0.97      0.90        39\n          28       0.65      0.92      0.76        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.89      0.64         9\n          32       0.71      0.85      0.77        68\n          33       0.50      1.00      0.67         1\n          34       0.85      0.82      0.84        34\n          35       0.90      0.90      0.90        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.25      1.00      0.40        12\n          39       0.86      0.86      0.86         7\n          40       0.84      0.98      0.91        49\n          41       0.45      0.42      0.43        12\n          42       0.82      0.85      0.84        27\n          43       0.19      1.00      0.32         9\n          44       0.75      1.00      0.86         3\n          45       0.29      0.79      0.43        19\n          46       0.87      0.93      0.90       656\n          47       1.00      1.00      1.00         5\n          48       0.54      1.00      0.70         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.55      0.86      0.67         7\n          52       0.25      0.60      0.35         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.29      0.93      0.44        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.25      1.00      0.40         4\n          73       0.85      0.95      0.90       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       1.00      1.00      1.00         6\n          77       0.58      1.00      0.73        11\n          78       0.55      0.98      0.70        53\n          79       0.55      1.00      0.71         6\n          81       0.50      1.00      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.33      0.50      0.40         2\n          84       1.00      1.00      1.00         6\n          86       0.37      1.00      0.54         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.50      1.00      0.67         2\n          90       0.46      1.00      0.63        12\n          92       0.00      1.00      0.00         0\n          93       0.33      1.00      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.28      0.83      0.42         6\n         103       0.62      0.65      0.63        20\n         104       0.94      0.97      0.96       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.88      0.98      0.93        58\n         115       0.45      1.00      0.62         5\n         116       0.92      0.98      0.95       132\n         117       0.50      1.00      0.67         2\n         118       0.37      0.88      0.52         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.55      0.89      0.61     10056\nweighted avg       0.94      0.89      0.91     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_64,"{'name': 'sequential_14', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_9'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_9', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_2', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_9', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9779146611690521,0.2768417149782181,0.05660764314234257,0.9744195342063904,0.2523820102214813,0.12525349482893944,2673.0,660.0,275.0,821.5,4785.5,0.6760521014452431,"['              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.97      1702\n           1       0.40      1.00      0.57         2\n           2       0.77      1.00      0.87        20\n           3       0.96      0.99      0.98       101\n           4       0.50      1.00      0.67         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.59      1.00      0.74        10\n           8       0.75      1.00      0.86         3\n           9       0.67      0.67      0.67         3\n          11       0.75      1.00      0.86         3\n          13       0.70      1.00      0.82         7\n          14       0.38      0.60      0.46         5\n          15       0.85      1.00      0.92        11\n          16       0.65      1.00      0.79        11\n          17       0.41      0.85      0.55        46\n          18       0.97      0.97      0.97        33\n          19       0.83      0.83      0.83         6\n          20       0.98      1.00      0.99        61\n          24       0.86      1.00      0.92        12\n          25       0.97      0.98      0.98       102\n          26       0.88      0.92      0.90        39\n          27       0.77      0.95      0.85        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.67      0.89      0.76         9\n          32       0.78      0.87      0.82        68\n          33       0.00      0.00      0.00         1\n          34       0.93      0.82      0.88        34\n          35       0.94      0.94      0.94        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.33      1.00      0.50        12\n          39       0.86      0.86      0.86         7\n          40       0.87      0.98      0.92        49\n          41       0.56      0.42      0.48        12\n          42       0.82      0.85      0.84        27\n          43       0.39      1.00      0.56         9\n          44       1.00      1.00      1.00         3\n          45       0.44      0.84      0.58        19\n          46       0.96      0.96      0.96       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       0.71      1.00      0.83         5\n          50       0.50      1.00      0.67         1\n          51       0.50      0.86      0.63         7\n          52       0.33      0.60      0.43         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.38      1.00      0.55        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.29      1.00      0.44         4\n          73       0.94      0.96      0.95       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       1.00      1.00      1.00         6\n          77       0.61      1.00      0.76        11\n          78       0.59      0.98      0.74        53\n          79       0.75      1.00      0.86         6\n          81       0.67      1.00      0.80         2\n          82       0.00      1.00      0.00         0\n          83       0.40      1.00      0.57         2\n          84       1.00      1.00      1.00         6\n          86       0.18      1.00      0.30         7\n          87       1.00      1.00      1.00        27\n          88       0.41      1.00      0.58         7\n          89       0.67      1.00      0.80         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.96      0.92        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.38      1.00      0.55         6\n         103       0.76      0.80      0.78        20\n         104       0.96      0.98      0.97       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.29      1.00      0.44         2\n         111       0.89      0.98      0.93        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.64      0.90      0.69     10056\nweighted avg       0.96      0.94      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.87      1.00      0.93        20\n           3       0.95      0.99      0.97       101\n           4       0.50      1.00      0.67         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.53      1.00      0.69        10\n           8       1.00      1.00      1.00         3\n           9       0.67      0.67      0.67         3\n          11       0.43      1.00      0.60         3\n          13       0.64      1.00      0.78         7\n          14       0.50      0.60      0.55         5\n          15       0.69      1.00      0.81        11\n          16       0.65      1.00      0.79        11\n          17       0.34      0.83      0.48        46\n          18       0.94      1.00      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.92      1.00      0.96        61\n          23       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.97      0.98      0.98       102\n          26       0.92      0.87      0.89        39\n          27       0.78      0.97      0.86        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.56      1.00      0.72         9\n          32       0.68      0.84      0.75        68\n          33       0.50      1.00      0.67         1\n          34       0.93      0.79      0.86        34\n          35       0.93      0.90      0.92        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.38      1.00      0.55        12\n          39       0.86      0.86      0.86         7\n          40       0.86      0.98      0.91        49\n          41       0.83      0.42      0.56        12\n          42       0.68      0.85      0.75        27\n          43       0.36      1.00      0.53         9\n          44       1.00      1.00      1.00         3\n          45       0.43      0.95      0.59        19\n          46       0.95      0.96      0.95       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.55      0.86      0.67         7\n          52       0.22      0.40      0.29         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.42      1.00      0.60        14\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.91      0.96      0.93       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.52      1.00      0.69        11\n          78       0.59      0.98      0.74        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.50      1.00      0.67         7\n          87       1.00      1.00      1.00        27\n          88       0.41      1.00      0.58         7\n          89       0.50      0.50      0.50         2\n          90       0.73      0.92      0.81        12\n          93       0.50      1.00      0.67         2\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.40      0.67      0.50         6\n         103       0.78      0.90      0.84        20\n         104       0.94      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.83      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.67      0.90      0.71     10056\nweighted avg       0.96      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_128,"{'name': 'sequential_17', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_11'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_11', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_11', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_11', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9832265377044678,0.28241802752017975,0.032920852303504944,0.9780549108982086,0.25325705111026764,0.10101558640599251,2686.5,571.0,261.5,720.0,4354.0,0.6707250064989598,"['              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.83      1.00      0.91        20\n           3       0.99      0.99      0.99       101\n           4       0.53      1.00      0.70         8\n           5       0.31      1.00      0.48         5\n           6       1.00      1.00      1.00         1\n           7       0.67      1.00      0.80        10\n           8       0.75      1.00      0.86         3\n           9       1.00      1.00      1.00         3\n          11       0.50      0.67      0.57         3\n          13       0.70      1.00      0.82         7\n          14       0.14      0.20      0.17         5\n          15       0.79      1.00      0.88        11\n          16       0.79      1.00      0.88        11\n          17       0.39      0.85      0.54        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.94      1.00      0.97        61\n          24       1.00      1.00      1.00        12\n          25       0.97      0.98      0.98       102\n          26       0.95      0.92      0.94        39\n          27       0.73      0.95      0.82        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.64      1.00      0.78         9\n          32       0.69      0.87      0.77        68\n          33       0.14      1.00      0.25         1\n          34       0.93      0.82      0.88        34\n          35       0.83      0.94      0.88        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.38      1.00      0.55        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.98      0.96        49\n          41       0.62      0.42      0.50        12\n          42       0.79      0.85      0.82        27\n          43       0.43      1.00      0.60         9\n          44       1.00      1.00      1.00         3\n          45       0.40      0.74      0.52        19\n          46       0.96      0.96      0.96       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.83      1.00      0.91         5\n          50       0.00      0.00      0.00         1\n          51       0.46      0.86      0.60         7\n          52       0.29      0.40      0.33         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.45      0.93      0.60        14\n          71       1.00      1.00      1.00         2\n          72       0.19      1.00      0.32         4\n          73       0.94      0.96      0.95       567\n          75       0.93      1.00      0.96        13\n          76       0.86      1.00      0.92         6\n          77       0.37      1.00      0.54        11\n          78       0.61      0.98      0.75        53\n          79       0.50      1.00      0.67         6\n          81       0.50      0.50      0.50         2\n          83       0.00      0.00      0.00         2\n          84       0.67      1.00      0.80         6\n          86       0.44      1.00      0.61         7\n          87       0.93      1.00      0.96        27\n          88       0.70      1.00      0.82         7\n          89       0.67      1.00      0.80         2\n          90       0.58      0.92      0.71        12\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.31      0.67      0.42         6\n         103       0.69      0.55      0.61        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.84      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.96      0.99      0.98       132\n         117       0.67      1.00      0.80         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.63      0.88      0.68     10056\nweighted avg       0.96      0.94      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.95      1.00      0.98        20\n           3       0.97      0.99      0.98       101\n           4       0.73      1.00      0.84         8\n           5       0.36      1.00      0.53         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       0.43      1.00      0.60         3\n           9       0.75      1.00      0.86         3\n          11       0.60      1.00      0.75         3\n          13       0.64      1.00      0.78         7\n          14       0.33      0.60      0.43         5\n          15       0.91      0.91      0.91        11\n          16       0.73      1.00      0.85        11\n          17       0.45      0.87      0.60        46\n          18       1.00      0.97      0.98        33\n          19       0.75      1.00      0.86         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.98      0.98      0.98       102\n          26       0.95      0.90      0.92        39\n          27       0.84      0.95      0.89        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.67      0.89      0.76         9\n          32       0.86      0.87      0.86        68\n          33       0.00      0.00      0.00         1\n          34       0.88      0.82      0.85        34\n          35       1.00      0.90      0.95        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.40      1.00      0.57        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.98      0.95        49\n          41       0.71      0.42      0.53        12\n          42       0.79      0.85      0.82        27\n          43       0.53      1.00      0.69         9\n          44       0.75      1.00      0.86         3\n          45       0.54      0.79      0.64        19\n          46       0.96      0.96      0.96       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.60      0.86      0.71         7\n          52       0.20      0.40      0.27         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.67      1.00      0.80        14\n          68       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.27      1.00      0.42         4\n          73       0.95      0.96      0.95       567\n          75       1.00      1.00      1.00        13\n          76       0.67      1.00      0.80         6\n          77       0.43      0.91      0.59        11\n          78       0.66      0.98      0.79        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          86       0.39      1.00      0.56         7\n          87       0.87      1.00      0.93        27\n          88       0.54      1.00      0.70         7\n          89       0.67      1.00      0.80         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.85      0.96      0.90        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.50      0.67      0.57         6\n         103       0.78      0.70      0.74        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.88      0.98      0.93        58\n         115       0.56      1.00      0.71         5\n         116       0.96      0.99      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.94      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.68      0.90      0.72     10056\nweighted avg       0.96      0.95      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_ave_32,"{'name': 'sequential_20', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_13'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_13', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_4', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'ave', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_13', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_13', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9455561339855194,0.2481125220656395,0.5765954256057739,0.9427318572998047,0.22279930859804153,0.3841131776571274,2545.0,1785.0,403.0,1942.5,8525.0,0.5324844932180076,"['              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1702\n           1       0.15      1.00      0.27         2\n           2       0.61      0.95      0.75        20\n           3       0.95      0.99      0.97       101\n           4       0.31      1.00      0.47         8\n           5       0.27      0.80      0.40         5\n           6       1.00      1.00      1.00         1\n           7       0.50      1.00      0.67        10\n           8       0.29      0.67      0.40         3\n           9       0.50      0.67      0.57         3\n          11       1.00      0.67      0.80         3\n          13       0.60      0.43      0.50         7\n          14       0.14      0.40      0.21         5\n          15       0.79      1.00      0.88        11\n          16       0.25      0.91      0.39        11\n          17       0.19      0.78      0.31        46\n          18       0.73      1.00      0.85        33\n          19       0.67      1.00      0.80         6\n          20       0.86      1.00      0.92        61\n          23       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.82      0.93      0.87       102\n          26       0.89      0.85      0.87        39\n          27       0.72      0.97      0.83        39\n          28       0.71      0.83      0.77        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.89      0.64         9\n          32       0.86      0.75      0.80        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.82      0.86        34\n          35       0.90      0.87      0.89        31\n          36       0.62      1.00      0.77         5\n          37       0.33      0.50      0.40         2\n          38       0.29      1.00      0.45        12\n          39       0.86      0.86      0.86         7\n          40       0.81      0.94      0.87        49\n          41       0.45      0.42      0.43        12\n          42       0.92      0.85      0.88        27\n          43       0.26      1.00      0.41         9\n          44       0.60      1.00      0.75         3\n          45       0.23      0.74      0.35        19\n          46       0.59      0.94      0.72       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.04      1.00      0.07         1\n          51       0.46      0.86      0.60         7\n          52       0.14      0.60      0.23         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.38      1.00      0.55         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.46        14\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.33      1.00      0.50         4\n          73       0.80      0.94      0.86       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.50      1.00      0.67         6\n          77       0.38      0.82      0.51        11\n          78       0.51      0.98      0.68        53\n          79       0.38      0.83      0.53         6\n          80       0.00      1.00      0.00         0\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.50      1.00      0.67         6\n          85       0.00      1.00      0.00         0\n          86       0.19      0.71      0.30         7\n          87       0.84      1.00      0.92        27\n          88       0.29      1.00      0.45         7\n          89       0.10      1.00      0.18         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.17      0.50      0.25         2\n          94       0.00      1.00      0.00         0\n          95       0.68      0.96      0.79        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.14      0.67      0.23         6\n         103       0.40      0.60      0.48        20\n         104       0.91      0.96      0.94       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.86      0.98      0.92        58\n         112       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.94      0.97      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.29      0.75      0.41         8\n         119       0.99      0.77      0.87      5640\n\n    accuracy                           0.84     10056\n   macro avg       0.50      0.84      0.54     10056\nweighted avg       0.92      0.84      0.86     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      1702\n           1       0.22      1.00      0.36         2\n           2       0.57      1.00      0.73        20\n           3       0.78      0.99      0.87       101\n           4       0.18      1.00      0.30         8\n           5       0.09      1.00      0.17         5\n           6       0.00      0.00      0.00         1\n           7       0.50      0.70      0.58        10\n           8       0.33      1.00      0.50         3\n           9       0.06      0.67      0.11         3\n          11       0.14      0.33      0.20         3\n          13       0.20      0.29      0.24         7\n          14       0.17      0.40      0.24         5\n          15       0.42      0.73      0.53        11\n          16       0.37      0.91      0.53        11\n          17       0.22      0.83      0.35        46\n          18       0.94      1.00      0.97        33\n          19       0.55      1.00      0.71         6\n          20       0.86      1.00      0.92        61\n          24       0.69      0.92      0.79        12\n          25       0.78      0.95      0.86       102\n          26       0.94      0.77      0.85        39\n          27       0.61      0.90      0.73        39\n          28       0.62      0.83      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.41      1.00      0.58         9\n          32       0.78      0.75      0.77        68\n          33       0.00      0.00      0.00         1\n          34       0.76      0.82      0.79        34\n          35       0.66      0.87      0.75        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.32      1.00      0.48        12\n          39       0.75      0.86      0.80         7\n          40       0.52      0.98      0.68        49\n          41       0.38      0.42      0.40        12\n          42       0.96      0.85      0.90        27\n          43       0.13      1.00      0.23         9\n          44       0.50      1.00      0.67         3\n          45       0.38      0.89      0.53        19\n          46       0.67      0.92      0.77       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       0.62      1.00      0.77         5\n          50       0.33      1.00      0.50         1\n          51       0.55      0.86      0.67         7\n          52       0.07      0.40      0.12         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.30      1.00      0.46         6\n          57       0.00      1.00      0.00         0\n          62       0.88      0.98      0.93        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.26      1.00      0.42        14\n          71       1.00      1.00      1.00         2\n          72       0.19      0.75      0.30         4\n          73       0.68      0.96      0.80       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.67      1.00      0.80         6\n          77       0.53      0.82      0.64        11\n          78       0.57      0.98      0.72        53\n          79       0.75      1.00      0.86         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.71      0.83      0.77         6\n          85       0.00      1.00      0.00         0\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.30      1.00      0.47         7\n          89       0.10      1.00      0.17         2\n          90       0.73      0.92      0.81        12\n          92       0.00      1.00      0.00         0\n          93       0.33      1.00      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.63      0.92      0.75        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.05      0.50      0.10         2\n         102       0.10      0.67      0.17         6\n         103       0.37      0.65      0.47        20\n         104       0.91      0.95      0.93       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.88      0.98      0.93        58\n         115       0.29      1.00      0.45         5\n         116       0.93      0.98      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.83     10056\n   macro avg       0.49      0.83      0.54     10056\nweighted avg       0.91      0.83      0.85     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_sum_32,"{'name': 'sequential_23', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_15'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_15', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_5', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'sum', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_15', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_15', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9627562165260315,0.26250526309013367,0.18581747263669968,0.9574184417724609,0.23245424032211304,0.22950005531311035,2593.0,1202.5,355.0,1334.0,6500.0,0.5711776095506612,"['              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.97      1702\n           1       0.40      1.00      0.57         2\n           2       0.59      1.00      0.74        20\n           3       0.89      0.99      0.94       101\n           4       0.21      1.00      0.35         8\n           5       0.33      0.80      0.47         5\n           6       1.00      0.00      0.00         1\n           7       0.32      1.00      0.49        10\n           8       1.00      1.00      1.00         3\n           9       0.60      1.00      0.75         3\n          11       0.40      0.67      0.50         3\n          13       0.43      0.86      0.57         7\n          14       0.20      0.20      0.20         5\n          15       0.92      1.00      0.96        11\n          16       0.58      1.00      0.73        11\n          17       0.17      0.78      0.28        46\n          18       0.97      0.97      0.97        33\n          19       0.55      1.00      0.71         6\n          20       0.81      1.00      0.90        61\n          24       0.61      0.92      0.73        12\n          25       0.84      0.94      0.89       102\n          26       0.82      0.85      0.84        39\n          27       0.81      0.97      0.88        39\n          28       0.62      0.83      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.35      0.78      0.48         9\n          32       0.88      0.76      0.82        68\n          33       0.20      1.00      0.33         1\n          34       0.96      0.79      0.87        34\n          35       0.90      0.87      0.89        31\n          36       0.71      1.00      0.83         5\n          37       0.50      0.50      0.50         2\n          38       0.30      1.00      0.46        12\n          39       0.86      0.86      0.86         7\n          40       0.65      0.96      0.78        49\n          41       0.83      0.42      0.56        12\n          42       0.86      0.89      0.87        27\n          43       0.29      1.00      0.45         9\n          44       0.75      1.00      0.86         3\n          45       0.17      0.74      0.27        19\n          46       0.86      0.94      0.90       656\n          47       1.00      1.00      1.00         5\n          48       0.70      1.00      0.82         7\n          49       0.71      1.00      0.83         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.17      0.60      0.26         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.31      1.00      0.47        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.29      1.00      0.44         4\n          73       0.80      0.95      0.87       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.67      1.00      0.80         6\n          77       0.53      0.91      0.67        11\n          78       0.57      0.98      0.72        53\n          79       0.56      0.83      0.67         6\n          81       0.50      0.50      0.50         2\n          83       0.50      1.00      0.67         2\n          84       0.55      1.00      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.26      0.71      0.38         7\n          87       0.96      1.00      0.98        27\n          88       0.41      1.00      0.58         7\n          89       1.00      1.00      1.00         2\n          90       0.79      0.92      0.85        12\n          92       0.00      1.00      0.00         0\n          93       0.29      1.00      0.44         2\n          95       0.70      0.79      0.75        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.44      0.67      0.53         6\n         103       0.58      0.70      0.64        20\n         104       0.95      0.96      0.96       110\n         105       1.00      0.50      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.93      0.98      0.95       132\n         117       0.67      1.00      0.80         2\n         118       0.39      0.88      0.54         8\n         119       0.99      0.85      0.91      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.57      0.87      0.60     10056\nweighted avg       0.94      0.89      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.97      1702\n           1       0.40      1.00      0.57         2\n           2       0.54      1.00      0.70        20\n           3       0.96      0.99      0.98       101\n           4       0.29      1.00      0.44         8\n           5       0.21      1.00      0.34         5\n           6       1.00      1.00      1.00         1\n           7       0.38      1.00      0.56        10\n           8       0.67      0.67      0.67         3\n           9       0.33      0.67      0.44         3\n          11       1.00      0.33      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.26      0.86      0.40         7\n          14       0.12      0.20      0.15         5\n          15       0.85      1.00      0.92        11\n          16       0.42      1.00      0.59        11\n          17       0.22      0.83      0.35        46\n          18       0.94      0.97      0.96        33\n          19       0.62      0.83      0.71         6\n          20       0.97      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.83      0.97      0.90       102\n          26       0.83      0.87      0.85        39\n          27       0.69      0.92      0.79        39\n          28       0.65      0.92      0.76        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.67      0.57         9\n          32       0.67      0.85      0.75        68\n          33       0.00      0.00      0.00         1\n          34       0.88      0.82      0.85        34\n          35       0.68      0.87      0.76        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.32      1.00      0.49        12\n          39       0.86      0.86      0.86         7\n          40       0.86      0.98      0.91        49\n          41       0.56      0.42      0.48        12\n          42       0.82      0.85      0.84        27\n          43       0.24      1.00      0.39         9\n          44       0.38      1.00      0.55         3\n          45       0.31      0.89      0.46        19\n          46       0.88      0.94      0.91       656\n          47       1.00      1.00      1.00         5\n          48       0.70      1.00      0.82         7\n          49       0.56      1.00      0.71         5\n          50       1.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.21      0.60      0.32         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.67      0.67      0.67         3\n          64       1.00      0.00      0.00         1\n          66       0.27      1.00      0.42        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.31      1.00      0.47         4\n          73       0.81      0.96      0.88       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.50      1.00      0.67         6\n          77       0.50      0.91      0.65        11\n          78       0.59      0.98      0.74        53\n          79       0.50      1.00      0.67         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          83       0.33      0.50      0.40         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.19      0.86      0.32         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.16         2\n          90       0.65      0.92      0.76        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.24      0.83      0.37         6\n         103       0.64      0.80      0.71        20\n         104       0.96      0.98      0.97       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.90      0.98      0.94       132\n         117       1.00      1.00      1.00         2\n         118       0.38      0.75      0.50         8\n         119       0.99      0.83      0.91      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.50      0.86      0.53     10056\nweighted avg       0.94      0.88      0.90     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_ave_64,"{'name': 'sequential_26', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_17'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_17', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_6', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_7', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'ave', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_7', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_17', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_17', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9677616059780121,0.2659803181886673,0.16022811830043793,0.9643983244895935,0.2414761483669281,0.20242410898208618,2630.0,981.0,318.0,1134.0,5783.5,0.6151091836931248,"['              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.97      1702\n           1       0.33      1.00      0.50         2\n           2       0.74      1.00      0.85        20\n           3       0.93      0.99      0.96       101\n           4       0.26      1.00      0.41         8\n           5       0.18      0.80      0.30         5\n           6       1.00      1.00      1.00         1\n           7       0.53      1.00      0.69        10\n           8       0.50      1.00      0.67         3\n           9       0.43      1.00      0.60         3\n          11       0.17      0.33      0.22         3\n          13       0.50      1.00      0.67         7\n          14       0.12      0.20      0.15         5\n          15       0.79      1.00      0.88        11\n          16       0.38      1.00      0.55        11\n          17       0.19      0.87      0.31        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          24       0.80      1.00      0.89        12\n          25       0.96      0.98      0.97       102\n          26       0.85      0.90      0.88        39\n          27       0.76      0.97      0.85        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.70      0.85      0.77        68\n          33       0.00      0.00      0.00         1\n          34       0.84      0.76      0.80        34\n          35       0.90      0.84      0.87        31\n          36       0.71      1.00      0.83         5\n          37       0.50      0.50      0.50         2\n          38       0.29      1.00      0.45        12\n          39       0.86      0.86      0.86         7\n          40       0.72      0.98      0.83        49\n          41       1.00      0.42      0.59        12\n          42       0.85      0.85      0.85        27\n          43       0.25      1.00      0.40         9\n          44       1.00      1.00      1.00         3\n          45       0.19      0.79      0.31        19\n          46       0.94      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       0.71      1.00      0.83         5\n          50       0.33      1.00      0.50         1\n          51       0.55      0.86      0.67         7\n          52       0.12      0.40      0.19         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.25      1.00      0.39        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.27      1.00      0.42         4\n          73       0.82      0.95      0.88       567\n          75       0.87      1.00      0.93        13\n          76       0.55      1.00      0.71         6\n          77       0.55      1.00      0.71        11\n          78       0.59      0.98      0.74        53\n          79       0.62      0.83      0.71         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          83       0.00      0.00      0.00         2\n          84       0.60      1.00      0.75         6\n          85       0.00      1.00      0.00         0\n          86       0.38      0.86      0.52         7\n          87       1.00      1.00      1.00        27\n          88       0.47      1.00      0.64         7\n          89       0.13      1.00      0.24         2\n          90       0.79      0.92      0.85        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.36      0.83      0.50         6\n         103       0.50      0.65      0.57        20\n         104       0.96      0.96      0.96       110\n         105       0.50      1.00      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.83      0.98      0.90        58\n         112       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.46      0.75      0.57         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.57      0.86      0.60     10056\nweighted avg       0.95      0.90      0.91     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1702\n           1       0.50      1.00      0.67         2\n           2       0.80      1.00      0.89        20\n           3       0.94      0.99      0.97       101\n           4       0.28      1.00      0.43         8\n           5       0.17      1.00      0.29         5\n           6       1.00      1.00      1.00         1\n           7       0.56      1.00      0.71        10\n           8       0.75      1.00      0.86         3\n           9       0.50      1.00      0.67         3\n          11       0.33      0.67      0.44         3\n          13       0.64      1.00      0.78         7\n          14       0.22      0.40      0.29         5\n          15       0.85      1.00      0.92        11\n          16       0.73      1.00      0.85        11\n          17       0.26      0.87      0.41        46\n          18       1.00      1.00      1.00        33\n          19       0.86      1.00      0.92         6\n          20       0.95      1.00      0.98        61\n          24       0.86      1.00      0.92        12\n          25       0.95      0.97      0.96       102\n          26       0.92      0.85      0.88        39\n          27       0.82      0.95      0.88        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.89      0.64         9\n          32       0.81      0.82      0.82        68\n          33       0.20      1.00      0.33         1\n          34       0.91      0.85      0.88        34\n          35       0.91      0.94      0.92        31\n          36       0.83      1.00      0.91         5\n          37       0.33      0.50      0.40         2\n          38       0.35      1.00      0.52        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.96      0.96        49\n          41       0.71      0.42      0.53        12\n          42       0.88      0.85      0.87        27\n          43       0.50      1.00      0.67         9\n          44       1.00      1.00      1.00         3\n          45       0.32      0.79      0.45        19\n          46       0.91      0.95      0.93       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       0.83      1.00      0.91         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.25      0.60      0.35         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.33      1.00      0.49        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.57      1.00      0.73         4\n          73       0.86      0.97      0.91       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.55      1.00      0.71        11\n          78       0.59      0.98      0.74        53\n          79       0.32      1.00      0.48         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.86      1.00      0.92         6\n          86       0.41      1.00      0.58         7\n          87       1.00      1.00      1.00        27\n          88       0.54      1.00      0.70         7\n          89       0.13      1.00      0.24         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.96      0.92        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.29      0.67      0.40         6\n         103       0.70      0.70      0.70        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         115       0.56      1.00      0.71         5\n         116       0.96      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.88      0.93      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.60      0.88      0.64     10056\nweighted avg       0.95      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_sum_64,"{'name': 'sequential_29', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_19'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_19', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_7', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_8', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'sum', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_8', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_19', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_19', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.977106899023056,0.27496081590652466,0.0614283662289381,0.9728078842163086,0.24887334555387497,0.13447503745555878,2647.5,710.0,300.5,846.0,4977.5,0.6444325015940601,"['              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.71      1.00      0.83        20\n           3       0.95      0.99      0.97       101\n           4       0.32      1.00      0.48         8\n           5       0.24      1.00      0.38         5\n           6       1.00      0.00      0.00         1\n           7       0.56      1.00      0.71        10\n           8       1.00      1.00      1.00         3\n           9       1.00      1.00      1.00         3\n          11       0.67      0.67      0.67         3\n          13       0.44      1.00      0.61         7\n          14       0.27      0.60      0.38         5\n          15       0.79      1.00      0.88        11\n          16       0.61      1.00      0.76        11\n          17       0.36      0.87      0.51        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          22       0.00      1.00      0.00         0\n          24       0.75      1.00      0.86        12\n          25       0.96      0.98      0.97       102\n          26       0.97      0.90      0.93        39\n          27       0.88      0.95      0.91        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.46      0.67      0.55         9\n          32       0.78      0.87      0.82        68\n          33       0.33      1.00      0.50         1\n          34       0.91      0.88      0.90        34\n          35       0.85      0.90      0.88        31\n          36       0.71      1.00      0.83         5\n          37       0.33      0.50      0.40         2\n          38       0.35      1.00      0.52        12\n          39       0.86      0.86      0.86         7\n          40       0.87      0.96      0.91        49\n          41       1.00      0.42      0.59        12\n          42       0.80      0.89      0.84        27\n          43       0.31      1.00      0.47         9\n          44       1.00      1.00      1.00         3\n          45       0.54      0.79      0.64        19\n          46       0.95      0.95      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.55      0.86      0.67         7\n          52       0.40      0.40      0.40         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.44      1.00      0.61        14\n          71       1.00      1.00      1.00         2\n          72       0.31      1.00      0.47         4\n          73       0.91      0.96      0.93       567\n          75       0.93      1.00      0.96        13\n          76       0.67      1.00      0.80         6\n          77       0.61      1.00      0.76        11\n          78       0.58      0.98      0.73        53\n          79       0.55      1.00      0.71         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          85       0.00      1.00      0.00         0\n          86       0.41      1.00      0.58         7\n          87       0.96      1.00      0.98        27\n          88       0.41      1.00      0.58         7\n          89       0.25      1.00      0.40         2\n          90       1.00      0.92      0.96        12\n          93       0.33      0.50      0.40         2\n          95       0.74      0.96      0.84        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.36      0.67      0.47         6\n         103       0.71      0.75      0.73        20\n         104       0.94      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         115       0.71      1.00      0.83         5\n         116       0.94      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.66      0.88      0.69     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1702\n           1       0.40      1.00      0.57         2\n           2       0.91      1.00      0.95        20\n           3       0.97      0.99      0.98       101\n           4       0.33      1.00      0.50         8\n           5       0.20      0.80      0.32         5\n           6       1.00      1.00      1.00         1\n           7       0.59      1.00      0.74        10\n           8       0.75      1.00      0.86         3\n           9       0.43      1.00      0.60         3\n          11       0.25      0.33      0.29         3\n          13       0.54      1.00      0.70         7\n          14       0.25      0.60      0.35         5\n          15       0.73      1.00      0.85        11\n          16       0.61      1.00      0.76        11\n          17       0.27      0.85      0.41        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          21       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.94      0.96      0.95       102\n          26       0.80      0.92      0.86        39\n          27       0.86      0.95      0.90        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.64      1.00      0.78         9\n          32       0.82      0.85      0.83        68\n          33       0.20      1.00      0.33         1\n          34       0.90      0.79      0.84        34\n          35       0.87      0.87      0.87        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.43      1.00      0.60        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.98      0.94        49\n          41       1.00      0.42      0.59        12\n          42       0.79      0.85      0.82        27\n          43       0.53      1.00      0.69         9\n          44       1.00      1.00      1.00         3\n          45       0.33      0.68      0.44        19\n          46       0.96      0.95      0.96       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.58      1.00      0.74         7\n          52       0.20      0.60      0.30         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.38      1.00      0.55        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.90      0.96      0.93       567\n          75       1.00      1.00      1.00        13\n          76       1.00      1.00      1.00         6\n          77       0.62      0.91      0.74        11\n          78       0.64      0.98      0.78        53\n          79       0.50      0.83      0.62         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          86       0.58      1.00      0.74         7\n          87       1.00      1.00      1.00        27\n          88       0.37      1.00      0.54         7\n          89       0.50      1.00      0.67         2\n          90       0.65      0.92      0.76        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.50      0.83      0.62         6\n         103       0.65      0.65      0.65        20\n         104       0.95      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.62      0.91      0.67     10056\nweighted avg       0.96      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

StackedLSTM_32,"{'name': 'sequential_32', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_21'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_21', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_9', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_10', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_21', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.8164914846420288,0.15586864948272705,1.3202374577522278,0.8173258602619171,0.14301159977912903,0.8892073333263397,1909.0,5609.0,1039.0,5815.5,10100.0,0.30313377234170025,"['              precision    recall  f1-score   support\n\n           0       1.00      0.75      0.86      1702\n           1       0.07      1.00      0.13         2\n           2       0.13      0.75      0.22        20\n           3       0.54      0.94      0.69       101\n           4       0.04      1.00      0.08         8\n           5       0.04      0.80      0.08         5\n           6       0.00      0.00      0.00         1\n           7       0.37      0.70      0.48        10\n           8       0.14      0.67      0.24         3\n           9       0.06      0.67      0.12         3\n          10       0.00      1.00      0.00         0\n          11       1.00      0.33      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.22      0.57      0.32         7\n          14       0.33      0.80      0.47         5\n          15       0.55      0.55      0.55        11\n          16       0.13      0.73      0.22        11\n          17       0.21      0.54      0.31        46\n          18       0.27      0.94      0.42        33\n          19       0.01      1.00      0.02         6\n          20       0.12      0.82      0.22        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.17      0.75      0.27        12\n          25       0.47      0.77      0.59       102\n          26       0.57      0.54      0.55        39\n          27       0.37      0.72      0.49        39\n          28       0.09      0.67      0.16        12\n          29       0.07      1.00      0.13         1\n          30       0.00      0.00      0.00         1\n          31       0.11      0.56      0.18         9\n          32       0.35      0.59      0.44        68\n          33       0.04      1.00      0.08         1\n          34       0.55      0.68      0.61        34\n          35       0.41      0.68      0.51        31\n          36       0.19      1.00      0.31         5\n          37       0.20      0.50      0.29         2\n          38       0.16      1.00      0.28        12\n          39       0.07      1.00      0.13         7\n          40       0.15      0.82      0.26        49\n          41       0.09      0.42      0.15        12\n          42       0.38      0.85      0.52        27\n          43       0.02      0.67      0.03         9\n          44       0.07      0.67      0.13         3\n          45       0.15      0.47      0.22        19\n          46       0.33      0.81      0.47       656\n          47       0.10      1.00      0.18         5\n          48       0.18      0.86      0.30         7\n          49       0.14      0.60      0.22         5\n          50       0.00      0.00      0.00         1\n          51       0.33      0.86      0.48         7\n          52       0.04      0.20      0.06         5\n          53       0.00      1.00      0.00         0\n          54       0.30      0.50      0.38         6\n          56       0.25      0.83      0.38         6\n          58       0.00      1.00      0.00         0\n          62       0.66      0.86      0.75        64\n          63       0.10      0.33      0.15         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.30      0.86      0.44        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.06      1.00      0.12         2\n          72       0.09      0.75      0.15         4\n          73       0.43      0.61      0.51       567\n          74       0.00      1.00      0.00         0\n          75       0.25      1.00      0.41        13\n          76       0.11      0.83      0.20         6\n          77       0.06      0.36      0.11        11\n          78       0.34      0.96      0.50        53\n          79       0.05      0.50      0.09         6\n          80       0.00      1.00      0.00         0\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.50      0.67         2\n          84       0.62      0.83      0.71         6\n          86       0.00      0.00      0.00         7\n          87       0.45      1.00      0.62        27\n          88       0.20      0.86      0.32         7\n          89       0.33      0.50      0.40         2\n          90       0.26      0.92      0.41        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.17      0.50      0.25         2\n          94       0.00      1.00      0.00         0\n          95       0.54      0.83      0.66        24\n          97       0.20      1.00      0.33         1\n          98       0.33      0.50      0.40         2\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.06      0.67      0.11         6\n         103       0.16      0.60      0.25        20\n         104       0.51      0.81      0.63       110\n         105       0.25      0.50      0.33         2\n         106       0.33      1.00      0.50         1\n         108       0.33      1.00      0.50         2\n         111       0.49      0.98      0.66        58\n         112       0.00      1.00      0.00         0\n         115       0.17      0.80      0.29         5\n         116       0.30      0.90      0.45       132\n         117       0.20      1.00      0.33         2\n         118       0.13      1.00      0.23         8\n         119       1.00      0.12      0.22      5640\n\n    accuracy                           0.40     10056\n   macro avg       0.22      0.75      0.26     10056\nweighted avg       0.83      0.40      0.39     10056\n', '              precision    recall  f1-score   support\n\n           0       1.00      0.76      0.87      1702\n           1       0.07      1.00      0.12         2\n           2       0.19      0.75      0.30        20\n           3       0.37      0.92      0.53       101\n           4       0.06      1.00      0.11         8\n           5       0.05      0.60      0.10         5\n           6       0.25      1.00      0.40         1\n           7       0.20      0.70      0.31        10\n           8       0.29      0.67      0.40         3\n           9       0.06      0.67      0.12         3\n          11       1.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.40      0.29      0.33         7\n          14       0.33      0.40      0.36         5\n          15       0.53      0.82      0.64        11\n          16       0.24      0.64      0.35        11\n          17       0.24      0.35      0.28        46\n          18       0.27      0.94      0.42        33\n          19       0.50      0.83      0.62         6\n          20       0.46      0.95      0.62        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.24      0.67      0.35        12\n          25       0.50      0.80      0.62       102\n          26       0.62      0.62      0.62        39\n          27       0.32      0.77      0.45        39\n          28       0.53      0.75      0.62        12\n          29       0.01      1.00      0.03         1\n          30       0.11      1.00      0.20         1\n          31       0.11      0.56      0.19         9\n          32       0.46      0.74      0.57        68\n          33       0.00      0.00      0.00         1\n          34       0.83      0.71      0.76        34\n          35       0.60      0.84      0.70        31\n          36       0.18      1.00      0.30         5\n          37       0.11      0.50      0.18         2\n          38       0.18      1.00      0.30        12\n          39       0.24      0.86      0.38         7\n          40       0.04      0.88      0.07        49\n          41       0.36      0.42      0.38        12\n          42       0.15      0.85      0.25        27\n          43       0.02      0.44      0.03         9\n          44       0.12      0.67      0.21         3\n          45       0.19      0.58      0.29        19\n          46       0.35      0.80      0.49       656\n          47       0.15      0.80      0.26         5\n          48       0.40      0.57      0.47         7\n          49       0.20      0.40      0.27         5\n          50       0.00      0.00      0.00         1\n          51       0.19      0.86      0.32         7\n          52       0.11      0.40      0.17         5\n          53       0.00      1.00      0.00         0\n          54       0.15      0.83      0.26         6\n          55       0.00      1.00      0.00         0\n          56       0.15      0.83      0.26         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.68      0.72      0.70        64\n          63       0.05      0.33      0.08         3\n          64       1.00      0.00      0.00         1\n          66       0.43      0.86      0.57        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.04      1.00      0.08         2\n          72       0.02      0.75      0.04         4\n          73       0.64      0.75      0.69       567\n          74       0.00      1.00      0.00         0\n          75       0.29      1.00      0.45        13\n          76       0.22      0.83      0.34         6\n          77       0.12      0.36      0.18        11\n          78       0.37      0.83      0.51        53\n          79       0.18      1.00      0.30         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.25      0.50      0.33         6\n          86       0.09      0.43      0.15         7\n          87       0.48      1.00      0.65        27\n          88       0.37      1.00      0.54         7\n          89       0.50      0.50      0.50         2\n          90       0.31      0.92      0.47        12\n          92       0.00      1.00      0.00         0\n          93       0.20      0.50      0.29         2\n          94       0.00      1.00      0.00         0\n          95       0.68      0.79      0.73        24\n          97       0.25      1.00      0.40         1\n          98       0.33      0.50      0.40         2\n         100       0.00      1.00      0.00         0\n         101       0.05      0.50      0.09         2\n         102       0.03      0.67      0.06         6\n         103       0.26      0.80      0.40        20\n         104       0.66      0.92      0.77       110\n         105       0.20      0.50      0.29         2\n         106       0.33      1.00      0.50         1\n         108       0.05      1.00      0.10         2\n         111       0.46      1.00      0.63        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.24      1.00      0.38         5\n         116       0.56      0.96      0.71       132\n         117       0.15      1.00      0.27         2\n         118       0.09      0.88      0.16         8\n         119       1.00      0.22      0.36      5640\n\n    accuracy                           0.46     10056\n   macro avg       0.26      0.75      0.29     10056\nweighted avg       0.85      0.46      0.49     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

StackedLSTM_64,"{'name': 'sequential_34', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_22'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_22', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_11', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_12', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_22', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_22', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.8704367578029633,0.19660068303346634,0.501368522644043,0.8781565725803375,0.17983607202768326,0.5784909427165985,2290.0,4190.5,658.0,4240.0,10077.0,0.40573124073870276,"['              precision    recall  f1-score   support\n\n           0       0.99      0.92      0.95      1702\n           1       0.15      1.00      0.27         2\n           2       0.33      0.85      0.47        20\n           3       0.60      0.97      0.74       101\n           4       0.07      1.00      0.13         8\n           5       0.09      0.80      0.16         5\n           6       0.00      0.00      0.00         1\n           7       0.29      1.00      0.45        10\n           8       0.50      0.67      0.57         3\n           9       0.25      0.67      0.36         3\n          11       1.00      0.33      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.86      0.63         7\n          14       0.33      0.80      0.47         5\n          15       0.90      0.82      0.86        11\n          16       0.14      0.91      0.24        11\n          17       0.19      0.48      0.28        46\n          18       0.60      1.00      0.75        33\n          19       0.14      1.00      0.24         6\n          20       0.56      1.00      0.72        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.29      0.83      0.43        12\n          25       0.48      0.86      0.61       102\n          26       0.72      0.74      0.73        39\n          27       0.38      0.97      0.55        39\n          28       0.09      0.75      0.16        12\n          29       0.20      1.00      0.33         1\n          30       0.25      1.00      0.40         1\n          31       0.14      0.56      0.22         9\n          32       0.65      0.81      0.72        68\n          33       0.06      1.00      0.11         1\n          34       0.87      0.76      0.81        34\n          35       0.73      0.87      0.79        31\n          36       0.45      1.00      0.62         5\n          37       0.33      0.50      0.40         2\n          38       0.21      1.00      0.35        12\n          39       0.10      0.86      0.18         7\n          40       0.38      0.82      0.52        49\n          41       0.15      0.67      0.25        12\n          42       0.37      0.85      0.52        27\n          43       0.03      0.78      0.05         9\n          44       0.21      1.00      0.35         3\n          45       0.16      0.79      0.26        19\n          46       0.47      0.87      0.61       656\n          47       0.28      1.00      0.43         5\n          48       0.22      1.00      0.36         7\n          49       0.05      0.20      0.08         5\n          50       0.00      0.00      0.00         1\n          51       0.38      0.86      0.52         7\n          52       0.06      0.20      0.09         5\n          53       0.00      1.00      0.00         0\n          54       0.57      0.67      0.62         6\n          55       0.00      1.00      0.00         0\n          56       0.20      1.00      0.33         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.57      0.98      0.72        64\n          63       0.33      0.67      0.44         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.32      0.86      0.47        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.11      1.00      0.19         2\n          72       0.09      0.75      0.16         4\n          73       0.46      0.88      0.60       567\n          74       0.00      1.00      0.00         0\n          75       0.59      1.00      0.74        13\n          76       0.25      0.83      0.38         6\n          77       0.12      0.91      0.21        11\n          78       0.41      1.00      0.58        53\n          79       0.16      1.00      0.27         6\n          81       1.00      0.00      0.00         2\n          83       0.50      0.50      0.50         2\n          84       0.43      1.00      0.60         6\n          86       0.25      0.71      0.37         7\n          87       0.69      1.00      0.82        27\n          88       0.47      1.00      0.64         7\n          89       0.20      0.50      0.29         2\n          90       0.50      0.92      0.65        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.33      0.50      0.40         2\n          95       0.72      0.96      0.82        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.30      1.00      0.46         6\n         103       0.37      0.80      0.51        20\n         104       0.72      0.94      0.81       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.62      0.98      0.76        58\n         115       0.28      1.00      0.43         5\n         116       0.64      0.98      0.77       132\n         117       0.50      1.00      0.67         2\n         118       0.31      1.00      0.47         8\n         119       1.00      0.44      0.61      5640\n\n    accuracy                           0.64     10056\n   macro avg       0.32      0.82      0.38     10056\nweighted avg       0.86      0.64      0.67     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.90      0.95      1702\n           1       0.10      1.00      0.18         2\n           2       0.30      0.90      0.45        20\n           3       0.57      0.97      0.72       101\n           4       0.04      1.00      0.08         8\n           5       0.15      1.00      0.26         5\n           6       1.00      0.00      0.00         1\n           7       0.44      0.80      0.57        10\n           8       1.00      1.00      1.00         3\n           9       0.23      1.00      0.38         3\n          11       1.00      0.67      0.80         3\n          12       0.00      1.00      0.00         0\n          13       0.50      0.86      0.63         7\n          14       0.24      0.80      0.36         5\n          15       0.39      0.82      0.53        11\n          16       0.17      1.00      0.29        11\n          17       0.18      0.41      0.26        46\n          18       0.40      0.97      0.57        33\n          19       0.15      1.00      0.27         6\n          20       0.74      1.00      0.85        61\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.40      0.83      0.54        12\n          25       0.57      0.87      0.69       102\n          26       0.63      0.85      0.73        39\n          27       0.27      0.92      0.42        39\n          28       0.11      0.92      0.20        12\n          29       0.17      1.00      0.29         1\n          30       1.00      1.00      1.00         1\n          31       0.50      0.78      0.61         9\n          32       0.53      0.87      0.66        68\n          33       0.33      1.00      0.50         1\n          34       0.49      0.82      0.62        34\n          35       0.44      0.90      0.60        31\n          36       0.38      1.00      0.56         5\n          37       0.08      0.50      0.14         2\n          38       0.19      1.00      0.32        12\n          39       0.38      0.86      0.52         7\n          40       0.19      0.82      0.31        49\n          41       0.21      0.42      0.28        12\n          42       0.45      0.85      0.59        27\n          43       0.01      0.67      0.02         9\n          44       0.07      1.00      0.14         3\n          45       0.21      0.74      0.32        19\n          46       0.49      0.87      0.63       656\n          47       0.19      0.80      0.31         5\n          48       0.22      0.71      0.33         7\n          49       0.29      0.40      0.33         5\n          50       0.03      1.00      0.06         1\n          51       0.24      1.00      0.39         7\n          52       0.03      0.20      0.06         5\n          54       0.31      0.83      0.45         6\n          55       0.00      1.00      0.00         0\n          56       0.21      1.00      0.34         6\n          62       0.71      0.97      0.82        64\n          63       0.25      0.67      0.36         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.31      1.00      0.47        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.18      1.00      0.31         2\n          72       0.10      0.75      0.17         4\n          73       0.47      0.84      0.61       567\n          74       0.00      1.00      0.00         0\n          75       0.32      1.00      0.48        13\n          76       0.38      0.83      0.53         6\n          77       0.10      0.55      0.17        11\n          78       0.54      1.00      0.70        53\n          79       0.32      1.00      0.48         6\n          80       0.00      1.00      0.00         0\n          81       0.29      1.00      0.44         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       0.50      0.83      0.62         6\n          85       0.00      1.00      0.00         0\n          86       0.14      0.86      0.24         7\n          87       0.60      1.00      0.75        27\n          88       0.41      1.00      0.58         7\n          89       0.50      1.00      0.67         2\n          90       0.44      0.92      0.59        12\n          93       0.29      1.00      0.44         2\n          94       0.00      1.00      0.00         0\n          95       0.59      0.96      0.73        24\n          97       0.33      1.00      0.50         1\n          98       0.33      0.50      0.40         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.09      0.50      0.15         2\n         102       0.14      0.83      0.24         6\n         103       0.27      0.65      0.38        20\n         104       0.72      0.95      0.82       110\n         105       0.50      1.00      0.67         2\n         106       1.00      0.00      0.00         1\n         108       0.25      1.00      0.40         2\n         111       0.61      0.98      0.75        58\n         114       0.00      1.00      0.00         0\n         115       0.29      1.00      0.45         5\n         116       0.75      0.97      0.85       132\n         117       0.33      1.00      0.50         2\n         118       0.35      1.00      0.52         8\n         119       0.99      0.38      0.55      5640\n\n    accuracy                           0.60     10056\n   macro avg       0.33      0.86      0.39     10056\nweighted avg       0.86      0.60      0.63     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalStackedLSTM_32,"{'name': 'sequential_38', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_25'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_25', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_8', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_13', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_13', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_9', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_14', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_14', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_25', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_25', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.8286776542663574,0.1562141552567482,0.46557745337486267,0.8354781270027161,0.14572595059871674,0.7549120187759399,2090.5,3984.0,857.5,4189.5,10082.5,0.41254862862892727,"['              precision    recall  f1-score   support\n\n           0       1.00      0.61      0.76      1702\n           1       0.09      1.00      0.16         2\n           2       0.21      0.95      0.34        20\n           3       0.44      0.89      0.59       101\n           4       0.12      1.00      0.22         8\n           5       0.24      0.80      0.36         5\n           6       0.00      0.00      0.00         1\n           7       0.26      0.80      0.39        10\n           8       0.29      0.67      0.40         3\n           9       0.13      0.67      0.22         3\n          11       0.33      0.67      0.44         3\n          12       0.00      1.00      0.00         0\n          13       0.20      0.57      0.30         7\n          14       0.12      0.40      0.19         5\n          15       0.60      0.82      0.69        11\n          16       0.37      0.91      0.53        11\n          17       0.13      0.74      0.22        46\n          18       0.39      0.97      0.56        33\n          19       0.20      0.83      0.32         6\n          20       0.31      0.95      0.47        61\n          23       0.00      1.00      0.00         0\n          24       0.42      0.83      0.56        12\n          25       0.49      0.80      0.61       102\n          26       0.49      0.69      0.57        39\n          27       0.34      0.85      0.49        39\n          28       0.22      0.92      0.35        12\n          29       0.50      1.00      0.67         1\n          30       1.00      0.00      0.00         1\n          31       0.29      0.67      0.40         9\n          32       0.46      0.87      0.60        68\n          33       0.33      1.00      0.50         1\n          34       0.71      0.71      0.71        34\n          35       0.64      0.87      0.74        31\n          36       0.31      1.00      0.48         5\n          37       0.33      0.50      0.40         2\n          38       0.15      1.00      0.27        12\n          39       0.25      1.00      0.40         7\n          40       0.23      0.96      0.38        49\n          41       0.23      0.58      0.33        12\n          42       0.23      0.89      0.36        27\n          43       0.15      0.67      0.25         9\n          44       0.15      1.00      0.26         3\n          45       0.15      0.58      0.24        19\n          46       0.46      0.81      0.59       656\n          47       0.29      1.00      0.45         5\n          48       0.17      0.57      0.27         7\n          49       0.14      0.80      0.24         5\n          50       0.04      1.00      0.07         1\n          51       0.22      1.00      0.36         7\n          52       0.04      0.40      0.07         5\n          53       0.00      1.00      0.00         0\n          54       0.40      1.00      0.57         6\n          55       0.00      1.00      0.00         0\n          56       0.20      1.00      0.33         6\n          58       0.00      1.00      0.00         0\n          62       0.55      0.98      0.70        64\n          63       0.43      1.00      0.60         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.16      1.00      0.27        14\n          68       0.00      1.00      0.00         0\n          71       0.10      1.00      0.17         2\n          72       0.10      1.00      0.17         4\n          73       0.39      0.71      0.50       567\n          74       0.00      1.00      0.00         0\n          75       0.34      1.00      0.51        13\n          76       0.09      0.67      0.15         6\n          77       0.09      1.00      0.17        11\n          78       0.36      1.00      0.52        53\n          79       0.20      1.00      0.33         6\n          81       0.00      0.00      0.00         2\n          83       1.00      0.00      0.00         2\n          84       0.22      0.83      0.34         6\n          86       0.14      1.00      0.25         7\n          87       0.52      1.00      0.68        27\n          88       0.28      1.00      0.44         7\n          89       0.40      1.00      0.57         2\n          90       0.29      0.83      0.43        12\n          92       0.00      1.00      0.00         0\n          93       0.22      1.00      0.36         2\n          95       0.55      0.71      0.62        24\n          97       0.33      1.00      0.50         1\n          98       0.33      0.50      0.40         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       1.00      0.00      0.00         2\n         102       0.26      1.00      0.41         6\n         103       0.21      0.40      0.27        20\n         104       0.40      0.90      0.55       110\n         105       0.50      0.50      0.50         2\n         106       1.00      0.00      0.00         1\n         108       0.33      1.00      0.50         2\n         110       0.00      1.00      0.00         0\n         111       0.48      1.00      0.65        58\n         114       0.00      1.00      0.00         0\n         115       0.14      1.00      0.24         5\n         116       0.42      0.96      0.58       132\n         117       0.25      1.00      0.40         2\n         118       0.29      0.88      0.44         8\n         119       0.98      0.45      0.61      5640\n\n    accuracy                           0.57     10056\n   macro avg       0.29      0.81      0.34     10056\nweighted avg       0.83      0.57      0.61     10056\n', '              precision    recall  f1-score   support\n\n           0       1.00      0.63      0.77      1702\n           1       0.07      1.00      0.13         2\n           2       0.24      0.95      0.38        20\n           3       0.36      0.98      0.53       101\n           4       0.13      1.00      0.23         8\n           5       0.15      1.00      0.26         5\n           6       1.00      1.00      1.00         1\n           7       0.26      0.80      0.39        10\n           8       0.20      0.67      0.31         3\n           9       0.33      0.67      0.44         3\n          10       0.00      1.00      0.00         0\n          11       1.00      0.67      0.80         3\n          12       0.00      1.00      0.00         0\n          13       0.19      0.71      0.30         7\n          14       0.18      0.60      0.27         5\n          15       0.90      0.82      0.86        11\n          16       0.30      0.82      0.44        11\n          17       0.11      0.67      0.20        46\n          18       0.51      1.00      0.67        33\n          19       0.18      0.83      0.29         6\n          20       0.27      1.00      0.43        61\n          24       0.39      0.92      0.55        12\n          25       0.44      0.81      0.57       102\n          26       0.48      0.69      0.57        39\n          27       0.43      0.92      0.59        39\n          28       0.31      0.92      0.46        12\n          29       0.50      1.00      0.67         1\n          30       0.33      1.00      0.50         1\n          31       0.20      0.56      0.29         9\n          32       0.37      0.78      0.50        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.79      0.84        34\n          35       0.60      0.81      0.68        31\n          36       0.29      1.00      0.45         5\n          37       0.33      0.50      0.40         2\n          38       0.13      1.00      0.23        12\n          39       0.21      0.86      0.34         7\n          40       0.29      0.96      0.44        49\n          41       0.30      0.50      0.38        12\n          42       0.26      0.85      0.40        27\n          43       0.09      0.78      0.16         9\n          44       0.08      0.67      0.15         3\n          45       0.16      0.74      0.26        19\n          46       0.39      0.89      0.54       656\n          47       0.26      1.00      0.42         5\n          48       0.32      0.86      0.46         7\n          49       0.33      1.00      0.50         5\n          50       0.00      0.00      0.00         1\n          51       0.29      1.00      0.45         7\n          52       0.09      0.80      0.16         5\n          54       0.45      0.83      0.59         6\n          55       0.00      1.00      0.00         0\n          56       0.21      1.00      0.35         6\n          61       0.00      1.00      0.00         0\n          62       0.57      0.98      0.72        64\n          63       0.21      1.00      0.35         3\n          64       0.00      0.00      0.00         1\n          66       0.19      1.00      0.32        14\n          68       0.00      1.00      0.00         0\n          71       0.20      1.00      0.33         2\n          72       0.22      1.00      0.36         4\n          73       0.48      0.81      0.60       567\n          74       0.00      1.00      0.00         0\n          75       0.43      1.00      0.60        13\n          76       0.14      0.67      0.23         6\n          77       0.19      1.00      0.32        11\n          78       0.46      1.00      0.63        53\n          79       0.26      0.83      0.40         6\n          81       0.25      0.50      0.33         2\n          83       0.50      0.50      0.50         2\n          84       0.27      0.67      0.38         6\n          86       0.12      1.00      0.21         7\n          87       0.50      1.00      0.67        27\n          88       0.25      1.00      0.40         7\n          89       0.33      1.00      0.50         2\n          90       0.28      0.92      0.43        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.20      0.50      0.29         2\n          94       0.00      1.00      0.00         0\n          95       0.53      0.79      0.63        24\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.10      0.67      0.18         6\n         103       0.24      0.60      0.34        20\n         104       0.52      0.92      0.67       110\n         105       0.40      1.00      0.57         2\n         106       0.33      1.00      0.50         1\n         108       0.17      1.00      0.29         2\n         111       0.46      1.00      0.63        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.17      1.00      0.29         5\n         116       0.46      0.95      0.63       132\n         117       0.50      1.00      0.67         2\n         118       0.29      0.75      0.41         8\n         119       0.99      0.42      0.59      5640\n\n    accuracy                           0.58     10056\n   macro avg       0.29      0.83      0.38     10056\nweighted avg       0.84      0.58      0.61     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalStackedLSTM_16,"{'name': 'sequential_41', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_27'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_27', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_27', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_27', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}], 'build_input_shape': (None, 46)}",0.915186196565628,0.17174362391233444,0.8875782489776611,0.914025068283081,0.15926188230514526,0.30328135192394257,1050.5,2463.0,1897.5,1673.0,9823.0,0.3450568798818039,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      0.90      0.92        20\n           3       0.97      0.99      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.00      0.00      0.00         5\n           6       1.00      0.00      0.00         1\n           7       0.08      0.60      0.14        10\n           8       0.00      0.00      0.00         3\n           9       0.11      0.33      0.17         3\n          10       0.00      1.00      0.00         0\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       1.00      0.00      0.00         7\n          14       1.00      0.00      0.00         5\n          15       0.40      0.18      0.25        11\n          16       0.31      0.73      0.43        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          24       0.73      0.92      0.81        12\n          25       0.88      0.34      0.49       102\n          26       1.00      0.33      0.50        39\n          27       0.83      0.49      0.61        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       0.84      0.40      0.54        68\n          33       0.00      0.00      0.00         1\n          34       0.44      0.12      0.19        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.31      0.46        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.03      0.56      0.05         9\n          44       1.00      0.33      0.50         3\n          45       0.02      0.05      0.03        19\n          46       0.60      0.28      0.38       656\n          47       1.00      0.00      0.00         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.14      1.00      0.25         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.43      0.04        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.23      0.75      0.35         4\n          73       0.61      0.37      0.46       567\n          75       1.00      0.00      0.00        13\n          76       0.09      0.83      0.16         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       0.19      0.67      0.30         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          86       0.00      0.00      0.00         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          92       0.00      1.00      0.00         0\n          93       1.00      0.00      0.00         2\n          94       0.00      1.00      0.00         0\n          95       0.00      0.00      0.00        24\n          96       0.00      1.00      0.00         0\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.00      0.17      0.00         6\n         103       1.00      0.00      0.00        20\n         104       0.62      0.37      0.47       110\n         105       1.00      0.00      0.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.66      0.71      0.68       132\n         117       0.50      1.00      0.67         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.72     10056\n   macro avg       0.50      0.60      0.36     10056\nweighted avg       0.90      0.72      0.78     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      0.90      0.92        20\n           3       0.97      0.99      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.00      0.00      0.00         5\n           6       1.00      0.00      0.00         1\n           7       0.10      0.40      0.15        10\n           8       0.00      0.00      0.00         3\n           9       0.11      0.33      0.17         3\n          10       0.00      1.00      0.00         0\n          11       0.12      0.33      0.18         3\n          12       0.00      1.00      0.00         0\n          13       1.00      0.00      0.00         7\n          14       1.00      0.00      0.00         5\n          15       0.43      0.27      0.33        11\n          16       0.31      0.73      0.43        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.93      0.14      0.24       102\n          26       0.96      0.56      0.71        39\n          27       0.83      0.49      0.61        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       1.00      0.00      0.00         9\n          32       0.84      0.40      0.54        68\n          33       0.00      0.00      0.00         1\n          34       0.67      0.18      0.28        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.95      0.86      0.90        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       0.67      0.67      0.67         3\n          45       0.02      0.05      0.03        19\n          46       0.68      0.21      0.32       656\n          47       1.00      0.00      0.00         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.50      0.50      0.50         2\n          72       0.23      0.75      0.35         4\n          73       0.62      0.26      0.37       567\n          75       0.62      1.00      0.76        13\n          76       0.09      0.83      0.16         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       0.14      0.67      0.24         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          86       0.00      0.00      0.00         7\n          87       1.00      0.00      0.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          92       0.00      1.00      0.00         0\n          93       1.00      0.00      0.00         2\n          95       0.00      0.00      0.00        24\n          96       0.00      1.00      0.00         0\n          97       0.04      1.00      0.07         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.01      0.50      0.03         2\n         102       0.08      0.67      0.14         6\n         103       1.00      0.00      0.00        20\n         104       0.62      0.37      0.47       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.64      0.56      0.60       132\n         117       0.67      1.00      0.80         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.82      0.90      5640\n\n    accuracy                           0.74     10056\n   macro avg       0.50      0.61      0.36     10056\nweighted avg       0.91      0.74      0.80     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalDropoutLSTM,"{'name': 'sequential_44', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_29'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_29', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_10', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_15', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_15', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_29', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_29', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9431761801242828,0.25024327635765076,0.19422060996294022,0.9554311037063599,0.23537517338991165,0.23167943954467773,2602.0,1075.5,346.0,1214.0,6130.5,0.5941791078326244,"['              precision    recall  f1-score   support\n\n           0       0.99      0.90      0.94      1702\n           1       0.25      1.00      0.40         2\n           2       0.74      1.00      0.85        20\n           3       0.86      0.99      0.92       101\n           4       0.24      1.00      0.38         8\n           5       0.26      1.00      0.42         5\n           6       1.00      1.00      1.00         1\n           7       0.50      1.00      0.67        10\n           8       0.50      0.67      0.57         3\n           9       0.50      1.00      0.67         3\n          11       0.38      1.00      0.55         3\n          13       0.24      0.71      0.36         7\n          14       0.33      0.60      0.43         5\n          15       0.92      1.00      0.96        11\n          16       0.48      1.00      0.65        11\n          17       0.22      0.87      0.35        46\n          18       0.94      0.97      0.96        33\n          19       0.75      1.00      0.86         6\n          20       0.85      1.00      0.92        61\n          24       0.75      1.00      0.86        12\n          25       0.88      0.97      0.92       102\n          26       0.81      0.87      0.84        39\n          27       0.59      0.97      0.74        39\n          28       0.50      0.83      0.62        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.68      0.87      0.76        68\n          33       0.00      0.00      0.00         1\n          34       0.80      0.82      0.81        34\n          35       0.93      0.90      0.92        31\n          36       0.62      1.00      0.77         5\n          37       0.50      0.50      0.50         2\n          38       0.24      1.00      0.39        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.98      0.94        49\n          41       0.25      0.42      0.31        12\n          42       0.82      0.85      0.84        27\n          43       0.16      1.00      0.28         9\n          44       1.00      1.00      1.00         3\n          45       0.27      0.79      0.40        19\n          46       0.95      0.95      0.95       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.62      1.00      0.77         5\n          50       0.33      1.00      0.50         1\n          51       0.55      0.86      0.67         7\n          52       0.14      0.60      0.23         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.28      0.93      0.43        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.17      1.00      0.30         4\n          73       0.92      0.95      0.93       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.46      1.00      0.63         6\n          77       0.33      0.91      0.49        11\n          78       0.58      0.98      0.73        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       0.50      1.00      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.86      1.00      0.92         6\n          86       0.23      1.00      0.38         7\n          87       0.75      1.00      0.86        27\n          88       0.35      1.00      0.52         7\n          89       0.14      1.00      0.25         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.92      0.90        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.40      0.67      0.50         6\n         103       0.72      0.65      0.68        20\n         104       0.95      0.96      0.96       110\n         105       0.33      1.00      0.50         2\n         106       1.00      1.00      1.00         1\n         108       0.29      1.00      0.44         2\n         111       0.74      0.98      0.84        58\n         113       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.97      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.38      1.00      0.55         8\n         119       0.98      0.85      0.91      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.52      0.89      0.58     10056\nweighted avg       0.94      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.93      0.96      1702\n           1       0.09      1.00      0.17         2\n           2       0.61      0.95      0.75        20\n           3       0.93      0.99      0.96       101\n           4       0.42      1.00      0.59         8\n           5       0.25      0.80      0.38         5\n           6       1.00      1.00      1.00         1\n           7       0.56      1.00      0.71        10\n           8       0.75      1.00      0.86         3\n           9       0.43      1.00      0.60         3\n          11       1.00      0.67      0.80         3\n          12       0.00      1.00      0.00         0\n          13       0.41      1.00      0.58         7\n          14       0.27      0.80      0.40         5\n          15       0.83      0.91      0.87        11\n          16       0.32      1.00      0.49        11\n          17       0.23      0.83      0.37        46\n          18       0.94      0.97      0.96        33\n          19       0.67      1.00      0.80         6\n          20       0.90      1.00      0.95        61\n          24       0.92      0.92      0.92        12\n          25       0.97      0.98      0.98       102\n          26       0.83      0.90      0.86        39\n          27       0.75      0.97      0.84        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.57      0.89      0.70         9\n          32       0.92      0.81      0.86        68\n          33       0.20      1.00      0.33         1\n          34       1.00      0.79      0.89        34\n          35       0.96      0.84      0.90        31\n          36       0.71      1.00      0.83         5\n          37       0.50      0.50      0.50         2\n          38       0.24      1.00      0.39        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.98      0.97        49\n          41       0.50      0.42      0.45        12\n          42       0.68      0.85      0.75        27\n          43       0.24      1.00      0.38         9\n          44       1.00      1.00      1.00         3\n          45       0.26      0.89      0.40        19\n          46       0.90      0.95      0.92       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.56      1.00      0.71         5\n          50       0.00      0.00      0.00         1\n          51       0.58      1.00      0.74         7\n          52       0.12      0.40      0.19         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.50      1.00      0.67         3\n          64       1.00      0.00      0.00         1\n          66       0.26      1.00      0.42        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.17      1.00      0.30         4\n          73       0.89      0.95      0.92       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.75      1.00      0.86         6\n          77       0.44      1.00      0.61        11\n          78       0.57      0.98      0.72        53\n          79       0.86      1.00      0.92         6\n          80       0.00      1.00      0.00         0\n          81       0.67      1.00      0.80         2\n          83       0.40      1.00      0.57         2\n          84       0.86      1.00      0.92         6\n          85       0.00      1.00      0.00         0\n          86       0.33      0.71      0.45         7\n          87       0.90      1.00      0.95        27\n          88       0.30      1.00      0.47         7\n          89       0.09      1.00      0.17         2\n          90       0.65      0.92      0.76        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.88      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.27      0.67      0.38         6\n         103       0.64      0.80      0.71        20\n         104       0.93      0.97      0.95       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       0.40      1.00      0.57         2\n         111       0.83      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.62      1.00      0.77         5\n         116       0.95      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.55      0.90      0.59     10056\nweighted avg       0.95      0.89      0.91     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalDropoutGRU,"{'name': 'sequential_47', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_31'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_31', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_11', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_31', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_31', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9665491282939911,0.27015525102615356,0.14861834049224854,0.9699482023715973,0.2503395825624466,0.14889637380838394,2596.0,821.0,352.0,1012.5,5617.0,0.6073493800204054,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.53      1.00      0.70         8\n           5       0.38      1.00      0.56         5\n           6       1.00      0.00      0.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      0.67      0.80         3\n           9       0.40      0.67      0.50         3\n          11       0.25      0.33      0.29         3\n          13       0.54      1.00      0.70         7\n          14       0.29      0.40      0.33         5\n          15       0.85      1.00      0.92        11\n          16       0.46      1.00      0.63        11\n          17       0.34      0.85      0.48        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.86      1.00      0.92        12\n          25       0.98      0.95      0.97       102\n          26       0.92      0.85      0.88        39\n          27       0.86      0.92      0.89        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.78      0.78      0.78         9\n          32       0.79      0.78      0.79        68\n          33       0.25      1.00      0.40         1\n          34       0.96      0.74      0.83        34\n          35       0.87      0.87      0.87        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.71      1.00      0.83        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.98      0.94        49\n          41       1.00      0.42      0.59        12\n          42       0.70      0.85      0.77        27\n          43       0.29      1.00      0.45         9\n          44       0.50      1.00      0.67         3\n          45       0.52      0.89      0.65        19\n          46       0.96      0.94      0.95       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.20      0.40      0.27         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       0.75      1.00      0.86         3\n          64       1.00      1.00      1.00         1\n          66       0.41      1.00      0.58        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.31      1.00      0.47         4\n          73       0.93      0.97      0.95       567\n          75       1.00      1.00      1.00        13\n          76       0.60      1.00      0.75         6\n          77       0.65      1.00      0.79        11\n          78       0.55      0.98      0.71        53\n          79       0.86      1.00      0.92         6\n          81       0.50      0.50      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.32      1.00      0.48         7\n          89       0.18      1.00      0.31         2\n          90       0.73      0.92      0.81        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.31      0.67      0.42         6\n         103       0.71      0.75      0.73        20\n         104       0.95      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.89      0.98      0.93        58\n         115       0.56      1.00      0.71         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.90      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.59      0.86      0.62     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1702\n           1       0.33      1.00      0.50         2\n           2       0.87      1.00      0.93        20\n           3       0.99      0.99      0.99       101\n           4       0.47      1.00      0.64         8\n           5       0.26      1.00      0.42         5\n           6       1.00      1.00      1.00         1\n           7       0.53      1.00      0.69        10\n           8       0.60      1.00      0.75         3\n           9       0.60      1.00      0.75         3\n          11       1.00      1.00      1.00         3\n          13       0.44      1.00      0.61         7\n          14       0.33      0.80      0.47         5\n          15       0.91      0.91      0.91        11\n          16       0.25      0.91      0.39        11\n          17       0.28      0.85      0.42        46\n          18       0.97      1.00      0.99        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.99      0.97      0.98       102\n          26       0.94      0.87      0.91        39\n          27       0.90      0.95      0.93        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.64      1.00      0.78         9\n          32       0.79      0.82      0.81        68\n          33       0.33      1.00      0.50         1\n          34       0.85      0.82      0.84        34\n          35       0.85      0.90      0.88        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.38      1.00      0.55        12\n          39       0.86      0.86      0.86         7\n          40       0.98      0.98      0.98        49\n          41       0.45      0.42      0.43        12\n          42       0.96      0.85      0.90        27\n          43       0.07      1.00      0.14         9\n          44       0.75      1.00      0.86         3\n          45       0.50      0.74      0.60        19\n          46       0.95      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.22      1.00      0.36         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.75      0.86      0.80         7\n          52       0.12      0.40      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       0.15      1.00      0.26         3\n          64       1.00      1.00      1.00         1\n          66       0.36      1.00      0.53        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.24      1.00      0.38         4\n          73       0.93      0.96      0.95       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.65      1.00      0.79        11\n          78       0.60      0.98      0.74        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.40      1.00      0.57         2\n          84       1.00      0.83      0.91         6\n          86       0.12      0.86      0.21         7\n          87       1.00      1.00      1.00        27\n          88       0.32      1.00      0.48         7\n          89       0.12      1.00      0.21         2\n          90       0.92      0.92      0.92        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.07      0.50      0.12         2\n         102       0.21      0.83      0.33         6\n         103       0.61      0.55      0.58        20\n         104       0.94      0.94      0.94       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       0.40      1.00      0.57         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.94      0.98      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.58      0.92      0.62     10056\nweighted avg       0.96      0.90      0.92     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutGRU,"{'name': 'sequential_49', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_32'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_32', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_32', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_32', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9432291090488434,0.2478998824954033,0.4012102782726288,0.9558189809322357,0.23476815223693848,0.2353070229291916,2514.5,1492.5,433.5,1539.0,7509.5,0.533381347602613,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.06      1.00      0.11         2\n           2       0.50      0.90      0.64        20\n           3       0.97      0.97      0.97       101\n           4       0.20      1.00      0.33         8\n           5       0.17      1.00      0.29         5\n           6       1.00      1.00      1.00         1\n           7       0.56      0.90      0.69        10\n           8       0.67      0.67      0.67         3\n           9       0.07      0.67      0.13         3\n          11       0.06      0.33      0.10         3\n          12       0.00      1.00      0.00         0\n          13       0.88      1.00      0.93         7\n          14       0.09      0.80      0.16         5\n          15       1.00      0.82      0.90        11\n          16       0.45      0.91      0.61        11\n          17       0.26      0.70      0.37        46\n          18       0.82      1.00      0.90        33\n          19       0.86      1.00      0.92         6\n          20       0.98      0.98      0.98        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.98      0.93      0.95       102\n          26       0.91      0.82      0.86        39\n          27       0.64      0.92      0.76        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.78      0.61         9\n          32       0.96      0.68      0.79        68\n          33       0.08      1.00      0.15         1\n          34       0.93      0.79      0.86        34\n          35       0.65      0.90      0.76        31\n          36       0.62      1.00      0.77         5\n          37       0.12      0.50      0.20         2\n          38       0.46      1.00      0.63        12\n          39       0.50      0.86      0.63         7\n          40       0.39      0.86      0.54        49\n          41       0.31      0.42      0.36        12\n          42       0.57      0.85      0.69        27\n          43       0.05      1.00      0.09         9\n          44       1.00      1.00      1.00         3\n          45       0.19      0.74      0.30        19\n          46       0.94      0.92      0.93       656\n          47       0.67      0.80      0.73         5\n          48       0.75      0.86      0.80         7\n          49       0.50      1.00      0.67         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       0.00      0.00      0.00         1\n          66       0.25      1.00      0.41        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.89      0.92      0.91       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.10      1.00      0.17         6\n          77       0.83      0.91      0.87        11\n          78       0.62      0.98      0.76        53\n          79       1.00      1.00      1.00         6\n          81       0.40      1.00      0.57         2\n          83       0.25      0.50      0.33         2\n          84       1.00      0.83      0.91         6\n          86       0.38      0.86      0.52         7\n          87       0.96      1.00      0.98        27\n          88       0.30      1.00      0.47         7\n          89       0.09      1.00      0.17         2\n          90       0.92      0.92      0.92        12\n          91       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.82      0.96      0.88        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.19      0.83      0.30         6\n         103       0.76      0.80      0.78        20\n         104       0.94      0.93      0.94       110\n         105       1.00      0.50      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.94      0.98      0.96       132\n         117       0.50      1.00      0.67         2\n         118       0.73      1.00      0.84         8\n         119       0.99      0.81      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.52      0.87      0.56     10056\nweighted avg       0.95      0.87      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.75      0.90      0.82        20\n           3       0.97      0.97      0.97       101\n           4       0.32      1.00      0.48         8\n           5       0.08      1.00      0.15         5\n           6       1.00      0.00      0.00         1\n           7       0.36      1.00      0.53        10\n           8       0.60      1.00      0.75         3\n           9       0.09      0.67      0.15         3\n          11       0.20      0.67      0.31         3\n          12       0.00      1.00      0.00         0\n          13       0.30      0.86      0.44         7\n          14       0.25      0.80      0.38         5\n          15       0.33      0.82      0.47        11\n          16       0.26      0.91      0.40        11\n          17       0.21      0.72      0.32        46\n          18       0.82      1.00      0.90        33\n          19       0.75      1.00      0.86         6\n          20       0.97      0.98      0.98        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.97      0.83      0.89       102\n          26       0.94      0.77      0.85        39\n          27       0.45      0.90      0.60        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.67      0.89      0.76         9\n          32       0.76      0.69      0.72        68\n          33       0.06      1.00      0.12         1\n          34       0.90      0.82      0.86        34\n          35       0.66      0.87      0.75        31\n          36       0.56      1.00      0.71         5\n          37       0.14      0.50      0.22         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.93      0.86      0.89        49\n          41       0.71      0.42      0.53        12\n          42       0.85      0.85      0.85        27\n          43       0.06      1.00      0.11         9\n          44       0.30      1.00      0.46         3\n          45       0.13      0.68      0.22        19\n          46       0.88      0.91      0.90       656\n          47       0.22      0.80      0.35         5\n          48       0.70      1.00      0.82         7\n          49       0.83      1.00      0.91         5\n          50       0.00      0.00      0.00         1\n          51       0.60      0.86      0.71         7\n          52       0.04      0.20      0.07         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.25      1.00      0.41        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      0.75      0.40         4\n          73       0.87      0.95      0.91       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.20      1.00      0.33         6\n          77       0.56      0.91      0.69        11\n          78       0.60      0.98      0.74        53\n          79       0.86      1.00      0.92         6\n          80       0.00      1.00      0.00         0\n          81       0.40      1.00      0.57         2\n          83       0.50      0.50      0.50         2\n          84       1.00      0.83      0.91         6\n          85       0.00      1.00      0.00         0\n          86       0.46      0.86      0.60         7\n          87       0.69      1.00      0.82        27\n          88       0.30      1.00      0.47         7\n          89       0.09      1.00      0.17         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.79      0.96      0.87        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.29      0.83      0.43         6\n         103       0.74      0.70      0.72        20\n         104       0.96      0.93      0.94       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.93      0.99      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.81      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.49      0.86      0.52     10056\nweighted avg       0.94      0.87      0.89     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutLSTM_64,"{'name': 'sequential_52', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_34'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_34', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_16', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_3', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_34', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_34', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9489173591136932,0.2525724917650223,0.19105777889490128,0.9594057202339172,0.23564599454402924,0.2267024740576744,2546.0,1349.5,402.0,1499.0,7199.0,0.5671216147564855,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.86      0.90      0.88        20\n           3       0.95      0.99      0.97       101\n           4       0.38      1.00      0.55         8\n           5       0.14      1.00      0.25         5\n           6       1.00      0.00      0.00         1\n           7       0.59      1.00      0.74        10\n           8       0.75      1.00      0.86         3\n           9       0.16      1.00      0.27         3\n          11       0.33      0.33      0.33         3\n          12       0.00      1.00      0.00         0\n          13       0.88      1.00      0.93         7\n          14       0.40      0.80      0.53         5\n          15       0.65      1.00      0.79        11\n          16       0.44      1.00      0.61        11\n          17       0.22      0.72      0.34        46\n          18       0.79      0.94      0.86        33\n          19       0.86      1.00      0.92         6\n          20       0.80      1.00      0.89        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.97      0.97      0.97       102\n          26       0.89      0.87      0.88        39\n          27       0.88      0.97      0.93        39\n          28       0.83      0.83      0.83        12\n          29       0.50      1.00      0.67         1\n          30       0.14      1.00      0.25         1\n          31       0.46      0.67      0.55         9\n          32       0.83      0.81      0.82        68\n          33       0.33      1.00      0.50         1\n          34       0.90      0.82      0.86        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       0.17      0.50      0.25         2\n          38       0.24      1.00      0.38        12\n          39       0.75      0.86      0.80         7\n          40       0.81      0.88      0.84        49\n          41       0.42      0.42      0.42        12\n          42       0.79      0.85      0.82        27\n          43       0.07      0.89      0.13         9\n          44       0.75      1.00      0.86         3\n          45       0.23      0.79      0.35        19\n          46       0.88      0.93      0.91       656\n          47       0.83      1.00      0.91         5\n          48       0.78      1.00      0.88         7\n          49       0.71      1.00      0.83         5\n          50       0.00      0.00      0.00         1\n          51       0.46      0.86      0.60         7\n          52       0.05      0.20      0.08         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.33      1.00      0.49        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.11      0.75      0.19         4\n          73       0.81      0.96      0.88       567\n          74       0.00      1.00      0.00         0\n          75       0.72      1.00      0.84        13\n          76       0.67      1.00      0.80         6\n          77       0.56      0.91      0.69        11\n          78       0.55      0.98      0.71        53\n          79       0.55      1.00      0.71         6\n          81       0.50      0.50      0.50         2\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          86       0.40      0.86      0.55         7\n          87       0.60      1.00      0.75        27\n          88       0.41      1.00      0.58         7\n          89       0.09      1.00      0.16         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.77      1.00      0.87        24\n          97       0.20      1.00      0.33         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.40      1.00      0.57         6\n         103       0.64      0.80      0.71        20\n         104       0.95      0.96      0.95       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.82        58\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.93      0.99      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.82      0.90      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.52      0.87      0.57     10056\nweighted avg       0.94      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.29      1.00      0.44         2\n           2       0.75      0.90      0.82        20\n           3       0.96      0.99      0.98       101\n           4       0.17      1.00      0.30         8\n           5       0.19      1.00      0.31         5\n           6       1.00      1.00      1.00         1\n           7       0.53      1.00      0.69        10\n           8       1.00      1.00      1.00         3\n           9       0.30      1.00      0.46         3\n          11       0.43      1.00      0.60         3\n          13       0.60      0.86      0.71         7\n          14       0.44      0.80      0.57         5\n          15       0.91      0.91      0.91        11\n          16       0.32      1.00      0.49        11\n          17       0.23      0.70      0.34        46\n          18       0.80      0.97      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.84      1.00      0.91        61\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.89      0.98      0.93       102\n          26       0.85      0.87      0.86        39\n          27       0.46      0.97      0.63        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.50      0.78      0.61         9\n          32       0.65      0.81      0.72        68\n          33       0.50      1.00      0.67         1\n          34       0.96      0.79      0.87        34\n          35       0.76      0.90      0.82        31\n          36       0.62      1.00      0.77         5\n          37       0.17      0.50      0.25         2\n          38       0.41      1.00      0.59        12\n          39       0.75      0.86      0.80         7\n          40       0.57      0.86      0.68        49\n          41       0.31      0.42      0.36        12\n          42       0.77      0.85      0.81        27\n          43       0.09      0.78      0.16         9\n          44       0.75      1.00      0.86         3\n          45       0.20      0.79      0.32        19\n          46       0.90      0.93      0.92       656\n          47       0.57      0.80      0.67         5\n          48       0.67      0.86      0.75         7\n          49       0.45      1.00      0.62         5\n          50       0.50      1.00      0.67         1\n          51       0.75      0.86      0.80         7\n          52       0.08      0.20      0.11         5\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          62       0.91      0.97      0.94        64\n          63       0.60      1.00      0.75         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.30      0.93      0.45        14\n          68       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.05      0.75      0.09         4\n          73       0.85      0.95      0.90       567\n          74       0.00      1.00      0.00         0\n          75       0.76      1.00      0.87        13\n          76       0.46      1.00      0.63         6\n          77       0.77      0.91      0.83        11\n          78       0.59      0.98      0.74        53\n          79       0.55      1.00      0.71         6\n          81       0.67      1.00      0.80         2\n          83       0.25      0.50      0.33         2\n          84       0.75      1.00      0.86         6\n          86       0.47      1.00      0.64         7\n          87       0.61      1.00      0.76        27\n          88       0.33      1.00      0.50         7\n          89       0.10      1.00      0.18         2\n          90       0.52      1.00      0.69        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.85      0.96      0.90        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.09      0.50      0.15         2\n         102       0.29      1.00      0.44         6\n         103       0.52      0.80      0.63        20\n         104       0.93      0.96      0.95       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.81        58\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.88      0.98      0.93       132\n         117       0.67      1.00      0.80         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.82      0.90      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.52      0.90      0.60     10056\nweighted avg       0.94      0.88      0.90     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutGRU02,"{'name': 'sequential_56', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_37'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_37', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_4', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_37', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_37', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9713734090328217,0.27511268854141235,0.16705193370580673,0.9667248427867889,0.2479265332221985,0.1738281026482582,2559.5,1016.5,388.5,1139.5,6422.5,0.6038208001987937,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.29      1.00      0.44         2\n           2       0.82      0.90      0.86        20\n           3       0.97      0.97      0.97       101\n           4       0.89      1.00      0.94         8\n           5       0.23      1.00      0.37         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       0.67      0.67      0.67         3\n           9       0.15      0.67      0.25         3\n          11       0.20      0.67      0.31         3\n          13       0.88      1.00      0.93         7\n          14       0.36      0.80      0.50         5\n          15       0.62      0.91      0.74        11\n          16       0.26      1.00      0.41        11\n          17       0.20      0.72      0.31        46\n          18       0.82      1.00      0.90        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.99      0.96      0.98       102\n          26       0.94      0.82      0.88        39\n          27       0.88      0.95      0.91        39\n          28       1.00      0.83      0.91        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.89      0.64         9\n          32       0.96      0.78      0.86        68\n          33       0.25      1.00      0.40         1\n          34       0.94      0.88      0.91        34\n          35       0.74      0.90      0.81        31\n          36       0.71      1.00      0.83         5\n          37       0.17      0.50      0.25         2\n          38       0.52      1.00      0.69        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.88      0.90        49\n          41       0.71      0.42      0.53        12\n          42       0.82      0.85      0.84        27\n          43       0.06      1.00      0.11         9\n          44       1.00      1.00      1.00         3\n          45       0.25      0.74      0.37        19\n          46       0.90      0.93      0.92       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       0.83      1.00      0.91         5\n          50       0.33      1.00      0.50         1\n          51       0.75      0.86      0.80         7\n          52       0.09      0.20      0.12         5\n          54       0.86      1.00      0.92         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.45      1.00      0.62        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.38      0.75      0.50         4\n          73       0.89      0.95      0.92       567\n          75       1.00      1.00      1.00        13\n          76       0.50      1.00      0.67         6\n          77       0.73      1.00      0.85        11\n          78       0.62      0.98      0.76        53\n          79       0.75      1.00      0.86         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      0.83      0.91         6\n          86       0.17      0.86      0.29         7\n          87       1.00      1.00      1.00        27\n          88       0.35      1.00      0.52         7\n          89       0.67      1.00      0.80         2\n          90       0.69      0.92      0.79        12\n          91       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.74      0.96      0.84        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      1.00      0.21         2\n         102       0.19      0.83      0.30         6\n         103       0.77      0.85      0.81        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.90      0.98      0.94       132\n         117       1.00      1.00      1.00         2\n         118       0.62      1.00      0.76         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.59      0.89      0.63     10056\nweighted avg       0.95      0.90      0.92     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.80      1.00      0.89        20\n           3       0.99      0.97      0.98       101\n           4       0.30      1.00      0.46         8\n           5       0.13      1.00      0.23         5\n           6       1.00      1.00      1.00         1\n           7       0.77      1.00      0.87        10\n           8       1.00      0.67      0.80         3\n           9       0.29      0.67      0.40         3\n          11       0.20      0.67      0.31         3\n          13       0.70      1.00      0.82         7\n          14       0.40      0.80      0.53         5\n          15       1.00      0.82      0.90        11\n          16       0.59      0.91      0.71        11\n          17       0.28      0.76      0.41        46\n          18       0.86      0.97      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.98      0.96      0.97       102\n          26       0.94      0.85      0.89        39\n          27       0.70      0.97      0.82        39\n          28       0.37      0.92      0.52        12\n          29       0.50      1.00      0.67         1\n          30       0.00      0.00      0.00         1\n          31       0.69      1.00      0.82         9\n          32       0.92      0.82      0.87        68\n          33       0.09      1.00      0.17         1\n          34       0.93      0.79      0.86        34\n          35       0.78      0.90      0.84        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.44      0.92      0.59        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.92      0.93        49\n          41       0.42      0.42      0.42        12\n          42       0.85      0.85      0.85        27\n          43       0.04      0.89      0.08         9\n          44       1.00      1.00      1.00         3\n          45       0.54      0.74      0.62        19\n          46       0.96      0.90      0.93       656\n          47       1.00      0.80      0.89         5\n          48       0.23      1.00      0.38         7\n          49       0.71      1.00      0.83         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.03      0.20      0.06         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.26      1.00      0.42        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.23      0.75      0.35         4\n          73       0.94      0.93      0.94       567\n          75       1.00      1.00      1.00        13\n          76       0.35      1.00      0.52         6\n          77       0.92      1.00      0.96        11\n          78       0.62      0.98      0.76        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      0.83      0.91         6\n          86       0.37      1.00      0.54         7\n          87       1.00      1.00      1.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.16         2\n          90       1.00      0.92      0.96        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.96      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.42      0.83      0.56         6\n         103       0.75      0.75      0.75        20\n         104       0.96      0.94      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         114       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.95      0.99      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.58      0.88      0.61     10056\nweighted avg       0.96      0.90      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutLSTM_6402,"{'name': 'sequential_58', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_38'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_38', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_17', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_5', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_38', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_38', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9673512578010559,0.26760073006153107,0.10505388677120209,0.9659492373466492,0.24049771577119827,0.17712166905403137,2566.5,1115.0,381.5,1226.5,6652.5,0.6125120864106166,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.72      0.90      0.80        20\n           3       0.96      0.98      0.97       101\n           4       0.89      1.00      0.94         8\n           5       0.13      0.80      0.23         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       1.00      1.00      1.00         3\n           9       0.33      1.00      0.50         3\n          11       0.38      1.00      0.55         3\n          12       0.00      1.00      0.00         0\n          13       0.70      1.00      0.82         7\n          14       0.44      0.80      0.57         5\n          15       0.92      1.00      0.96        11\n          16       0.35      1.00      0.52        11\n          17       0.23      0.78      0.35        46\n          18       0.82      0.97      0.89        33\n          19       0.86      1.00      0.92         6\n          20       0.79      1.00      0.88        61\n          22       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.97      0.98      0.98       102\n          26       0.87      0.87      0.87        39\n          27       0.79      0.97      0.87        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.38      0.67      0.48         9\n          32       0.81      0.82      0.82        68\n          33       1.00      1.00      1.00         1\n          34       0.90      0.79      0.84        34\n          35       1.00      0.90      0.95        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.44      1.00      0.62        12\n          39       0.75      0.86      0.80         7\n          40       0.77      0.88      0.82        49\n          41       0.71      0.42      0.53        12\n          42       0.68      0.85      0.75        27\n          43       0.07      0.89      0.13         9\n          44       1.00      1.00      1.00         3\n          45       0.33      0.84      0.47        19\n          46       0.92      0.93      0.93       656\n          47       1.00      0.80      0.89         5\n          48       1.00      0.86      0.92         7\n          49       0.62      1.00      0.77         5\n          50       1.00      1.00      1.00         1\n          51       0.55      0.86      0.67         7\n          52       0.08      0.20      0.11         5\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.95      0.92        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.31      0.93      0.46        14\n          70       0.00      1.00      0.00         0\n          71       0.50      1.00      0.67         2\n          72       0.17      0.75      0.27         4\n          73       0.89      0.95      0.92       567\n          74       0.00      1.00      0.00         0\n          75       0.76      1.00      0.87        13\n          76       0.86      1.00      0.92         6\n          77       0.48      1.00      0.65        11\n          78       0.56      0.98      0.71        53\n          79       0.55      1.00      0.71         6\n          81       0.67      1.00      0.80         2\n          83       0.00      0.00      0.00         2\n          84       0.86      1.00      0.92         6\n          86       0.50      1.00      0.67         7\n          87       0.66      1.00      0.79        27\n          88       0.47      1.00      0.64         7\n          89       1.00      1.00      1.00         2\n          90       0.57      1.00      0.73        12\n          93       0.50      1.00      0.67         2\n          95       0.74      0.96      0.84        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.27      1.00      0.43         6\n         103       0.65      0.85      0.74        20\n         104       0.94      0.95      0.95       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.74      0.98      0.84        58\n         112       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.90      0.98      0.94       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.60      0.90      0.65     10056\nweighted avg       0.95      0.90      0.92     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.82      0.90      0.86        20\n           3       0.92      0.99      0.95       101\n           4       0.36      1.00      0.53         8\n           5       0.12      0.60      0.21         5\n           6       1.00      0.00      0.00         1\n           7       0.45      1.00      0.62        10\n           8       1.00      1.00      1.00         3\n           9       0.17      1.00      0.29         3\n          11       0.43      1.00      0.60         3\n          12       0.00      1.00      0.00         0\n          13       0.86      0.86      0.86         7\n          14       0.44      0.80      0.57         5\n          15       0.91      0.91      0.91        11\n          16       0.24      1.00      0.39        11\n          17       0.21      0.76      0.33        46\n          18       0.76      0.97      0.85        33\n          19       0.75      1.00      0.86         6\n          20       0.80      1.00      0.89        61\n          24       0.85      0.92      0.88        12\n          25       0.94      0.98      0.96       102\n          26       0.85      0.87      0.86        39\n          27       0.49      0.97      0.66        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.20      1.00      0.33         1\n          31       0.50      0.78      0.61         9\n          32       0.85      0.84      0.84        68\n          33       1.00      1.00      1.00         1\n          34       0.97      0.85      0.91        34\n          35       0.82      0.87      0.84        31\n          36       0.56      1.00      0.71         5\n          37       0.25      0.50      0.33         2\n          38       0.44      1.00      0.62        12\n          39       0.75      0.86      0.80         7\n          40       0.93      0.86      0.89        49\n          41       0.56      0.42      0.48        12\n          42       0.77      0.85      0.81        27\n          43       0.11      0.89      0.19         9\n          44       0.75      1.00      0.86         3\n          45       0.29      0.79      0.42        19\n          46       0.93      0.94      0.94       656\n          47       0.80      0.80      0.80         5\n          48       0.88      1.00      0.93         7\n          49       0.56      1.00      0.71         5\n          50       0.00      0.00      0.00         1\n          51       0.60      0.86      0.71         7\n          52       0.12      0.20      0.15         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.67      1.00      0.80         6\n          57       0.00      1.00      0.00         0\n          62       0.89      0.97      0.93        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.41      0.93      0.57        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.12      0.75      0.20         4\n          73       0.88      0.94      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.72      1.00      0.84        13\n          76       0.33      1.00      0.50         6\n          77       0.46      1.00      0.63        11\n          78       0.59      0.98      0.74        53\n          79       0.75      1.00      0.86         6\n          81       0.50      1.00      0.67         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.44      1.00      0.61         7\n          87       0.69      1.00      0.82        27\n          88       0.54      1.00      0.70         7\n          89       0.10      1.00      0.17         2\n          90       0.58      0.92      0.71        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.74      0.96      0.84        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.43      1.00      0.60         6\n         103       0.67      0.80      0.73        20\n         104       0.91      0.96      0.94       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.82        58\n         115       0.56      1.00      0.71         5\n         116       0.95      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.55      0.87      0.60     10056\nweighted avg       0.95      0.90      0.92     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

LSTM_256,"{'name': 'sequential_61', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_40'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_40', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_18', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_40', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_40', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.979207307100296,0.2779979556798935,0.04788918048143387,0.9740317761898041,0.2504207640886307,0.14032430201768875,2612.0,805.5,336.0,917.5,5284.5,0.6491815355278348,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.78      0.90      0.84        20\n           3       0.92      0.97      0.94       101\n           4       0.67      1.00      0.80         8\n           5       0.25      1.00      0.40         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       0.60      1.00      0.75         3\n           9       0.38      1.00      0.55         3\n          11       0.60      1.00      0.75         3\n          13       0.78      1.00      0.88         7\n          14       0.57      0.80      0.67         5\n          15       0.77      0.91      0.83        11\n          16       0.30      1.00      0.46        11\n          17       0.27      0.80      0.40        46\n          18       0.88      0.91      0.90        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.97      0.98       102\n          26       0.92      0.90      0.91        39\n          27       0.90      0.97      0.94        39\n          28       0.73      0.92      0.81        12\n          29       0.50      1.00      0.67         1\n          30       0.33      1.00      0.50         1\n          31       0.70      0.78      0.74         9\n          32       0.86      0.84      0.85        68\n          33       0.50      1.00      0.67         1\n          34       1.00      0.85      0.92        34\n          35       0.71      0.87      0.78        31\n          36       0.71      1.00      0.83         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.92      0.93        49\n          41       0.62      0.42      0.50        12\n          42       0.77      0.85      0.81        27\n          43       0.15      0.89      0.26         9\n          44       1.00      1.00      1.00         3\n          45       0.43      0.84      0.57        19\n          46       0.94      0.94      0.94       656\n          47       0.80      0.80      0.80         5\n          48       1.00      1.00      1.00         7\n          49       0.62      1.00      0.77         5\n          50       0.17      1.00      0.29         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.97      0.94        64\n          63       0.60      1.00      0.75         3\n          64       1.00      0.00      0.00         1\n          66       0.38      1.00      0.55        14\n          71       1.00      1.00      1.00         2\n          72       0.25      0.75      0.38         4\n          73       0.89      0.95      0.92       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.67      1.00      0.80         6\n          77       0.69      1.00      0.81        11\n          78       0.66      0.98      0.79        53\n          79       0.55      1.00      0.71         6\n          81       0.50      1.00      0.67         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          86       0.50      1.00      0.67         7\n          87       0.87      1.00      0.93        27\n          88       0.64      1.00      0.78         7\n          89       0.67      1.00      0.80         2\n          90       0.65      0.92      0.76        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.32      1.00      0.48         6\n         103       0.70      0.80      0.74        20\n         104       0.93      0.97      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.88      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.97      0.98      0.98       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.90      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.62      0.91      0.66     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.67      0.90      0.77        20\n           3       0.94      0.96      0.95       101\n           4       0.73      1.00      0.84         8\n           5       0.24      0.80      0.36         5\n           6       1.00      1.00      1.00         1\n           7       0.67      1.00      0.80        10\n           8       1.00      1.00      1.00         3\n           9       0.30      1.00      0.46         3\n          11       0.43      1.00      0.60         3\n          13       1.00      0.86      0.92         7\n          14       0.50      0.80      0.62         5\n          15       0.77      0.91      0.83        11\n          16       0.41      1.00      0.58        11\n          17       0.33      0.83      0.47        46\n          18       0.86      0.91      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          24       0.77      0.83      0.80        12\n          25       0.96      0.97      0.97       102\n          26       0.97      0.92      0.95        39\n          27       0.93      0.95      0.94        39\n          28       0.69      0.92      0.79        12\n          29       0.50      1.00      0.67         1\n          30       0.25      1.00      0.40         1\n          31       0.50      0.78      0.61         9\n          32       0.93      0.84      0.88        68\n          33       1.00      1.00      1.00         1\n          34       0.93      0.79      0.86        34\n          35       0.74      0.84      0.79        31\n          36       0.71      1.00      0.83         5\n          37       0.25      0.50      0.33         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.88      0.94      0.91        49\n          41       0.71      0.42      0.53        12\n          42       0.75      0.89      0.81        27\n          43       0.15      1.00      0.26         9\n          44       0.75      1.00      0.86         3\n          45       0.42      0.89      0.58        19\n          46       0.94      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       0.62      1.00      0.77         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.22      0.40      0.29         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.39      1.00      0.56        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      0.75      0.38         4\n          73       0.86      0.95      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.87      1.00      0.93        13\n          76       0.46      1.00      0.63         6\n          77       0.92      1.00      0.96        11\n          78       0.67      0.98      0.79        53\n          79       0.50      1.00      0.67         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.44      1.00      0.61         7\n          87       0.79      1.00      0.89        27\n          88       0.41      1.00      0.58         7\n          89       0.67      1.00      0.80         2\n          90       0.50      0.92      0.65        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.72      0.96      0.82        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.33      1.00      0.50         6\n         103       0.64      0.80      0.71        20\n         104       0.94      0.97      0.96       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.83      0.98      0.90        58\n         115       0.45      1.00      0.62         5\n         116       0.96      0.99      0.97       132\n         117       0.40      1.00      0.57         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.61      0.91      0.66     10056\nweighted avg       0.95      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

GRU_256,"{'name': 'sequential_65', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_43'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_43', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 256, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_43', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_43', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9842313528060913,0.2894003838300705,0.032617902383208275,0.9756677150726318,0.2572271525859833,0.12605198845267296,2486.5,749.5,461.5,690.0,5525.0,0.6589290739716982,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.77      1.00      0.87        20\n           3       0.99      0.97      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.13      1.00      0.23         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       1.00      0.67      0.80         3\n           9       0.67      0.67      0.67         3\n          11       0.50      1.00      0.67         3\n          13       0.88      1.00      0.93         7\n          14       0.40      0.80      0.53         5\n          15       0.83      0.91      0.87        11\n          16       0.55      1.00      0.71        11\n          17       0.41      0.74      0.53        46\n          18       0.89      0.94      0.91        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.99      0.98      0.99       102\n          26       0.94      0.87      0.91        39\n          27       0.90      0.95      0.93        39\n          28       0.69      0.92      0.79        12\n          29       0.50      1.00      0.67         1\n          30       0.50      1.00      0.67         1\n          31       0.62      0.89      0.73         9\n          32       0.93      0.81      0.87        68\n          33       1.00      1.00      1.00         1\n          34       1.00      0.82      0.90        34\n          35       0.93      0.87      0.90        31\n          36       1.00      1.00      1.00         5\n          37       0.12      0.50      0.20         2\n          38       0.48      1.00      0.65        12\n          39       0.86      0.86      0.86         7\n          40       0.88      0.92      0.90        49\n          41       0.71      0.42      0.53        12\n          42       0.82      0.85      0.84        27\n          43       0.16      1.00      0.28         9\n          44       1.00      1.00      1.00         3\n          45       0.54      0.74      0.62        19\n          46       0.95      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.25      1.00      0.40         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.94      0.97      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.52      0.93      0.67        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.03      1.00      0.05         4\n          73       0.93      0.71      0.81       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.46      1.00      0.63         6\n          77       0.73      1.00      0.85        11\n          78       0.79      0.98      0.87        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.54      1.00      0.70         7\n          89       1.00      1.00      1.00         2\n          90       0.52      0.92      0.67        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.88      0.96      0.92        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.38      0.83      0.53         6\n         103       0.73      0.80      0.76        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.97      0.97      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.64      0.90      0.66     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.90      0.90      0.90        20\n           3       0.97      0.99      0.98       101\n           4       0.26      1.00      0.41         8\n           5       0.22      0.80      0.35         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       0.60      1.00      0.75         3\n           9       0.40      0.67      0.50         3\n          11       0.40      0.67      0.50         3\n          13       1.00      1.00      1.00         7\n          14       0.44      0.80      0.57         5\n          15       1.00      0.91      0.95        11\n          16       0.73      1.00      0.85        11\n          17       0.39      0.76      0.51        46\n          18       0.84      0.97      0.90        33\n          19       0.86      1.00      0.92         6\n          20       0.95      1.00      0.98        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.98      0.99       102\n          26       0.94      0.85      0.89        39\n          27       0.82      0.95      0.88        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.20      1.00      0.33         1\n          31       0.64      1.00      0.78         9\n          32       0.93      0.84      0.88        68\n          33       0.50      1.00      0.67         1\n          34       1.00      0.82      0.90        34\n          35       0.96      0.84      0.90        31\n          36       1.00      1.00      1.00         5\n          37       0.17      0.50      0.25         2\n          38       0.50      1.00      0.67        12\n          39       0.86      0.86      0.86         7\n          40       0.93      0.88      0.91        49\n          41       0.83      0.42      0.56        12\n          42       0.82      0.85      0.84        27\n          43       0.23      1.00      0.38         9\n          44       1.00      1.00      1.00         3\n          45       0.50      0.74      0.60        19\n          46       0.95      0.95      0.95       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.20      1.00      0.33         1\n          51       0.86      0.86      0.86         7\n          52       0.11      0.20      0.14         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.93      0.97      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.41      0.93      0.57        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.03      1.00      0.06         4\n          73       0.93      0.77      0.84       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.46      1.00      0.63         6\n          77       0.85      1.00      0.92        11\n          78       0.87      0.98      0.92        53\n          79       0.75      1.00      0.86         6\n          81       0.50      0.50      0.50         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.47      1.00      0.64         7\n          87       1.00      1.00      1.00        27\n          88       0.37      1.00      0.54         7\n          89       0.67      1.00      0.80         2\n          90       0.79      0.92      0.85        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          95       0.96      0.96      0.96        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n         100       0.00      1.00      0.00         0\n         101       0.09      0.50      0.15         2\n         102       0.33      0.83      0.48         6\n         103       0.58      0.75      0.65        20\n         104       0.96      0.95      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.29      1.00      0.45         5\n         116       0.98      0.95      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.66      0.89      0.68     10056\nweighted avg       0.97      0.94      0.95     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_16,"{'name': 'sequential_68', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_45'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_45', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_12', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_19', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_19', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 16, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_45', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_45', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9300515353679657,0.23670199513435364,0.5555495023727417,0.9303839206695557,0.21158644556999207,0.4401743859052658,2467.0,1996.5,481.0,2095.5,8902.0,0.48001069979742883,"['              precision    recall  f1-score   support\n\n           0       0.99      0.90      0.94      1702\n           1       0.06      1.00      0.11         2\n           2       0.80      1.00      0.89        20\n           3       0.81      0.99      0.89       101\n           4       0.17      1.00      0.29         8\n           5       0.20      1.00      0.33         5\n           6       0.14      1.00      0.25         1\n           7       0.20      0.90      0.33        10\n           8       0.30      1.00      0.46         3\n           9       0.14      0.33      0.20         3\n          11       0.50      0.33      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.20      0.57      0.30         7\n          14       0.20      0.20      0.20         5\n          15       0.69      0.82      0.75        11\n          16       0.37      0.91      0.53        11\n          17       0.25      0.80      0.38        46\n          18       0.92      1.00      0.96        33\n          19       0.60      1.00      0.75         6\n          20       0.88      1.00      0.94        61\n          23       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.44      0.90      0.60       102\n          26       0.82      0.82      0.82        39\n          27       0.51      0.97      0.67        39\n          28       0.48      0.92      0.63        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.38      0.56      0.45         9\n          32       0.70      0.72      0.71        68\n          33       0.17      1.00      0.29         1\n          34       0.80      0.71      0.75        34\n          35       0.87      0.84      0.85        31\n          36       0.56      1.00      0.71         5\n          37       0.50      0.50      0.50         2\n          38       0.27      1.00      0.43        12\n          39       0.86      0.86      0.86         7\n          40       0.31      0.98      0.47        49\n          41       0.24      0.42      0.30        12\n          42       0.74      0.85      0.79        27\n          43       0.09      0.89      0.16         9\n          44       0.75      1.00      0.86         3\n          45       0.31      0.84      0.46        19\n          46       0.82      0.93      0.87       656\n          47       1.00      1.00      1.00         5\n          48       0.20      1.00      0.33         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.58      1.00      0.74         7\n          52       0.10      0.60      0.17         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.29      1.00      0.44         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.25      1.00      0.41        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.29      1.00      0.44         2\n          72       0.19      1.00      0.32         4\n          73       0.60      0.91      0.72       567\n          75       0.76      1.00      0.87        13\n          76       0.30      1.00      0.46         6\n          77       0.16      0.73      0.26        11\n          78       0.56      0.98      0.71        53\n          79       0.46      1.00      0.63         6\n          80       0.00      1.00      0.00         0\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.50      0.67      0.57         6\n          86       0.38      0.71      0.50         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       0.10      1.00      0.18         2\n          90       0.61      0.92      0.73        12\n          92       0.00      1.00      0.00         0\n          93       0.25      0.50      0.33         2\n          94       0.00      1.00      0.00         0\n          95       0.72      0.88      0.79        24\n          97       0.25      1.00      0.40         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.05      0.50      0.09         2\n         102       0.29      0.83      0.43         6\n         103       0.50      0.70      0.58        20\n         104       0.84      0.96      0.90       110\n         105       0.40      1.00      0.57         2\n         106       1.00      0.00      0.00         1\n         108       0.17      1.00      0.29         2\n         111       0.64      0.98      0.78        58\n         112       0.00      1.00      0.00         0\n         115       0.38      1.00      0.56         5\n         116       0.77      0.96      0.86       132\n         117       1.00      1.00      1.00         2\n         118       0.27      1.00      0.42         8\n         119       0.99      0.72      0.84      5640\n\n    accuracy                           0.80     10056\n   macro avg       0.44      0.85      0.49     10056\nweighted avg       0.91      0.80      0.83     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.92      0.95      1702\n           1       0.17      1.00      0.29         2\n           2       0.57      1.00      0.73        20\n           3       0.89      0.99      0.94       101\n           4       0.21      1.00      0.35         8\n           5       0.16      1.00      0.28         5\n           6       0.33      1.00      0.50         1\n           7       0.36      0.80      0.50        10\n           8       0.07      0.67      0.13         3\n           9       0.12      0.67      0.20         3\n          11       0.50      0.33      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.05      0.29      0.09         7\n          14       0.14      0.20      0.17         5\n          15       0.67      0.73      0.70        11\n          16       0.20      0.91      0.33        11\n          17       0.21      0.76      0.33        46\n          18       0.85      1.00      0.92        33\n          19       0.67      1.00      0.80         6\n          20       0.95      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.69      0.92      0.79        12\n          25       0.82      0.95      0.88       102\n          26       0.79      0.87      0.83        39\n          27       0.48      0.95      0.64        39\n          28       0.38      0.92      0.54        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.89      0.64         9\n          32       0.57      0.81      0.67        68\n          33       0.09      1.00      0.17         1\n          34       0.79      0.68      0.73        34\n          35       0.86      0.81      0.83        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.33      0.92      0.49        12\n          39       0.86      0.86      0.86         7\n          40       0.66      0.96      0.78        49\n          41       0.05      0.42      0.10        12\n          42       0.71      0.89      0.79        27\n          43       0.22      1.00      0.36         9\n          44       0.19      1.00      0.32         3\n          45       0.32      0.89      0.47        19\n          46       0.90      0.93      0.91       656\n          47       0.83      1.00      0.91         5\n          48       0.27      1.00      0.42         7\n          49       0.62      1.00      0.77         5\n          50       0.07      1.00      0.12         1\n          51       0.40      0.86      0.55         7\n          52       0.07      0.40      0.12         5\n          54       0.62      0.83      0.71         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          62       0.82      0.98      0.89        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.28      0.86      0.42        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.07      1.00      0.13         2\n          72       0.14      1.00      0.25         4\n          73       0.62      0.92      0.74       567\n          74       0.00      1.00      0.00         0\n          75       0.62      1.00      0.76        13\n          76       0.17      0.83      0.29         6\n          77       0.40      0.91      0.56        11\n          78       0.55      0.98      0.71        53\n          79       0.19      0.83      0.30         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          83       0.00      0.00      0.00         2\n          84       0.25      0.67      0.36         6\n          86       0.29      0.86      0.43         7\n          87       0.82      1.00      0.90        27\n          88       0.35      1.00      0.52         7\n          89       0.17      1.00      0.29         2\n          90       0.55      0.92      0.69        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.75      0.50      0.60        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.07      0.50      0.12         2\n         102       0.25      0.83      0.38         6\n         103       0.38      0.80      0.52        20\n         104       0.83      0.96      0.89       110\n         105       1.00      0.50      0.67         2\n         106       1.00      0.00      0.00         1\n         108       0.29      1.00      0.44         2\n         111       0.70      0.98      0.81        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.12      1.00      0.21         5\n         116       0.91      0.95      0.93       132\n         117       0.67      1.00      0.80         2\n         118       0.37      0.88      0.52         8\n         119       0.99      0.74      0.85      5640\n\n    accuracy                           0.82     10056\n   macro avg       0.41      0.84      0.45     10056\nweighted avg       0.92      0.82      0.85     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_32,"{'name': 'sequential_71', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_47'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_47', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_13', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_20', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_20', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_47', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_47', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9593389630317688,0.2558201551437378,0.1525622010231018,0.9589331448078156,0.23291124403476715,0.2099323868751526,2611.0,1145.0,337.0,1282.5,6385.0,0.5770558908114095,"['              precision    recall  f1-score   support\n\n           0       0.99      0.94      0.97      1702\n           1       0.67      1.00      0.80         2\n           2       0.71      1.00      0.83        20\n           3       0.86      0.99      0.92       101\n           4       0.20      1.00      0.33         8\n           5       0.18      1.00      0.30         5\n           6       1.00      1.00      1.00         1\n           7       0.42      1.00      0.59        10\n           8       0.50      0.67      0.57         3\n           9       0.43      1.00      0.60         3\n          11       0.50      0.33      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.54      1.00      0.70         7\n          14       0.14      0.20      0.17         5\n          15       0.71      0.91      0.80        11\n          16       0.48      1.00      0.65        11\n          17       0.22      0.83      0.35        46\n          18       0.73      0.97      0.83        33\n          19       0.75      1.00      0.86         6\n          20       0.87      1.00      0.93        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.97      0.96      0.97       102\n          26       0.80      0.85      0.82        39\n          27       0.80      0.95      0.87        39\n          28       0.67      0.83      0.74        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.33      0.78      0.47         9\n          32       0.64      0.88      0.74        68\n          33       0.00      0.00      0.00         1\n          34       0.85      0.82      0.84        34\n          35       0.93      0.84      0.88        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.29      1.00      0.44        12\n          39       0.75      0.86      0.80         7\n          40       0.68      0.98      0.80        49\n          41       0.56      0.42      0.48        12\n          42       0.82      0.85      0.84        27\n          43       0.10      0.89      0.18         9\n          44       0.50      1.00      0.67         3\n          45       0.21      0.79      0.34        19\n          46       0.89      0.93      0.91       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.16      0.60      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.35      1.00      0.52         6\n          57       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.33      1.00      0.50        14\n          68       0.00      1.00      0.00         0\n          71       0.08      1.00      0.14         2\n          72       0.17      1.00      0.30         4\n          73       0.81      0.95      0.88       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.43      1.00      0.60         6\n          77       0.38      0.91      0.54        11\n          78       0.58      0.98      0.73        53\n          79       0.55      1.00      0.71         6\n          81       0.50      0.50      0.50         2\n          83       0.50      0.50      0.50         2\n          84       0.46      1.00      0.63         6\n          85       0.00      1.00      0.00         0\n          86       0.27      0.86      0.41         7\n          87       0.77      1.00      0.87        27\n          88       0.39      1.00      0.56         7\n          89       1.00      1.00      1.00         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.73      0.79      0.76        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.21      0.67      0.32         6\n         103       0.56      0.70      0.62        20\n         104       0.94      0.97      0.96       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.71      0.98      0.83        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.94      0.98      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.40      1.00      0.57         8\n         119       0.99      0.83      0.90      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.52      0.85      0.57     10056\nweighted avg       0.93      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      1702\n           1       0.29      1.00      0.44         2\n           2       0.63      0.95      0.76        20\n           3       0.93      0.99      0.96       101\n           4       0.20      1.00      0.33         8\n           5       0.25      0.80      0.38         5\n           6       1.00      1.00      1.00         1\n           7       0.45      1.00      0.62        10\n           8       0.23      1.00      0.38         3\n           9       0.38      1.00      0.55         3\n          11       0.33      1.00      0.50         3\n          13       0.35      1.00      0.52         7\n          14       0.44      0.80      0.57         5\n          15       0.85      1.00      0.92        11\n          16       0.46      1.00      0.63        11\n          17       0.28      0.83      0.42        46\n          18       0.94      0.97      0.96        33\n          19       0.86      1.00      0.92         6\n          20       0.77      1.00      0.87        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.85      0.94      0.89       102\n          26       0.81      0.90      0.85        39\n          27       0.82      0.95      0.88        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.62      0.82      0.71        68\n          33       0.25      1.00      0.40         1\n          34       0.93      0.76      0.84        34\n          35       0.88      0.90      0.89        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.30      1.00      0.46        12\n          39       0.86      0.86      0.86         7\n          40       0.75      0.96      0.84        49\n          41       1.00      0.42      0.59        12\n          42       0.82      0.85      0.84        27\n          43       0.32      0.89      0.47         9\n          44       0.38      1.00      0.55         3\n          45       0.44      0.89      0.59        19\n          46       0.92      0.95      0.94       656\n          47       0.80      0.80      0.80         5\n          48       1.00      1.00      1.00         7\n          49       0.83      1.00      0.91         5\n          50       0.00      0.00      0.00         1\n          51       0.50      0.86      0.63         7\n          52       0.21      0.60      0.32         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.40      1.00      0.57         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.88      0.98      0.93        64\n          63       0.60      1.00      0.75         3\n          64       0.33      1.00      0.50         1\n          65       0.00      1.00      0.00         0\n          66       0.41      1.00      0.58        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.24      1.00      0.38         4\n          73       0.87      0.96      0.91       567\n          75       0.86      0.92      0.89        13\n          76       0.67      1.00      0.80         6\n          77       0.44      1.00      0.61        11\n          78       0.58      0.98      0.73        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       0.50      1.00      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.28      0.71      0.40         7\n          87       0.84      1.00      0.92        27\n          88       0.35      1.00      0.52         7\n          89       1.00      1.00      1.00         2\n          90       0.52      0.92      0.67        12\n          93       0.50      0.50      0.50         2\n          95       0.92      0.96      0.94        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.07      0.50      0.12         2\n         102       0.27      0.67      0.38         6\n         103       0.64      0.70      0.67        20\n         104       0.95      0.95      0.95       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       0.67      1.00      0.80         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.92      0.99      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.87      0.93      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.51      0.90      0.57     10056\nweighted avg       0.95      0.90      0.92     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_64,"{'name': 'sequential_74', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_49'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_49', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_14', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_21', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_21', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_49', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_49', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9718983173370361,0.2676611989736557,0.05981721729040146,0.9685425460338593,0.24340195953845978,0.14365290105342865,2664.5,786.0,283.5,946.5,5279.5,0.6370301983583067,"['              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      1702\n           1       0.33      1.00      0.50         2\n           2       0.91      1.00      0.95        20\n           3       0.93      0.99      0.96       101\n           4       0.44      1.00      0.62         8\n           5       0.29      1.00      0.45         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       0.60      1.00      0.75         3\n           9       0.75      1.00      0.86         3\n          11       0.50      0.67      0.57         3\n          13       0.41      1.00      0.58         7\n          14       0.36      0.80      0.50         5\n          15       0.85      1.00      0.92        11\n          16       0.52      1.00      0.69        11\n          17       0.31      0.87      0.46        46\n          18       0.89      1.00      0.94        33\n          19       0.86      1.00      0.92         6\n          20       0.94      1.00      0.97        61\n          24       0.92      1.00      0.96        12\n          25       0.98      0.98      0.98       102\n          26       0.75      0.92      0.83        39\n          27       0.86      0.97      0.92        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.56      1.00      0.72         9\n          32       0.83      0.87      0.85        68\n          33       0.50      1.00      0.67         1\n          34       0.90      0.82      0.86        34\n          35       0.85      0.94      0.89        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.35      1.00      0.52        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.98      0.97        49\n          41       0.71      0.42      0.53        12\n          42       0.82      0.85      0.84        27\n          43       0.41      1.00      0.58         9\n          44       1.00      1.00      1.00         3\n          45       0.45      0.74      0.56        19\n          46       0.91      0.96      0.93       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       0.71      1.00      0.83         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.21      0.60      0.32         5\n          54       0.83      0.83      0.83         6\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.39      1.00      0.56        14\n          71       1.00      1.00      1.00         2\n          72       0.29      1.00      0.44         4\n          73       0.91      0.96      0.93       567\n          75       1.00      1.00      1.00        13\n          76       1.00      1.00      1.00         6\n          77       0.46      1.00      0.63        11\n          78       0.58      0.98      0.73        53\n          79       0.60      1.00      0.75         6\n          81       0.67      1.00      0.80         2\n          82       0.00      1.00      0.00         0\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.44      1.00      0.61         7\n          87       1.00      1.00      1.00        27\n          88       0.47      1.00      0.64         7\n          89       0.33      1.00      0.50         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.44      0.67      0.53         6\n         103       0.74      0.70      0.72        20\n         104       0.95      0.98      0.96       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.90      0.98      0.94        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.94      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.91      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.64      0.89      0.68     10056\nweighted avg       0.95      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      1702\n           1       0.50      1.00      0.67         2\n           2       0.73      0.95      0.83        20\n           3       0.91      0.99      0.95       101\n           4       0.33      1.00      0.50         8\n           5       0.33      1.00      0.50         5\n           6       1.00      0.00      0.00         1\n           7       0.59      1.00      0.74        10\n           8       0.33      1.00      0.50         3\n           9       0.75      1.00      0.86         3\n          11       0.75      1.00      0.86         3\n          13       0.78      1.00      0.88         7\n          14       0.36      0.80      0.50         5\n          15       0.85      1.00      0.92        11\n          16       0.65      1.00      0.79        11\n          17       0.25      0.87      0.38        46\n          18       0.80      0.97      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.84      1.00      0.91        61\n          24       0.86      1.00      0.92        12\n          25       0.97      0.98      0.98       102\n          26       0.68      0.87      0.76        39\n          27       0.79      0.97      0.87        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.69      1.00      0.82         9\n          32       0.79      0.85      0.82        68\n          33       0.00      0.00      0.00         1\n          34       0.91      0.88      0.90        34\n          35       0.90      0.90      0.90        31\n          36       1.00      1.00      1.00         5\n          37       0.33      0.50      0.40         2\n          38       0.32      1.00      0.48        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.96      0.95        49\n          41       0.50      0.42      0.45        12\n          42       0.74      0.85      0.79        27\n          43       0.27      1.00      0.43         9\n          44       1.00      1.00      1.00         3\n          45       0.35      0.74      0.47        19\n          46       0.94      0.96      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      0.86      0.92         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.36      0.80      0.50         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.38      0.93      0.54        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.19      1.00      0.32         4\n          73       0.89      0.96      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.86      1.00      0.92         6\n          77       0.40      0.91      0.56        11\n          78       0.55      0.98      0.71        53\n          79       0.50      1.00      0.67         6\n          81       0.67      1.00      0.80         2\n          82       0.00      1.00      0.00         0\n          83       0.67      1.00      0.80         2\n          84       1.00      1.00      1.00         6\n          86       0.35      1.00      0.52         7\n          87       0.96      1.00      0.98        27\n          88       0.44      1.00      0.61         7\n          89       0.67      1.00      0.80         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.73      0.92      0.81        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.29      0.67      0.40         6\n         103       0.76      0.65      0.70        20\n         104       0.93      0.98      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.88      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.96      0.99      0.98       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.88      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.62      0.89      0.65     10056\nweighted avg       0.95      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_128,"{'name': 'sequential_77', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_51'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_51', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_15', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_22', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_22', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_51', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_51', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9817047715187073,0.27685101330280304,0.03248361870646477,0.978103369474411,0.2545141130685806,0.10320678725838661,2688.0,552.0,260.0,713.5,4198.5,0.6736388664483479,"['              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1702\n           1       0.50      1.00      0.67         2\n           2       0.91      1.00      0.95        20\n           3       0.97      0.99      0.98       101\n           4       0.42      1.00      0.59         8\n           5       0.31      1.00      0.48         5\n           6       1.00      1.00      1.00         1\n           7       0.67      1.00      0.80        10\n           8       0.50      1.00      0.67         3\n           9       0.43      1.00      0.60         3\n          11       0.50      1.00      0.67         3\n          13       0.78      1.00      0.88         7\n          14       0.33      0.80      0.47         5\n          15       0.91      0.91      0.91        11\n          16       0.69      1.00      0.81        11\n          17       0.35      0.87      0.50        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          24       0.92      1.00      0.96        12\n          25       0.97      0.98      0.98       102\n          26       0.90      0.92      0.91        39\n          27       0.86      0.95      0.90        39\n          28       0.79      0.92      0.85        12\n          29       0.50      1.00      0.67         1\n          30       0.50      1.00      0.67         1\n          31       0.67      0.89      0.76         9\n          32       0.84      0.87      0.86        68\n          33       0.50      1.00      0.67         1\n          34       0.90      0.82      0.86        34\n          35       0.97      0.94      0.95        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.38      1.00      0.55        12\n          39       0.86      0.86      0.86         7\n          40       0.81      0.98      0.89        49\n          41       0.71      0.42      0.53        12\n          42       0.78      0.93      0.85        27\n          43       0.35      1.00      0.51         9\n          44       1.00      1.00      1.00         3\n          45       0.39      0.79      0.53        19\n          46       0.95      0.96      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       0.83      1.00      0.91         5\n          50       0.33      1.00      0.50         1\n          51       0.55      0.86      0.67         7\n          52       0.50      0.60      0.55         5\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          62       0.89      0.98      0.93        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.45      1.00      0.62        14\n          71       1.00      1.00      1.00         2\n          72       0.27      1.00      0.42         4\n          73       0.91      0.96      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.86      1.00      0.92         6\n          77       0.43      0.91      0.59        11\n          78       0.58      0.98      0.73        53\n          79       0.75      1.00      0.86         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.64      1.00      0.78         7\n          89       0.33      1.00      0.50         2\n          90       0.58      0.92      0.71        12\n          93       0.67      1.00      0.80         2\n          95       0.77      0.96      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.27      0.67      0.38         6\n         103       0.70      0.70      0.70        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.83      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.67      0.91      0.72     10056\nweighted avg       0.96      0.94      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.91      1.00      0.95        20\n           3       0.99      0.99      0.99       101\n           4       0.35      1.00      0.52         8\n           5       0.36      1.00      0.53         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       1.00      1.00      1.00         3\n           9       1.00      1.00      1.00         3\n          11       0.60      1.00      0.75         3\n          13       0.78      1.00      0.88         7\n          14       0.30      0.60      0.40         5\n          15       1.00      0.91      0.95        11\n          16       0.79      1.00      0.88        11\n          17       0.55      0.89      0.68        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.99      0.98      0.99       102\n          26       0.88      0.92      0.90        39\n          27       0.93      0.95      0.94        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.73      0.89      0.80         9\n          32       0.92      0.85      0.89        68\n          33       0.00      0.00      0.00         1\n          34       0.96      0.79      0.87        34\n          35       0.94      0.94      0.94        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.98      0.95        49\n          41       0.83      0.42      0.56        12\n          42       0.82      0.85      0.84        27\n          43       0.47      1.00      0.64         9\n          44       1.00      1.00      1.00         3\n          45       0.60      0.79      0.68        19\n          46       0.97      0.96      0.96       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.43      0.60      0.50         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       0.50      1.00      0.67         1\n          66       0.52      1.00      0.68        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      1.00      0.42         4\n          73       0.93      0.97      0.95       567\n          75       0.93      1.00      0.96        13\n          76       0.75      1.00      0.86         6\n          77       0.58      1.00      0.73        11\n          78       0.60      0.98      0.74        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.58      1.00      0.74         7\n          87       1.00      1.00      1.00        27\n          88       0.54      1.00      0.70         7\n          89       0.67      1.00      0.80         2\n          90       0.61      0.92      0.73        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.31      0.67      0.42         6\n         103       0.74      0.70      0.72        20\n         104       0.95      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.98      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.67      0.92      0.70     10056\nweighted avg       0.97      0.95      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_ave_32,"{'name': 'sequential_80', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_53'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_53', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_16', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_23', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'ave', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_23', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_53', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_53', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9506737887859344,0.25038009881973267,0.5858564674854279,0.9477485716342926,0.2203524261713028,0.3744902163743973,2498.5,1671.5,449.5,1835.0,7931.0,0.5274776085944779,"['              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1702\n           1       0.12      1.00      0.21         2\n           2       0.56      1.00      0.71        20\n           3       0.83      0.99      0.90       101\n           4       0.19      1.00      0.32         8\n           5       0.16      1.00      0.27         5\n           6       1.00      1.00      1.00         1\n           7       0.38      1.00      0.56        10\n           8       1.00      0.67      0.80         3\n           9       0.40      0.67      0.50         3\n          11       0.33      0.33      0.33         3\n          13       0.38      0.86      0.52         7\n          14       0.15      0.40      0.22         5\n          15       0.73      1.00      0.85        11\n          16       0.38      1.00      0.55        11\n          17       0.16      0.80      0.26        46\n          18       0.97      1.00      0.99        33\n          19       0.75      1.00      0.86         6\n          20       0.70      1.00      0.82        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.67      1.00      0.80        12\n          25       0.54      0.88      0.67       102\n          26       0.78      0.92      0.85        39\n          27       0.74      0.95      0.83        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.56      1.00      0.72         9\n          32       0.72      0.78      0.75        68\n          33       0.00      0.00      0.00         1\n          34       0.87      0.79      0.83        34\n          35       0.58      0.84      0.68        31\n          36       0.83      1.00      0.91         5\n          37       0.33      0.50      0.40         2\n          38       0.40      1.00      0.57        12\n          39       0.67      0.86      0.75         7\n          40       0.80      0.96      0.87        49\n          41       0.62      0.42      0.50        12\n          42       0.71      0.89      0.79        27\n          43       0.08      0.89      0.14         9\n          44       0.60      1.00      0.75         3\n          45       0.18      0.74      0.29        19\n          46       0.80      0.94      0.86       656\n          47       0.80      0.80      0.80         5\n          48       0.64      1.00      0.78         7\n          49       0.71      1.00      0.83         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.14      0.60      0.23         5\n          54       0.83      0.83      0.83         6\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          62       0.88      0.98      0.93        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.27      1.00      0.42        14\n          71       0.50      1.00      0.67         2\n          72       0.18      0.75      0.29         4\n          73       0.71      0.94      0.81       567\n          74       0.00      1.00      0.00         0\n          75       0.92      0.92      0.92        13\n          76       0.75      1.00      0.86         6\n          77       0.57      0.73      0.64        11\n          78       0.54      0.98      0.69        53\n          79       0.60      1.00      0.75         6\n          81       1.00      0.50      0.67         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.71      0.83      0.77         6\n          85       0.00      1.00      0.00         0\n          86       0.27      0.86      0.41         7\n          87       0.79      1.00      0.89        27\n          88       0.35      1.00      0.52         7\n          89       0.10      1.00      0.18         2\n          90       0.77      0.83      0.80        12\n          92       0.00      1.00      0.00         0\n          93       0.25      0.50      0.33         2\n          95       0.72      0.96      0.82        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.24      1.00      0.39         6\n         103       0.37      0.65      0.47        20\n         104       0.93      0.96      0.95       110\n         105       0.50      0.50      0.50         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.86      0.98      0.92        58\n         112       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.90      0.98      0.94       132\n         117       0.50      1.00      0.67         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.76      0.86      5640\n\n    accuracy                           0.84     10056\n   macro avg       0.52      0.84      0.56     10056\nweighted avg       0.92      0.84      0.86     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1702\n           1       0.25      1.00      0.40         2\n           2       0.56      1.00      0.71        20\n           3       0.95      0.99      0.97       101\n           4       0.25      1.00      0.40         8\n           5       0.15      0.80      0.26         5\n           6       1.00      0.00      0.00         1\n           7       0.60      0.90      0.72        10\n           8       0.33      0.67      0.44         3\n           9       0.40      0.67      0.50         3\n          11       1.00      0.00      0.00         3\n          13       0.17      0.43      0.24         7\n          14       0.14      0.20      0.17         5\n          15       0.56      0.91      0.69        11\n          16       0.61      1.00      0.76        11\n          17       0.16      0.85      0.27        46\n          18       0.75      1.00      0.86        33\n          19       0.86      1.00      0.92         6\n          20       0.67      1.00      0.80        61\n          21       0.00      1.00      0.00         0\n          24       0.71      1.00      0.83        12\n          25       0.96      0.94      0.95       102\n          26       0.78      0.79      0.78        39\n          27       0.61      0.95      0.74        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.43      0.67      0.52         9\n          32       0.73      0.71      0.72        68\n          33       0.00      0.00      0.00         1\n          34       0.86      0.71      0.77        34\n          35       0.81      0.84      0.83        31\n          36       0.71      1.00      0.83         5\n          37       0.50      0.50      0.50         2\n          38       0.26      1.00      0.41        12\n          39       0.86      0.86      0.86         7\n          40       0.56      0.96      0.71        49\n          41       0.45      0.42      0.43        12\n          42       0.79      0.85      0.82        27\n          43       0.14      1.00      0.24         9\n          44       1.00      1.00      1.00         3\n          45       0.27      0.74      0.39        19\n          46       0.90      0.93      0.91       656\n          47       1.00      1.00      1.00         5\n          48       0.75      0.86      0.80         7\n          49       0.50      1.00      0.67         5\n          50       0.00      0.00      0.00         1\n          51       0.50      0.86      0.63         7\n          52       0.10      0.40      0.16         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.30      1.00      0.46         6\n          57       0.00      1.00      0.00         0\n          62       0.71      0.98      0.82        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.29      0.86      0.43        14\n          71       1.00      1.00      1.00         2\n          72       0.24      1.00      0.38         4\n          73       0.70      0.96      0.81       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.75      1.00      0.86         6\n          77       0.30      0.82      0.44        11\n          78       0.58      0.98      0.73        53\n          79       0.42      0.83      0.56         6\n          81       0.00      0.00      0.00         2\n          83       0.50      0.50      0.50         2\n          84       0.36      0.67      0.47         6\n          86       0.33      0.57      0.42         7\n          87       0.93      1.00      0.96        27\n          88       0.33      1.00      0.50         7\n          89       0.12      1.00      0.21         2\n          90       0.59      0.83      0.69        12\n          92       0.00      1.00      0.00         0\n          93       0.25      1.00      0.40         2\n          94       0.00      1.00      0.00         0\n          95       0.66      0.79      0.72        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.07      0.50      0.12         2\n         102       0.20      0.67      0.31         6\n         103       0.46      0.65      0.54        20\n         104       0.90      0.95      0.92       110\n         105       0.33      0.50      0.40         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.69      0.98      0.81        58\n         112       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.90      0.99      0.94       132\n         117       0.33      1.00      0.50         2\n         118       0.38      1.00      0.55         8\n         119       0.99      0.79      0.88      5640\n\n    accuracy                           0.86     10056\n   macro avg       0.50      0.80      0.54     10056\nweighted avg       0.93      0.86      0.88     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_sum_32,"{'name': 'sequential_83', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_55'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_55', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_17', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_24', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'sum', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_24', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_55', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_55', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.9602266848087311,0.25814008712768555,0.1854895055294037,0.9565945267677307,0.23187577724456787,0.25318434089422226,2584.5,1162.5,363.5,1325.5,6489.0,0.5580659453115707,"['              precision    recall  f1-score   support\n\n           0       0.99      0.94      0.96      1702\n           1       0.29      1.00      0.44         2\n           2       0.71      1.00      0.83        20\n           3       0.96      0.99      0.98       101\n           4       0.33      1.00      0.50         8\n           5       0.21      1.00      0.34         5\n           6       1.00      0.00      0.00         1\n           7       0.53      1.00      0.69        10\n           8       0.50      1.00      0.67         3\n           9       0.38      1.00      0.55         3\n          11       0.43      1.00      0.60         3\n          13       0.32      1.00      0.48         7\n          14       0.15      0.40      0.22         5\n          15       0.64      0.82      0.72        11\n          16       0.61      1.00      0.76        11\n          17       0.17      0.87      0.28        46\n          18       0.94      1.00      0.97        33\n          19       0.67      1.00      0.80         6\n          20       0.98      1.00      0.99        61\n          24       0.73      0.92      0.81        12\n          25       0.87      0.97      0.92       102\n          26       0.89      0.87      0.88        39\n          27       0.58      0.92      0.71        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.50      0.67      0.57         9\n          32       0.63      0.81      0.71        68\n          33       0.00      0.00      0.00         1\n          34       0.86      0.74      0.79        34\n          35       0.74      0.90      0.81        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.32      1.00      0.48        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.96      0.95        49\n          41       1.00      0.42      0.59        12\n          42       0.85      0.85      0.85        27\n          43       0.33      0.89      0.48         9\n          44       1.00      1.00      1.00         3\n          45       0.33      0.74      0.46        19\n          46       0.96      0.93      0.94       656\n          47       1.00      0.80      0.89         5\n          48       1.00      0.86      0.92         7\n          49       0.22      1.00      0.36         5\n          50       0.14      1.00      0.25         1\n          51       0.55      0.86      0.67         7\n          52       0.19      0.60      0.29         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.43      1.00      0.60         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.85      0.98      0.91        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.35      0.93      0.51        14\n          68       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.24      1.00      0.38         4\n          73       0.82      0.96      0.88       567\n          75       0.81      1.00      0.90        13\n          76       0.67      1.00      0.80         6\n          77       0.59      0.91      0.71        11\n          78       0.54      0.98      0.70        53\n          79       0.55      1.00      0.71         6\n          81       0.67      1.00      0.80         2\n          83       0.00      0.00      0.00         2\n          84       0.62      0.83      0.71         6\n          85       0.00      1.00      0.00         0\n          86       0.18      0.86      0.29         7\n          87       0.77      1.00      0.87        27\n          88       0.29      1.00      0.45         7\n          89       0.10      1.00      0.18         2\n          90       0.69      0.92      0.79        12\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.24      1.00      0.39         6\n         103       0.70      0.70      0.70        20\n         104       0.93      0.97      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.29      1.00      0.44         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.91      0.99      0.95       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.55      0.87      0.59     10056\nweighted avg       0.94      0.89      0.91     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      1702\n           1       0.33      1.00      0.50         2\n           2       0.62      1.00      0.77        20\n           3       0.81      0.99      0.89       101\n           4       0.31      1.00      0.47         8\n           5       0.29      1.00      0.45         5\n           6       1.00      1.00      1.00         1\n           7       0.42      1.00      0.59        10\n           8       0.40      0.67      0.50         3\n           9       0.18      0.67      0.29         3\n          11       0.14      0.33      0.20         3\n          12       0.00      1.00      0.00         0\n          13       0.17      1.00      0.29         7\n          14       0.12      0.20      0.15         5\n          15       0.58      1.00      0.73        11\n          16       0.52      1.00      0.69        11\n          17       0.22      0.85      0.35        46\n          18       0.94      1.00      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.95      1.00      0.98        61\n          23       0.00      1.00      0.00         0\n          24       0.60      1.00      0.75        12\n          25       0.87      0.97      0.92       102\n          26       0.81      0.87      0.84        39\n          27       0.57      0.95      0.71        39\n          28       1.00      0.83      0.91        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.42      0.89      0.57         9\n          32       0.80      0.87      0.83        68\n          33       0.00      0.00      0.00         1\n          34       0.97      0.82      0.89        34\n          35       0.74      0.90      0.81        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.24      1.00      0.39        12\n          39       0.75      0.86      0.80         7\n          40       0.80      0.98      0.88        49\n          41       0.62      0.42      0.50        12\n          42       0.92      0.85      0.88        27\n          43       0.26      1.00      0.41         9\n          44       1.00      1.00      1.00         3\n          45       0.43      0.68      0.53        19\n          46       0.89      0.94      0.91       656\n          47       1.00      0.80      0.89         5\n          48       1.00      0.71      0.83         7\n          49       0.83      1.00      0.91         5\n          50       0.25      1.00      0.40         1\n          51       0.54      1.00      0.70         7\n          52       0.12      0.40      0.18         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.35      1.00      0.52         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.35      1.00      0.52        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.80      0.95      0.87       567\n          74       0.00      1.00      0.00         0\n          75       0.81      1.00      0.90        13\n          76       0.50      1.00      0.67         6\n          77       0.67      0.91      0.77        11\n          78       0.55      0.98      0.70        53\n          79       0.29      1.00      0.44         6\n          81       0.33      0.50      0.40         2\n          83       0.00      0.00      0.00         2\n          84       0.67      1.00      0.80         6\n          85       0.00      1.00      0.00         0\n          86       0.35      0.86      0.50         7\n          87       1.00      1.00      1.00        27\n          88       0.27      1.00      0.42         7\n          89       0.09      1.00      0.16         2\n          90       0.61      0.92      0.73        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          95       0.88      0.88      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.19      0.67      0.30         6\n         103       0.42      0.65      0.51        20\n         104       0.95      0.95      0.95       110\n         105       1.00      0.50      0.67         2\n         106       1.00      1.00      1.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.92      0.99      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.84      0.91      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.50      0.87      0.54     10056\nweighted avg       0.94      0.88      0.90     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_ave_64,"{'name': 'sequential_86', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_57'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_57', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_18', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_25', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'ave', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_25', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_57', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_57', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9637529253959656,0.2631366103887558,0.16590796411037445,0.960072249174118,0.23816028237342834,0.23711568862199783,2610.5,1123.5,337.5,1287.5,6480.5,0.6113565639239821,"['              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1702\n           1       0.40      1.00      0.57         2\n           2       0.63      0.95      0.76        20\n           3       0.93      0.99      0.96       101\n           4       0.33      1.00      0.50         8\n           5       0.12      1.00      0.22         5\n           6       1.00      0.00      0.00         1\n           7       0.48      1.00      0.65        10\n           8       0.75      1.00      0.86         3\n           9       0.43      1.00      0.60         3\n          11       0.40      0.67      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.88      1.00      0.93         7\n          14       0.15      0.60      0.24         5\n          15       0.62      0.91      0.74        11\n          16       0.73      1.00      0.85        11\n          17       0.20      0.83      0.33        46\n          18       0.94      1.00      0.97        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.80      1.00      0.89        12\n          25       0.78      0.98      0.87       102\n          26       0.85      0.87      0.86        39\n          27       0.84      0.95      0.89        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.62      0.89      0.73         9\n          32       0.83      0.87      0.85        68\n          33       0.00      0.00      0.00         1\n          34       0.90      0.79      0.84        34\n          35       0.82      0.87      0.84        31\n          36       0.83      1.00      0.91         5\n          37       0.33      0.50      0.40         2\n          38       0.26      1.00      0.41        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.98      0.96        49\n          41       0.62      0.42      0.50        12\n          42       0.82      0.85      0.84        27\n          43       0.23      0.78      0.36         9\n          44       1.00      1.00      1.00         3\n          45       0.34      0.74      0.47        19\n          46       0.91      0.94      0.92       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.20      0.60      0.30         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.34      1.00      0.51        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.20      1.00      0.33         4\n          73       0.79      0.97      0.87       567\n          75       0.93      1.00      0.96        13\n          76       0.86      1.00      0.92         6\n          77       0.50      1.00      0.67        11\n          78       0.58      0.98      0.73        53\n          79       0.60      1.00      0.75         6\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.67      1.00      0.80         6\n          86       0.54      1.00      0.70         7\n          87       0.96      1.00      0.98        27\n          88       0.35      1.00      0.52         7\n          89       0.18      1.00      0.31         2\n          90       0.73      0.92      0.81        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.75      0.88      0.81        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.38      0.83      0.53         6\n         103       0.41      0.60      0.49        20\n         104       0.94      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.40      1.00      0.57         2\n         111       0.86      0.98      0.92        58\n         115       0.50      1.00      0.67         5\n         116       0.94      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.56      0.86      0.61     10056\nweighted avg       0.94      0.90      0.91     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.95      0.97      1702\n           1       0.29      1.00      0.44         2\n           2       0.69      1.00      0.82        20\n           3       0.95      0.99      0.97       101\n           4       0.29      1.00      0.44         8\n           5       0.17      1.00      0.29         5\n           6       1.00      1.00      1.00         1\n           7       0.45      1.00      0.62        10\n           8       0.60      1.00      0.75         3\n           9       0.33      1.00      0.50         3\n          11       0.50      0.67      0.57         3\n          13       0.55      0.86      0.67         7\n          14       0.30      0.60      0.40         5\n          15       0.79      1.00      0.88        11\n          16       0.69      1.00      0.81        11\n          17       0.24      0.87      0.38        46\n          18       0.97      1.00      0.99        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          23       0.00      1.00      0.00         0\n          24       0.86      1.00      0.92        12\n          25       0.92      0.98      0.95       102\n          26       0.87      0.85      0.86        39\n          27       0.48      0.97      0.64        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.53      0.89      0.67         9\n          32       0.67      0.82      0.74        68\n          33       0.33      1.00      0.50         1\n          34       0.93      0.82      0.88        34\n          35       0.78      0.90      0.84        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.28      1.00      0.44        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.96      0.95        49\n          41       1.00      0.42      0.59        12\n          42       0.82      0.85      0.84        27\n          43       0.43      1.00      0.60         9\n          44       1.00      1.00      1.00         3\n          45       0.25      0.74      0.38        19\n          46       0.87      0.94      0.91       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.83      1.00      0.91         5\n          50       0.25      1.00      0.40         1\n          51       0.55      0.86      0.67         7\n          52       0.16      0.60      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.38      1.00      0.55         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.47        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.84      0.96      0.89       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.59      0.91      0.71        11\n          78       0.58      0.98      0.73        53\n          79       0.35      1.00      0.52         6\n          81       0.67      1.00      0.80         2\n          83       0.67      1.00      0.80         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.12      0.71      0.21         7\n          87       1.00      1.00      1.00        27\n          88       0.47      1.00      0.64         7\n          89       0.33      1.00      0.50         2\n          90       0.61      0.92      0.73        12\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.36      0.67      0.47         6\n         103       0.52      0.60      0.56        20\n         104       0.94      0.96      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.40      1.00      0.57         2\n         109       0.00      1.00      0.00         0\n         111       0.83      0.98      0.90        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.90      0.98      0.94       132\n         117       1.00      1.00      1.00         2\n         118       0.50      0.88      0.64         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.58      0.91      0.62     10056\nweighted avg       0.94      0.90      0.91     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalLSTM_Max_sum_64,"{'name': 'sequential_89', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_59'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_59', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_19', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_26', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'sum', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_26', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_59', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_59', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9739110171794891,0.2693236619234085,0.06201178766787052,0.9704935550689697,0.2435157597064972,0.1443718522787094,2659.0,729.0,289.0,874.0,4905.5,0.6420440200815094,"['              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      1702\n           1       0.40      1.00      0.57         2\n           2       0.83      0.95      0.88        20\n           3       0.98      0.99      0.99       101\n           4       0.35      1.00      0.52         8\n           5       0.33      0.80      0.47         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       0.50      1.00      0.67         3\n           9       0.75      1.00      0.86         3\n          10       0.00      1.00      0.00         0\n          11       0.33      0.67      0.44         3\n          12       0.00      1.00      0.00         0\n          13       0.58      1.00      0.74         7\n          14       0.44      0.80      0.57         5\n          15       0.73      1.00      0.85        11\n          16       0.52      1.00      0.69        11\n          17       0.33      0.85      0.48        46\n          18       0.94      0.97      0.96        33\n          19       0.86      1.00      0.92         6\n          20       0.97      1.00      0.98        61\n          24       1.00      1.00      1.00        12\n          25       0.98      0.98      0.98       102\n          26       0.88      0.90      0.89        39\n          27       0.82      0.95      0.88        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.53      0.89      0.67         9\n          32       0.91      0.85      0.88        68\n          33       0.00      0.00      0.00         1\n          34       0.85      0.82      0.84        34\n          35       0.93      0.90      0.92        31\n          36       1.00      1.00      1.00         5\n          37       0.33      0.50      0.40         2\n          38       0.36      1.00      0.53        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.98      0.96        49\n          41       0.83      0.42      0.56        12\n          42       0.77      0.85      0.81        27\n          43       0.50      1.00      0.67         9\n          44       1.00      1.00      1.00         3\n          45       0.47      0.79      0.59        19\n          46       0.94      0.95      0.94       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.71      1.00      0.83         5\n          50       1.00      0.00      0.00         1\n          51       0.60      0.86      0.71         7\n          52       0.18      0.40      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.38      0.93      0.54        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.25      1.00      0.40         4\n          73       0.95      0.96      0.95       567\n          75       1.00      1.00      1.00        13\n          76       0.67      1.00      0.80         6\n          77       0.37      0.91      0.53        11\n          78       0.59      0.98      0.74        53\n          79       0.60      1.00      0.75         6\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.00      0.00      0.00         2\n          84       0.75      1.00      0.86         6\n          86       0.28      1.00      0.44         7\n          87       1.00      1.00      1.00        27\n          88       0.41      1.00      0.58         7\n          89       0.22      1.00      0.36         2\n          90       0.61      0.92      0.73        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.40      0.67      0.50         6\n         103       0.59      0.65      0.62        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.67      1.00      0.80         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         115       0.45      1.00      0.62         5\n         116       0.93      0.99      0.96       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.92      0.95      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.60      0.87      0.63     10056\nweighted avg       0.96      0.93      0.94     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.96      0.98      1702\n           1       0.67      1.00      0.80         2\n           2       0.74      1.00      0.85        20\n           3       0.96      0.99      0.98       101\n           4       0.53      1.00      0.70         8\n           5       0.25      1.00      0.40         5\n           6       1.00      1.00      1.00         1\n           7       0.45      1.00      0.62        10\n           8       0.60      1.00      0.75         3\n           9       0.43      1.00      0.60         3\n          11       0.50      1.00      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.54      1.00      0.70         7\n          14       0.44      0.80      0.57         5\n          15       0.83      0.91      0.87        11\n          16       0.41      1.00      0.58        11\n          17       0.36      0.87      0.51        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.92      1.00      0.96        61\n          21       0.00      1.00      0.00         0\n          24       0.92      1.00      0.96        12\n          25       0.98      0.97      0.98       102\n          26       0.80      0.90      0.84        39\n          27       0.84      0.97      0.90        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.54      0.78      0.64         9\n          32       0.71      0.88      0.79        68\n          33       0.14      1.00      0.25         1\n          34       0.96      0.76      0.85        34\n          35       0.97      0.90      0.93        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.31      1.00      0.47        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.96      0.94        49\n          41       1.00      0.42      0.59        12\n          42       0.69      0.89      0.77        27\n          43       0.43      1.00      0.60         9\n          44       1.00      1.00      1.00         3\n          45       0.38      0.79      0.52        19\n          46       0.92      0.95      0.94       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.00      0.00      0.00         1\n          51       0.54      1.00      0.70         7\n          52       0.21      0.60      0.32         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.40      1.00      0.57        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.31      1.00      0.47         4\n          73       0.86      0.95      0.91       567\n          75       0.87      1.00      0.93        13\n          76       0.60      1.00      0.75         6\n          77       0.50      0.91      0.65        11\n          78       0.60      0.98      0.74        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.67      1.00      0.80         6\n          85       0.00      1.00      0.00         0\n          86       0.35      1.00      0.52         7\n          87       0.96      1.00      0.98        27\n          88       0.54      1.00      0.70         7\n          89       0.40      1.00      0.57         2\n          90       0.55      0.92      0.69        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.78      0.88      0.82        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.31      0.83      0.45         6\n         103       0.64      0.70      0.67        20\n         104       0.95      0.98      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.98      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.89      0.94      5640\n\n    accuracy                           0.92     10056\n   macro avg       0.57      0.90      0.62     10056\nweighted avg       0.95      0.92      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

StackedLSTM_32,"{'name': 'sequential_91', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_60'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_60', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_27', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_28', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_60', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_60', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 32)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 32)}}], 'build_input_shape': (None, 46)}",0.8249035179615021,0.16018442809581757,1.3031952381134033,0.8279773592948914,0.1437813565135002,0.8927575647830963,1839.0,5203.0,1109.0,5477.5,10098.5,0.31930003183149547,"['              precision    recall  f1-score   support\n\n           0       1.00      0.81      0.89      1702\n           1       0.04      1.00      0.09         2\n           2       0.16      0.80      0.26        20\n           3       0.47      0.90      0.62       101\n           4       0.13      1.00      0.23         8\n           5       0.14      1.00      0.24         5\n           6       0.00      0.00      0.00         1\n           7       0.39      0.70      0.50        10\n           8       0.40      0.67      0.50         3\n           9       0.10      0.67      0.17         3\n          11       0.33      0.67      0.44         3\n          13       0.29      0.57      0.38         7\n          14       0.33      0.80      0.47         5\n          15       0.44      0.64      0.52        11\n          16       0.14      0.82      0.23        11\n          17       0.19      0.54      0.28        46\n          18       0.18      0.94      0.31        33\n          19       0.10      1.00      0.18         6\n          20       0.67      0.92      0.77        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.28      0.83      0.42        12\n          25       0.64      0.85      0.73       102\n          26       0.74      0.67      0.70        39\n          27       0.22      0.87      0.35        39\n          28       0.25      0.75      0.38        12\n          29       0.03      1.00      0.06         1\n          30       1.00      0.00      0.00         1\n          31       0.18      0.44      0.26         9\n          32       0.52      0.51      0.52        68\n          33       0.04      1.00      0.08         1\n          34       0.82      0.53      0.64        34\n          35       0.64      0.74      0.69        31\n          36       0.02      1.00      0.05         5\n          37       0.50      0.50      0.50         2\n          38       0.22      1.00      0.36        12\n          39       0.19      0.86      0.32         7\n          40       0.04      0.84      0.08        49\n          41       0.14      0.42      0.21        12\n          42       0.35      0.85      0.49        27\n          43       0.03      0.56      0.05         9\n          44       0.23      1.00      0.38         3\n          45       0.09      0.53      0.15        19\n          46       0.39      0.74      0.51       656\n          47       0.10      1.00      0.18         5\n          48       0.33      0.71      0.45         7\n          49       0.11      0.40      0.17         5\n          50       0.00      0.00      0.00         1\n          51       0.58      1.00      0.74         7\n          52       0.04      0.20      0.07         5\n          54       0.29      0.67      0.40         6\n          55       0.00      1.00      0.00         0\n          56       0.26      1.00      0.41         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.55      0.84      0.66        64\n          63       0.12      0.33      0.18         3\n          64       1.00      0.00      0.00         1\n          66       0.34      0.79      0.48        14\n          70       0.00      1.00      0.00         0\n          71       0.10      1.00      0.17         2\n          72       0.09      0.75      0.17         4\n          73       0.44      0.75      0.55       567\n          74       0.00      1.00      0.00         0\n          75       0.29      0.92      0.44        13\n          76       0.20      0.83      0.32         6\n          77       0.03      0.27      0.06        11\n          78       0.49      0.92      0.64        53\n          79       0.16      0.83      0.26         6\n          81       0.40      1.00      0.57         2\n          83       1.00      0.50      0.67         2\n          84       0.23      0.50      0.32         6\n          86       0.27      1.00      0.42         7\n          87       0.41      1.00      0.58        27\n          88       0.15      1.00      0.26         7\n          89       0.50      0.50      0.50         2\n          90       0.50      0.75      0.60        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.33      0.50      0.40         2\n          94       0.00      1.00      0.00         0\n          95       0.42      0.83      0.56        24\n          97       0.14      1.00      0.25         1\n          98       0.50      0.50      0.50         2\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.07      0.83      0.12         6\n         103       0.25      0.70      0.37        20\n         104       0.52      0.86      0.65       110\n         105       0.14      0.50      0.22         2\n         106       0.00      0.00      0.00         1\n         108       0.20      1.00      0.33         2\n         111       0.50      1.00      0.67        58\n         112       0.00      1.00      0.00         0\n         115       0.16      1.00      0.28         5\n         116       0.54      0.91      0.68       132\n         117       0.17      1.00      0.29         2\n         118       0.12      1.00      0.21         8\n         119       0.98      0.22      0.36      5640\n\n    accuracy                           0.47     10056\n   macro avg       0.27      0.75      0.31     10056\nweighted avg       0.83      0.47      0.49     10056\n', '              precision    recall  f1-score   support\n\n           0       1.00      0.77      0.87      1702\n           1       0.07      1.00      0.12         2\n           2       0.21      0.80      0.34        20\n           3       0.45      0.78      0.57       101\n           4       0.09      1.00      0.16         8\n           5       0.06      0.60      0.10         5\n           6       0.00      0.00      0.00         1\n           7       0.14      0.60      0.23        10\n           8       0.25      0.33      0.29         3\n           9       0.25      0.67      0.36         3\n          11       0.00      0.00      0.00         3\n          12       0.00      1.00      0.00         0\n          13       0.26      0.86      0.40         7\n          14       0.27      0.60      0.38         5\n          15       0.24      0.36      0.29        11\n          16       0.18      0.45      0.26        11\n          17       0.17      0.54      0.26        46\n          18       0.30      0.94      0.45        33\n          19       0.06      1.00      0.12         6\n          20       0.33      0.95      0.49        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.21      0.75      0.33        12\n          25       0.32      0.87      0.47       102\n          26       0.54      0.54      0.54        39\n          27       0.64      0.82      0.72        39\n          28       0.08      0.75      0.14        12\n          29       0.00      0.00      0.00         1\n          30       0.00      0.00      0.00         1\n          31       0.12      0.44      0.19         9\n          32       0.39      0.75      0.51        68\n          33       0.00      0.00      0.00         1\n          34       0.57      0.59      0.58        34\n          35       0.61      0.71      0.66        31\n          36       0.09      1.00      0.16         5\n          37       0.33      0.50      0.40         2\n          38       0.18      1.00      0.31        12\n          39       0.07      1.00      0.13         7\n          40       0.17      0.84      0.28        49\n          41       0.12      0.42      0.19        12\n          42       0.26      0.78      0.39        27\n          43       0.01      0.78      0.03         9\n          44       0.33      1.00      0.50         3\n          45       0.15      0.37      0.21        19\n          46       0.37      0.78      0.50       656\n          47       0.14      0.80      0.24         5\n          48       0.08      0.71      0.15         7\n          49       0.17      0.60      0.26         5\n          50       0.00      0.00      0.00         1\n          51       0.26      0.86      0.40         7\n          52       0.10      0.40      0.15         5\n          54       0.14      0.50      0.21         6\n          55       0.00      1.00      0.00         0\n          56       0.26      1.00      0.41         6\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.74      0.83      0.78        64\n          63       0.25      0.67      0.36         3\n          64       1.00      0.00      0.00         1\n          66       0.29      0.86      0.43        14\n          67       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.04      1.00      0.08         2\n          72       0.09      0.75      0.16         4\n          73       0.52      0.57      0.54       567\n          74       0.00      1.00      0.00         0\n          75       0.30      1.00      0.46        13\n          76       0.03      0.33      0.06         6\n          77       0.04      0.73      0.08        11\n          78       0.23      0.83      0.36        53\n          79       0.08      0.33      0.12         6\n          80       0.00      1.00      0.00         0\n          81       0.40      1.00      0.57         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.25      0.50      0.33         6\n          86       0.26      0.86      0.40         7\n          87       0.42      1.00      0.59        27\n          88       0.27      1.00      0.42         7\n          89       1.00      0.00      0.00         2\n          90       0.34      0.92      0.50        12\n          91       0.00      1.00      0.00         0\n          93       0.29      1.00      0.44         2\n          94       0.00      1.00      0.00         0\n          95       0.49      0.79      0.60        24\n          97       0.25      1.00      0.40         1\n          98       0.11      0.50      0.18         2\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.08      0.67      0.15         6\n         103       0.30      0.50      0.38        20\n         104       0.34      0.94      0.49       110\n         105       0.25      0.50      0.33         2\n         106       0.00      0.00      0.00         1\n         108       0.29      1.00      0.44         2\n         111       0.50      1.00      0.67        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.20      1.00      0.33         5\n         116       0.37      0.97      0.54       132\n         117       0.08      1.00      0.15         2\n         118       0.21      0.88      0.34         8\n         119       1.00      0.26      0.41      5640\n\n    accuracy                           0.48     10056\n   macro avg       0.22      0.72      0.25     10056\nweighted avg       0.83      0.48      0.51     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

StackedLSTM_64,"{'name': 'sequential_94', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_62'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_62', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_29', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_30', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_62', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_62', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.8643517792224884,0.19549497216939926,0.513497918844223,0.868317037820816,0.17767693102359772,0.6191941201686859,2294.0,4260.5,654.0,4360.0,10090.0,0.4079635986004253,"['              precision    recall  f1-score   support\n\n           0       0.99      0.88      0.93      1702\n           1       0.08      1.00      0.15         2\n           2       0.45      0.85      0.59        20\n           3       0.82      0.96      0.88       101\n           4       0.12      1.00      0.22         8\n           5       0.06      0.80      0.11         5\n           6       1.00      1.00      1.00         1\n           7       0.24      0.90      0.38        10\n           8       0.60      1.00      0.75         3\n           9       0.07      0.67      0.13         3\n          11       0.33      0.33      0.33         3\n          12       0.00      1.00      0.00         0\n          13       0.30      0.86      0.44         7\n          14       0.26      1.00      0.42         5\n          15       0.44      0.73      0.55        11\n          16       0.36      0.91      0.51        11\n          17       0.19      0.59      0.29        46\n          18       0.41      1.00      0.58        33\n          19       0.29      1.00      0.44         6\n          20       0.39      0.98      0.56        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.21      0.83      0.34        12\n          25       0.37      0.97      0.54       102\n          26       0.74      0.87      0.80        39\n          27       0.70      0.95      0.80        39\n          28       0.17      0.75      0.27        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.16      0.67      0.26         9\n          32       0.47      0.76      0.58        68\n          33       0.08      1.00      0.14         1\n          34       0.81      0.76      0.79        34\n          35       0.66      0.87      0.75        31\n          36       0.21      1.00      0.34         5\n          37       0.25      0.50      0.33         2\n          38       0.16      1.00      0.27        12\n          39       0.11      0.86      0.19         7\n          40       0.23      0.88      0.36        49\n          41       0.26      0.50      0.34        12\n          42       0.41      0.85      0.55        27\n          43       0.01      0.78      0.02         9\n          44       0.33      1.00      0.50         3\n          45       0.25      0.63      0.36        19\n          46       0.50      0.88      0.64       656\n          47       0.13      1.00      0.23         5\n          48       0.36      0.71      0.48         7\n          49       0.14      0.60      0.23         5\n          50       0.00      0.00      0.00         1\n          51       0.33      1.00      0.50         7\n          52       0.03      0.20      0.06         5\n          53       0.00      1.00      0.00         0\n          54       0.38      0.83      0.53         6\n          55       0.00      1.00      0.00         0\n          56       0.28      0.83      0.42         6\n          57       0.00      1.00      0.00         0\n          62       0.61      0.97      0.75        64\n          63       0.50      0.67      0.57         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.29      0.86      0.43        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.13      1.00      0.24         2\n          72       0.14      0.75      0.24         4\n          73       0.49      0.83      0.62       567\n          74       0.00      1.00      0.00         0\n          75       0.33      0.92      0.49        13\n          76       0.31      0.83      0.45         6\n          77       0.07      0.55      0.12        11\n          78       0.50      0.98      0.67        53\n          79       0.15      1.00      0.27         6\n          80       0.00      1.00      0.00         0\n          81       0.20      0.50      0.29         2\n          83       1.00      0.50      0.67         2\n          84       0.42      0.83      0.56         6\n          85       0.00      1.00      0.00         0\n          86       0.15      0.86      0.25         7\n          87       0.66      1.00      0.79        27\n          88       0.29      1.00      0.45         7\n          89       0.40      1.00      0.57         2\n          90       0.38      0.92      0.54        12\n          92       0.00      1.00      0.00         0\n          93       0.25      0.50      0.33         2\n          95       0.76      0.79      0.78        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.14      1.00      0.24         6\n         103       0.26      0.55      0.35        20\n         104       0.76      0.97      0.86       110\n         105       0.50      1.00      0.67         2\n         106       0.20      1.00      0.33         1\n         108       0.50      1.00      0.67         2\n         111       0.55      0.98      0.70        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.25      1.00      0.40         5\n         116       0.73      0.97      0.83       132\n         117       0.18      1.00      0.31         2\n         118       0.17      1.00      0.29         8\n         119       0.99      0.39      0.56      5640\n\n    accuracy                           0.61     10056\n   macro avg       0.30      0.84      0.38     10056\nweighted avg       0.86      0.61      0.64     10056\n', '              precision    recall  f1-score   support\n\n           0       1.00      0.81      0.89      1702\n           1       0.08      1.00      0.14         2\n           2       0.36      0.90      0.51        20\n           3       0.47      0.98      0.64       101\n           4       0.06      1.00      0.12         8\n           5       0.07      0.80      0.13         5\n           6       0.00      0.00      0.00         1\n           7       0.36      0.80      0.50        10\n           8       0.40      0.67      0.50         3\n           9       0.15      0.67      0.25         3\n          11       0.40      0.67      0.50         3\n          12       0.00      1.00      0.00         0\n          13       0.32      0.86      0.46         7\n          14       0.56      1.00      0.71         5\n          15       0.56      0.91      0.69        11\n          16       0.16      0.91      0.27        11\n          17       0.13      0.46      0.20        46\n          18       0.23      0.97      0.38        33\n          19       0.40      1.00      0.57         6\n          20       0.73      1.00      0.85        61\n          23       0.00      1.00      0.00         0\n          24       0.09      0.83      0.16        12\n          25       0.56      0.90      0.69       102\n          26       0.89      0.85      0.87        39\n          27       0.47      0.95      0.63        39\n          28       0.24      0.75      0.36        12\n          29       0.14      1.00      0.25         1\n          30       0.50      1.00      0.67         1\n          31       0.11      0.67      0.18         9\n          32       0.47      0.79      0.59        68\n          33       0.11      1.00      0.20         1\n          34       0.92      0.71      0.80        34\n          35       0.53      0.81      0.64        31\n          36       0.62      1.00      0.77         5\n          37       0.08      0.50      0.13         2\n          38       0.23      1.00      0.38        12\n          39       0.09      0.86      0.16         7\n          40       0.26      0.86      0.39        49\n          41       0.29      0.42      0.34        12\n          42       0.25      0.85      0.38        27\n          43       0.01      0.56      0.02         9\n          44       0.12      1.00      0.22         3\n          45       0.20      0.68      0.31        19\n          46       0.49      0.87      0.63       656\n          47       0.29      0.80      0.42         5\n          48       0.22      0.71      0.33         7\n          49       0.23      0.60      0.33         5\n          50       0.00      0.00      0.00         1\n          51       0.44      1.00      0.61         7\n          52       0.06      0.40      0.10         5\n          54       0.62      0.83      0.71         6\n          55       0.00      1.00      0.00         0\n          56       0.27      1.00      0.43         6\n          62       0.73      0.95      0.83        64\n          63       0.14      0.33      0.20         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.30      0.93      0.46        14\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.29      1.00      0.44         2\n          72       0.09      0.75      0.17         4\n          73       0.48      0.87      0.62       567\n          74       0.00      1.00      0.00         0\n          75       0.71      0.92      0.80        13\n          76       0.05      0.83      0.10         6\n          77       0.14      0.73      0.23        11\n          78       0.47      1.00      0.64        53\n          79       0.14      1.00      0.24         6\n          81       0.40      1.00      0.57         2\n          83       1.00      0.00      0.00         2\n          84       0.71      0.83      0.77         6\n          85       0.00      1.00      0.00         0\n          86       0.16      0.43      0.23         7\n          87       0.60      1.00      0.75        27\n          88       0.33      1.00      0.50         7\n          89       0.33      0.50      0.40         2\n          90       0.39      0.92      0.55        12\n          91       0.00      1.00      0.00         0\n          93       0.33      0.50      0.40         2\n          94       0.00      1.00      0.00         0\n          95       0.72      0.88      0.79        24\n          97       0.33      1.00      0.50         1\n          98       0.33      0.50      0.40         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.18      1.00      0.31         6\n         103       0.37      0.70      0.48        20\n         104       0.62      0.95      0.75       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         107       0.00      1.00      0.00         0\n         108       0.25      1.00      0.40         2\n         111       0.50      1.00      0.67        58\n         112       0.00      1.00      0.00         0\n         114       0.00      1.00      0.00         0\n         115       0.36      1.00      0.53         5\n         116       0.68      0.98      0.81       132\n         117       0.33      1.00      0.50         2\n         118       0.27      1.00      0.42         8\n         119       0.99      0.37      0.54      5640\n\n    accuracy                           0.58     10056\n   macro avg       0.31      0.81      0.37     10056\nweighted avg       0.86      0.58      0.62     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalStackedLSTM_32,"{'name': 'sequential_98', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_65'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_65', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_20', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_31', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_31', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_21', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_32', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_32', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 32, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_65', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_65', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.8275394439697266,0.15815985947847366,0.4525270462036133,0.8328122198581696,0.14525459706783295,0.7406896650791168,2130.5,4208.5,817.5,4398.5,10075.0,0.40000308207412183,"['              precision    recall  f1-score   support\n\n           0       1.00      0.65      0.79      1702\n           1       0.09      1.00      0.16         2\n           2       0.21      0.85      0.33        20\n           3       0.44      0.99      0.61       101\n           4       0.17      1.00      0.30         8\n           5       0.16      0.80      0.27         5\n           6       0.50      1.00      0.67         1\n           7       0.32      1.00      0.49        10\n           8       0.38      1.00      0.55         3\n           9       0.50      1.00      0.67         3\n          11       0.33      0.33      0.33         3\n          13       0.16      0.71      0.26         7\n          14       0.17      0.80      0.29         5\n          15       0.62      0.91      0.74        11\n          16       0.23      0.91      0.36        11\n          17       0.15      0.70      0.25        46\n          18       0.32      1.00      0.49        33\n          19       0.21      0.83      0.33         6\n          20       0.23      1.00      0.38        61\n          23       0.00      1.00      0.00         0\n          24       0.28      0.92      0.43        12\n          25       0.48      0.85      0.61       102\n          26       0.56      0.69      0.62        39\n          27       0.51      0.90      0.65        39\n          28       0.25      0.92      0.39        12\n          29       1.00      1.00      1.00         1\n          30       1.00      0.00      0.00         1\n          31       0.24      0.56      0.33         9\n          32       0.35      0.76      0.48        68\n          33       0.00      0.00      0.00         1\n          34       0.89      0.74      0.81        34\n          35       0.67      0.90      0.77        31\n          36       0.28      1.00      0.43         5\n          37       0.50      0.50      0.50         2\n          38       0.13      1.00      0.23        12\n          39       0.28      1.00      0.44         7\n          40       0.20      0.98      0.33        49\n          41       0.36      0.67      0.47        12\n          42       0.27      0.89      0.42        27\n          43       0.05      0.67      0.10         9\n          44       0.15      0.67      0.25         3\n          45       0.15      0.74      0.25        19\n          46       0.42      0.83      0.56       656\n          47       0.25      1.00      0.40         5\n          48       0.15      0.57      0.24         7\n          49       0.18      0.80      0.30         5\n          50       0.14      1.00      0.25         1\n          51       0.29      1.00      0.45         7\n          52       0.09      0.60      0.16         5\n          54       0.57      0.67      0.62         6\n          55       0.00      1.00      0.00         0\n          56       0.17      1.00      0.29         6\n          62       0.53      0.98      0.69        64\n          63       0.10      0.33      0.15         3\n          64       1.00      0.00      0.00         1\n          66       0.18      1.00      0.30        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          71       0.15      1.00      0.27         2\n          72       0.22      1.00      0.36         4\n          73       0.46      0.80      0.58       567\n          75       0.41      1.00      0.58        13\n          76       0.03      0.67      0.06         6\n          77       0.16      1.00      0.28        11\n          78       0.42      1.00      0.59        53\n          79       0.22      0.83      0.34         6\n          81       0.33      1.00      0.50         2\n          83       0.00      0.00      0.00         2\n          84       0.50      0.67      0.57         6\n          85       0.00      1.00      0.00         0\n          86       0.09      0.86      0.16         7\n          87       0.55      1.00      0.71        27\n          88       0.47      1.00      0.64         7\n          89       0.29      1.00      0.44         2\n          90       0.28      0.92      0.42        12\n          93       0.20      1.00      0.33         2\n          94       0.00      1.00      0.00         0\n          95       0.80      0.83      0.82        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.26      1.00      0.41         6\n         103       0.25      0.65      0.36        20\n         104       0.47      0.94      0.63       110\n         105       0.40      1.00      0.57         2\n         106       0.50      1.00      0.67         1\n         108       0.25      1.00      0.40         2\n         109       0.00      1.00      0.00         0\n         111       0.51      0.98      0.67        58\n         112       0.00      1.00      0.00         0\n         115       0.25      1.00      0.40         5\n         116       0.53      0.97      0.69       132\n         117       0.50      1.00      0.67         2\n         118       0.44      0.88      0.58         8\n         119       0.99      0.42      0.59      5640\n\n    accuracy                           0.58     10056\n   macro avg       0.31      0.83      0.38     10056\nweighted avg       0.84      0.58      0.61     10056\n', '              precision    recall  f1-score   support\n\n           0       1.00      0.62      0.77      1702\n           1       0.07      1.00      0.13         2\n           2       0.26      0.90      0.40        20\n           3       0.41      0.95      0.57       101\n           4       0.15      1.00      0.25         8\n           5       0.27      0.80      0.40         5\n           6       0.00      0.00      0.00         1\n           7       0.15      0.80      0.25        10\n           8       0.29      0.67      0.40         3\n           9       0.18      0.67      0.29         3\n          11       0.29      0.67      0.40         3\n          12       0.00      1.00      0.00         0\n          13       0.24      0.57      0.33         7\n          14       0.31      1.00      0.48         5\n          15       0.62      0.73      0.67        11\n          16       0.36      0.91      0.51        11\n          17       0.11      0.61      0.19        46\n          18       0.43      1.00      0.61        33\n          19       0.31      0.83      0.45         6\n          20       0.26      0.98      0.42        61\n          21       0.00      1.00      0.00         0\n          23       0.00      1.00      0.00         0\n          24       0.27      1.00      0.43        12\n          25       0.40      0.78      0.53       102\n          26       0.53      0.67      0.59        39\n          27       0.46      0.92      0.61        39\n          28       0.29      0.92      0.44        12\n          29       1.00      1.00      1.00         1\n          30       1.00      1.00      1.00         1\n          31       0.23      0.78      0.36         9\n          32       0.30      0.74      0.42        68\n          33       0.11      1.00      0.20         1\n          34       0.83      0.74      0.78        34\n          35       0.62      0.81      0.70        31\n          36       0.20      1.00      0.33         5\n          37       0.50      0.50      0.50         2\n          38       0.13      1.00      0.24        12\n          39       0.29      0.86      0.43         7\n          40       0.22      0.98      0.36        49\n          41       0.19      0.58      0.29        12\n          42       0.23      0.85      0.36        27\n          43       0.03      0.67      0.06         9\n          44       0.09      0.67      0.16         3\n          45       0.16      0.63      0.25        19\n          46       0.39      0.77      0.51       656\n          47       0.16      1.00      0.28         5\n          48       0.28      0.71      0.40         7\n          49       0.36      1.00      0.53         5\n          50       1.00      0.00      0.00         1\n          51       0.27      1.00      0.42         7\n          52       0.03      0.60      0.06         5\n          54       0.26      0.83      0.40         6\n          55       0.00      1.00      0.00         0\n          56       0.16      0.83      0.27         6\n          59       0.00      1.00      0.00         0\n          62       0.42      0.97      0.59        64\n          63       0.22      0.67      0.33         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.18      1.00      0.30        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.14      1.00      0.25         2\n          72       0.12      0.75      0.21         4\n          73       0.42      0.75      0.54       567\n          74       0.00      1.00      0.00         0\n          75       0.38      0.85      0.52        13\n          76       0.23      0.50      0.32         6\n          77       0.13      0.64      0.21        11\n          78       0.36      1.00      0.53        53\n          79       0.29      1.00      0.44         6\n          80       0.00      1.00      0.00         0\n          81       0.00      0.00      0.00         2\n          82       0.00      1.00      0.00         0\n          83       1.00      0.00      0.00         2\n          84       0.20      0.67      0.31         6\n          86       0.06      0.71      0.12         7\n          87       0.57      1.00      0.73        27\n          88       0.37      1.00      0.54         7\n          89       0.09      0.50      0.15         2\n          90       0.25      0.67      0.36        12\n          92       0.00      1.00      0.00         0\n          93       0.06      0.50      0.11         2\n          95       0.55      0.67      0.60        24\n          97       0.50      1.00      0.67         1\n          98       0.10      0.50      0.17         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.23      1.00      0.38         6\n         103       0.23      0.40      0.29        20\n         104       0.38      0.93      0.54       110\n         105       0.33      1.00      0.50         2\n         106       0.00      0.00      0.00         1\n         108       0.25      1.00      0.40         2\n         111       0.55      1.00      0.71        58\n         115       0.09      0.80      0.16         5\n         116       0.46      0.95      0.62       132\n         117       0.40      1.00      0.57         2\n         118       0.28      1.00      0.43         8\n         119       0.99      0.38      0.55      5640\n\n    accuracy                           0.54     10056\n   macro avg       0.27      0.79      0.33     10056\nweighted avg       0.83      0.54      0.57     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalStackedLSTM_16,"{'name': 'sequential_101', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_67'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_67', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_67', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_67', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}], 'build_input_shape': (None, 46)}",0.912449061870575,0.16833605617284775,0.899000883102417,0.9148853421211243,0.15837841480970383,0.30164994299411774,1042.0,2471.5,1906.0,1602.0,9840.5,0.33878063350018245,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      0.90      0.92        20\n           3       0.97      0.99      0.98       101\n           4       1.00      1.00      1.00         8\n           5       0.01      0.20      0.01         5\n           6       1.00      0.00      0.00         1\n           7       0.10      0.60      0.17        10\n           8       0.20      0.33      0.25         3\n           9       0.11      0.33      0.17         3\n          11       0.12      0.33      0.18         3\n          13       1.00      0.00      0.00         7\n          14       0.17      0.20      0.18         5\n          15       0.43      0.27      0.33        11\n          16       0.08      0.09      0.09        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.90      0.46      0.61       102\n          26       1.00      0.31      0.47        39\n          27       0.83      0.49      0.61        39\n          28       0.91      0.83      0.87        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       0.84      0.40      0.54        68\n          33       0.00      0.00      0.00         1\n          34       0.67      0.18      0.28        34\n          35       0.79      0.84      0.81        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.95      0.86      0.90        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       1.00      0.00      0.00         3\n          45       0.02      0.05      0.03        19\n          46       0.57      0.35      0.43       656\n          47       1.00      0.00      0.00         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.43      0.05        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.04      0.50      0.07         4\n          73       0.61      0.32      0.42       567\n          75       1.00      0.00      0.00        13\n          76       0.09      0.83      0.16         6\n          77       0.33      0.27      0.30        11\n          78       0.59      0.98      0.74        53\n          79       0.16      0.83      0.26         6\n          80       0.00      1.00      0.00         0\n          81       1.00      0.00      0.00         2\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          85       0.00      1.00      0.00         0\n          86       0.00      0.00      0.00         7\n          87       1.00      0.00      0.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.60      0.25      0.35        12\n          92       0.00      1.00      0.00         0\n          93       0.07      1.00      0.13         2\n          95       0.00      0.00      0.00        24\n          97       0.04      1.00      0.07         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.01      0.50      0.03         2\n         102       0.08      0.67      0.14         6\n         103       0.60      0.15      0.24        20\n         104       0.62      0.42      0.50       110\n         105       1.00      0.00      0.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.33      0.40      0.36         5\n         116       0.65      0.54      0.59       132\n         117       0.50      1.00      0.67         2\n         118       0.20      0.25      0.22         8\n         119       0.99      0.84      0.91      5640\n\n    accuracy                           0.77     10056\n   macro avg       0.48      0.61      0.36     10056\nweighted avg       0.90      0.77      0.82     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.65      1.00      0.78        20\n           3       0.99      0.89      0.94       101\n           4       1.00      1.00      1.00         8\n           5       0.00      0.00      0.00         5\n           6       1.00      0.00      0.00         1\n           7       0.03      0.10      0.04        10\n           8       1.00      0.00      0.00         3\n           9       0.25      0.33      0.29         3\n          10       0.00      1.00      0.00         0\n          11       0.00      0.00      0.00         3\n          13       1.00      0.00      0.00         7\n          14       0.00      0.00      0.00         5\n          15       0.43      0.27      0.33        11\n          16       0.31      0.73      0.43        11\n          17       0.00      0.00      0.00        46\n          18       0.72      1.00      0.84        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.73      0.92      0.81        12\n          25       0.67      0.02      0.04       102\n          26       1.00      0.33      0.50        39\n          27       0.86      0.62      0.72        39\n          28       1.00      0.17      0.29        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.45      0.56      0.50         9\n          32       1.00      0.01      0.03        68\n          33       0.00      0.00      0.00         1\n          34       0.40      0.06      0.10        34\n          35       1.00      0.61      0.76        31\n          36       1.00      1.00      1.00         5\n          37       1.00      0.50      0.67         2\n          38       1.00      1.00      1.00        12\n          39       0.86      0.86      0.86         7\n          40       0.95      0.86      0.90        49\n          41       0.83      0.42      0.56        12\n          42       1.00      0.85      0.92        27\n          43       0.02      0.44      0.04         9\n          44       1.00      0.00      0.00         3\n          45       0.01      0.21      0.03        19\n          46       0.67      0.19      0.29       656\n          47       1.00      0.00      0.00         5\n          48       0.13      0.71      0.22         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.20      0.18         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       1.00      1.00      1.00         6\n          57       0.00      1.00      0.00         0\n          58       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      0.00      0.00         3\n          64       1.00      0.00      0.00         1\n          66       0.02      0.57      0.04        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.40      1.00      0.57         2\n          72       0.05      0.75      0.10         4\n          73       0.54      0.24      0.33       567\n          75       0.62      1.00      0.76        13\n          76       0.09      0.83      0.16         6\n          77       1.00      0.18      0.31        11\n          78       0.59      0.98      0.74        53\n          79       1.00      0.00      0.00         6\n          80       0.00      1.00      0.00         0\n          81       0.05      0.50      0.09         2\n          83       1.00      0.00      0.00         2\n          84       0.20      0.33      0.25         6\n          85       0.00      1.00      0.00         0\n          86       0.00      0.00      0.00         7\n          87       0.96      1.00      0.98        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.56      0.42      0.48        12\n          93       0.07      1.00      0.13         2\n          95       0.00      0.00      0.00        24\n          97       1.00      0.00      0.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.01      0.50      0.03         2\n         102       0.01      0.67      0.02         6\n         103       0.52      0.55      0.54        20\n         104       0.55      0.15      0.24       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         110       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.27      0.80      0.40         5\n         116       0.66      0.58      0.61       132\n         117       0.67      1.00      0.80         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.75      0.85      5640\n\n    accuracy                           0.70     10056\n   macro avg       0.49      0.62      0.36     10056\nweighted avg       0.90      0.70      0.76     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalDropoutLSTM,"{'name': 'sequential_104', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_69'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_69', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_22', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'forward_lstm_33', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'backward_lstm_33', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_6', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_69', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_69', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.935190349817276,0.24269160628318787,0.18222732841968536,0.9486088752746582,0.2292894646525383,0.2411324307322502,2601.0,1262.5,347.0,1379.5,6831.0,0.5804600194645255,"['              precision    recall  f1-score   support\n\n           0       0.99      0.86      0.92      1702\n           1       0.17      1.00      0.29         2\n           2       0.73      0.95      0.83        20\n           3       0.92      0.99      0.95       101\n           4       0.33      1.00      0.50         8\n           5       0.36      1.00      0.53         5\n           6       1.00      1.00      1.00         1\n           7       0.50      1.00      0.67        10\n           8       0.38      1.00      0.55         3\n           9       0.30      1.00      0.46         3\n          11       0.75      1.00      0.86         3\n          12       0.00      1.00      0.00         0\n          13       0.32      1.00      0.48         7\n          14       0.33      0.60      0.43         5\n          15       0.92      1.00      0.96        11\n          16       0.50      1.00      0.67        11\n          17       0.20      0.85      0.33        46\n          18       0.68      0.97      0.80        33\n          19       0.86      1.00      0.92         6\n          20       0.87      1.00      0.93        61\n          24       0.71      1.00      0.83        12\n          25       0.94      0.97      0.96       102\n          26       0.57      0.85      0.68        39\n          27       0.88      0.95      0.91        39\n          28       0.58      0.92      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.20      1.00      0.33         1\n          31       0.57      0.89      0.70         9\n          32       0.67      0.82      0.74        68\n          33       0.12      1.00      0.22         1\n          34       0.88      0.82      0.85        34\n          35       0.93      0.90      0.92        31\n          36       0.83      1.00      0.91         5\n          37       0.50      0.50      0.50         2\n          38       0.26      1.00      0.41        12\n          39       0.86      0.86      0.86         7\n          40       0.71      0.98      0.82        49\n          41       0.83      0.42      0.56        12\n          42       0.68      0.85      0.75        27\n          43       0.24      1.00      0.38         9\n          44       0.60      1.00      0.75         3\n          45       0.26      0.79      0.39        19\n          46       0.95      0.94      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.62      1.00      0.77         5\n          50       0.17      1.00      0.29         1\n          51       0.55      0.86      0.67         7\n          52       0.19      0.80      0.31         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.73      0.98      0.84        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.30      1.00      0.47        14\n          68       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.18      1.00      0.31         4\n          73       0.86      0.95      0.90       567\n          74       0.00      1.00      0.00         0\n          75       0.93      1.00      0.96        13\n          76       0.67      1.00      0.80         6\n          77       0.29      0.91      0.44        11\n          78       0.57      0.98      0.72        53\n          79       0.55      1.00      0.71         6\n          81       0.50      0.50      0.50         2\n          83       0.50      1.00      0.67         2\n          84       1.00      1.00      1.00         6\n          86       0.27      0.86      0.41         7\n          87       1.00      1.00      1.00        27\n          88       0.41      1.00      0.58         7\n          89       0.25      1.00      0.40         2\n          90       0.61      0.92      0.73        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.76      0.92      0.83        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.28      0.83      0.42         6\n         103       0.55      0.60      0.57        20\n         104       0.95      0.95      0.95       110\n         105       0.67      1.00      0.80         2\n         106       1.00      1.00      1.00         1\n         108       0.33      1.00      0.50         2\n         111       0.88      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.42      1.00      0.59         5\n         116       0.94      0.98      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.36      1.00      0.53         8\n         119       0.97      0.85      0.91      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.54      0.92      0.60     10056\nweighted avg       0.93      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.90      0.94      1702\n           1       0.33      1.00      0.50         2\n           2       0.69      1.00      0.82        20\n           3       0.84      0.99      0.91       101\n           4       0.29      1.00      0.44         8\n           5       0.33      1.00      0.50         5\n           6       0.50      1.00      0.67         1\n           7       0.43      0.90      0.58        10\n           8       0.50      1.00      0.67         3\n           9       0.30      1.00      0.46         3\n          11       0.50      0.67      0.57         3\n          12       0.00      1.00      0.00         0\n          13       0.26      0.86      0.40         7\n          14       0.40      0.80      0.53         5\n          15       0.83      0.91      0.87        11\n          16       0.33      1.00      0.50        11\n          17       0.17      0.83      0.28        46\n          18       0.79      1.00      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.74      1.00      0.85        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.91      0.83      0.87        12\n          25       0.67      0.96      0.79       102\n          26       0.73      0.90      0.80        39\n          27       0.50      0.92      0.65        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.40      0.89      0.55         9\n          32       0.69      0.84      0.75        68\n          33       0.00      0.00      0.00         1\n          34       0.85      0.82      0.84        34\n          35       0.88      0.90      0.89        31\n          36       1.00      1.00      1.00         5\n          37       0.33      0.50      0.40         2\n          38       0.24      1.00      0.39        12\n          39       0.86      0.86      0.86         7\n          40       0.53      0.98      0.69        49\n          41       0.28      0.42      0.33        12\n          42       0.62      0.89      0.73        27\n          43       0.18      1.00      0.31         9\n          44       0.75      1.00      0.86         3\n          45       0.23      0.89      0.37        19\n          46       0.94      0.93      0.93       656\n          47       1.00      0.80      0.89         5\n          48       0.70      1.00      0.82         7\n          49       0.62      1.00      0.77         5\n          50       0.12      1.00      0.22         1\n          51       0.55      0.86      0.67         7\n          52       0.12      0.60      0.20         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.27      1.00      0.42        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.20      1.00      0.33         4\n          73       0.76      0.94      0.84       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.38      1.00      0.55         6\n          77       0.43      0.91      0.59        11\n          78       0.54      0.98      0.69        53\n          79       0.60      1.00      0.75         6\n          80       0.00      1.00      0.00         0\n          81       0.50      0.50      0.50         2\n          83       0.50      1.00      0.67         2\n          84       0.75      1.00      0.86         6\n          86       0.50      0.86      0.63         7\n          87       0.73      1.00      0.84        27\n          88       0.28      1.00      0.44         7\n          89       0.25      1.00      0.40         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.40      1.00      0.57         2\n          94       0.00      1.00      0.00         0\n          95       0.77      0.96      0.85        24\n          96       0.00      1.00      0.00         0\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.33      1.00      0.50         6\n         103       0.67      0.70      0.68        20\n         104       0.95      0.95      0.95       110\n         105       0.33      0.50      0.40         2\n         106       0.00      0.00      0.00         1\n         108       0.40      1.00      0.57         2\n         111       0.88      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.92      0.99      0.96       132\n         117       0.50      1.00      0.67         2\n         118       0.38      1.00      0.55         8\n         119       0.98      0.82      0.89      5640\n\n    accuracy                           0.86     10056\n   macro avg       0.47      0.89      0.53     10056\nweighted avg       0.92      0.86      0.88     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

BidirectionalDropoutGRU,"{'name': 'sequential_107', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_71'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_71', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_23', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_71', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_71', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9685562252998352,0.26966992020606995,0.1479612961411476,0.9711720049381256,0.2523322254419327,0.1497737616300583,2596.0,771.5,352.0,935.5,5139.0,0.5924828492279252,"['              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.35      1.00      0.52         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.60      0.90      0.72        10\n           8       0.27      1.00      0.43         3\n           9       0.67      0.67      0.67         3\n          11       0.38      1.00      0.55         3\n          13       0.88      1.00      0.93         7\n          14       0.50      0.80      0.62         5\n          15       1.00      0.91      0.95        11\n          16       0.65      1.00      0.79        11\n          17       0.38      0.80      0.52        46\n          18       0.97      1.00      0.99        33\n          19       0.86      1.00      0.92         6\n          20       0.95      1.00      0.98        61\n          21       0.00      1.00      0.00         0\n          24       0.71      1.00      0.83        12\n          25       0.97      0.98      0.98       102\n          26       0.90      0.90      0.90        39\n          27       0.80      0.95      0.87        39\n          28       0.83      0.83      0.83        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.75      1.00      0.86         9\n          32       0.81      0.84      0.83        68\n          33       0.50      1.00      0.67         1\n          34       0.97      0.82      0.89        34\n          35       0.83      0.94      0.88        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.67      1.00      0.80        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.98      0.97        49\n          41       0.56      0.42      0.48        12\n          42       0.92      0.85      0.88        27\n          43       0.09      0.89      0.16         9\n          44       0.75      1.00      0.86         3\n          45       0.43      0.95      0.59        19\n          46       0.95      0.95      0.95       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.17      0.60      0.26         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.46      1.00      0.63         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.24      0.93      0.38        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      1.00      0.42         4\n          73       0.93      0.93      0.93       567\n          75       1.00      1.00      1.00        13\n          76       0.38      1.00      0.55         6\n          77       0.55      1.00      0.71        11\n          78       0.59      0.98      0.74        53\n          79       0.71      0.83      0.77         6\n          81       0.67      1.00      0.80         2\n          82       0.00      1.00      0.00         0\n          83       0.50      0.50      0.50         2\n          84       0.75      1.00      0.86         6\n          85       0.00      1.00      0.00         0\n          86       0.46      0.86      0.60         7\n          87       1.00      1.00      1.00        27\n          88       0.32      1.00      0.48         7\n          89       0.09      1.00      0.17         2\n          90       0.60      1.00      0.75        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.84      0.88      0.86        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.11      0.83      0.20         6\n         103       0.71      0.75      0.73        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.35      0.88      0.50         8\n         119       0.99      0.88      0.93      5640\n\n    accuracy                           0.91     10056\n   macro avg       0.59      0.90      0.62     10056\nweighted avg       0.96      0.91      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.57      1.00      0.73         8\n           5       0.38      1.00      0.56         5\n           6       1.00      1.00      1.00         1\n           7       0.59      1.00      0.74        10\n           8       0.67      0.67      0.67         3\n           9       0.67      0.67      0.67         3\n          11       0.67      0.67      0.67         3\n          12       0.00      1.00      0.00         0\n          13       0.18      1.00      0.30         7\n          14       0.29      0.80      0.42         5\n          15       0.83      0.91      0.87        11\n          16       0.58      1.00      0.73        11\n          17       0.43      0.87      0.57        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          21       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.99      0.96      0.98       102\n          26       0.90      0.92      0.91        39\n          27       0.90      0.92      0.91        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.73      0.89      0.80         9\n          32       0.89      0.84      0.86        68\n          33       0.25      1.00      0.40         1\n          34       1.00      0.79      0.89        34\n          35       0.76      0.90      0.82        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.32      1.00      0.49        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.98      0.97        49\n          41       0.83      0.42      0.56        12\n          42       0.77      0.85      0.81        27\n          43       0.35      1.00      0.51         9\n          44       1.00      1.00      1.00         3\n          45       0.50      0.89      0.64        19\n          46       0.96      0.95      0.96       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.18      0.40      0.25         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       0.67      0.67      0.67         3\n          64       1.00      0.00      0.00         1\n          66       0.29      0.93      0.44        14\n          67       0.00      1.00      0.00         0\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.27      1.00      0.42         4\n          73       0.96      0.94      0.95       567\n          75       0.81      1.00      0.90        13\n          76       0.40      1.00      0.57         6\n          77       0.92      1.00      0.96        11\n          78       0.59      0.98      0.74        53\n          79       1.00      1.00      1.00         6\n          80       0.00      1.00      0.00         0\n          81       0.33      1.00      0.50         2\n          82       0.00      1.00      0.00         0\n          83       0.12      1.00      0.22         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.32      0.86      0.46         7\n          87       1.00      1.00      1.00        27\n          88       0.30      1.00      0.47         7\n          89       0.09      1.00      0.17         2\n          90       0.73      0.92      0.81        12\n          92       0.00      1.00      0.00         0\n          93       0.50      0.50      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.91      0.88      0.89        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.06      0.50      0.11         2\n         102       0.23      0.83      0.36         6\n         103       0.65      0.75      0.70        20\n         104       0.96      0.97      0.96       110\n         105       0.50      1.00      0.67         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.97      0.98      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.90      0.94      5640\n\n    accuracy                           0.93     10056\n   macro avg       0.57      0.90      0.59     10056\nweighted avg       0.96      0.93      0.94     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutGRU,"{'name': 'sequential_109', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_72'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_72', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_72', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_72', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9518997669219971,0.2530708461999893,0.397516131401062,0.9598298966884613,0.23941169679164886,0.22413122653961182,2484.5,1279.0,463.5,1361.5,7249.0,0.5501211377382509,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.77      1.00      0.87        20\n           3       0.98      0.96      0.97       101\n           4       0.15      1.00      0.27         8\n           5       0.28      1.00      0.43         5\n           6       0.50      1.00      0.67         1\n           7       0.35      0.90      0.50        10\n           8       0.33      1.00      0.50         3\n           9       0.40      0.67      0.50         3\n          11       0.04      0.33      0.07         3\n          12       0.00      1.00      0.00         0\n          13       0.25      0.71      0.37         7\n          14       0.44      0.80      0.57         5\n          15       0.69      1.00      0.81        11\n          16       0.30      1.00      0.46        11\n          17       0.26      0.78      0.40        46\n          18       0.85      1.00      0.92        33\n          19       0.86      1.00      0.92         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.94      0.86      0.90       102\n          26       0.93      0.69      0.79        39\n          27       0.89      0.87      0.88        39\n          28       0.62      0.83      0.71        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.57      0.89      0.70         9\n          32       0.86      0.65      0.74        68\n          33       0.09      1.00      0.17         1\n          34       0.96      0.76      0.85        34\n          35       0.82      0.87      0.84        31\n          36       1.00      1.00      1.00         5\n          37       0.17      0.50      0.25         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.98      0.90      0.94        49\n          41       0.71      0.42      0.53        12\n          42       0.61      0.85      0.71        27\n          43       0.04      1.00      0.09         9\n          44       0.43      1.00      0.60         3\n          45       0.46      0.58      0.51        19\n          46       0.96      0.91      0.94       656\n          47       1.00      1.00      1.00         5\n          48       0.78      1.00      0.88         7\n          49       0.62      1.00      0.77         5\n          50       0.25      1.00      0.40         1\n          51       0.67      0.86      0.75         7\n          52       0.14      0.20      0.17         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.91      0.98      0.95        64\n          63       1.00      0.67      0.80         3\n          64       1.00      0.00      0.00         1\n          66       0.25      1.00      0.39        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.43      0.75      0.55         4\n          73       0.91      0.92      0.92       567\n          75       1.00      1.00      1.00        13\n          76       0.17      1.00      0.29         6\n          77       0.92      1.00      0.96        11\n          78       0.57      0.98      0.72        53\n          79       0.62      0.83      0.71         6\n          81       0.33      0.50      0.40         2\n          82       0.00      1.00      0.00         0\n          83       0.17      0.50      0.25         2\n          84       0.67      1.00      0.80         6\n          86       0.38      0.86      0.52         7\n          87       0.73      1.00      0.84        27\n          88       0.29      1.00      0.45         7\n          89       0.09      1.00      0.16         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.92      0.92      0.92        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.29      0.83      0.43         6\n         103       0.71      0.85      0.77        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.32      1.00      0.48         8\n         119       0.99      0.84      0.91      5640\n\n    accuracy                           0.89     10056\n   macro avg       0.51      0.87      0.55     10056\nweighted avg       0.96      0.89      0.91     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.78      0.90      0.84        20\n           3       0.97      0.99      0.98       101\n           4       0.35      1.00      0.52         8\n           5       0.14      1.00      0.25         5\n           6       1.00      1.00      1.00         1\n           7       0.80      0.80      0.80        10\n           8       0.75      1.00      0.86         3\n           9       0.14      0.67      0.24         3\n          11       0.21      1.00      0.35         3\n          12       0.00      1.00      0.00         0\n          13       0.57      0.57      0.57         7\n          14       0.15      0.60      0.24         5\n          15       0.69      0.82      0.75        11\n          16       0.20      0.91      0.33        11\n          17       0.16      0.74      0.26        46\n          18       0.80      1.00      0.89        33\n          19       0.60      1.00      0.75         6\n          20       1.00      0.98      0.99        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       1.00      0.92      0.96        12\n          25       0.97      0.98      0.98       102\n          26       0.94      0.85      0.89        39\n          27       0.79      0.95      0.86        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.08      1.00      0.15         1\n          31       0.44      0.89      0.59         9\n          32       0.92      0.66      0.77        68\n          33       0.17      1.00      0.29         1\n          34       0.88      0.85      0.87        34\n          35       0.72      0.90      0.80        31\n          36       0.83      1.00      0.91         5\n          37       0.33      0.50      0.40         2\n          38       0.46      1.00      0.63        12\n          39       0.60      0.86      0.71         7\n          40       0.71      0.86      0.78        49\n          41       0.83      0.42      0.56        12\n          42       0.88      0.85      0.87        27\n          43       0.04      1.00      0.08         9\n          44       0.75      1.00      0.86         3\n          45       0.27      0.68      0.38        19\n          46       0.91      0.92      0.92       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.60      0.86      0.71         7\n          52       0.09      0.20      0.12         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          61       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.33      0.93      0.49        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       0.18      1.00      0.31         2\n          72       0.14      0.75      0.23         4\n          73       0.86      0.92      0.89       567\n          74       0.00      1.00      0.00         0\n          75       1.00      1.00      1.00        13\n          76       0.21      1.00      0.34         6\n          77       1.00      1.00      1.00        11\n          78       0.58      0.98      0.73        53\n          79       0.55      1.00      0.71         6\n          80       0.00      1.00      0.00         0\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      0.83      0.91         6\n          86       0.46      0.86      0.60         7\n          87       0.75      1.00      0.86        27\n          88       0.30      1.00      0.47         7\n          89       0.08      1.00      0.15         2\n          90       0.79      0.92      0.85        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.33      1.00      0.50         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.36      0.83      0.50         6\n         103       0.54      0.75      0.62        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.67      1.00      0.80         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         113       0.00      1.00      0.00         0\n         115       0.45      1.00      0.62         5\n         116       0.93      0.99      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.81      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.51      0.89      0.55     10056\nweighted avg       0.95      0.87      0.90     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutLSTM_64,"{'name': 'sequential_112', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_74'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_74', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_34', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_9', 'trainable': True, 'dtype': 'float32', 'rate': 0.5, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_74', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_74', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9489305317401886,0.25365225970745087,0.18074406683444977,0.9582909941673279,0.235859252512455,0.2238897830247879,2553.5,1394.5,394.5,1545.0,7362.5,0.5500584398314182,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.18      1.00      0.31         2\n           2       0.67      0.90      0.77        20\n           3       0.93      0.98      0.96       101\n           4       0.33      1.00      0.50         8\n           5       0.10      0.80      0.18         5\n           6       1.00      1.00      1.00         1\n           7       0.62      1.00      0.77        10\n           8       1.00      1.00      1.00         3\n           9       0.19      1.00      0.32         3\n          11       0.38      1.00      0.55         3\n          13       0.70      1.00      0.82         7\n          14       0.44      0.80      0.57         5\n          15       1.00      0.91      0.95        11\n          16       0.40      0.91      0.56        11\n          17       0.24      0.70      0.36        46\n          18       0.80      0.97      0.88        33\n          19       0.86      1.00      0.92         6\n          20       0.85      1.00      0.92        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.79      0.92      0.85        12\n          25       0.94      0.98      0.96       102\n          26       0.85      0.87      0.86        39\n          27       0.47      0.97      0.64        39\n          28       0.83      0.83      0.83        12\n          29       0.50      1.00      0.67         1\n          30       0.17      1.00      0.29         1\n          31       0.54      0.78      0.64         9\n          32       0.87      0.79      0.83        68\n          33       0.33      1.00      0.50         1\n          34       0.93      0.82      0.88        34\n          35       0.78      0.90      0.84        31\n          36       0.62      1.00      0.77         5\n          37       0.17      0.50      0.25         2\n          38       0.38      1.00      0.55        12\n          39       0.67      0.86      0.75         7\n          40       0.90      0.88      0.89        49\n          41       0.42      0.42      0.42        12\n          42       0.82      0.85      0.84        27\n          43       0.06      0.78      0.12         9\n          44       0.60      1.00      0.75         3\n          45       0.23      0.84      0.36        19\n          46       0.86      0.94      0.90       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.62      1.00      0.77         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.11      0.20      0.14         5\n          53       0.00      1.00      0.00         0\n          54       0.75      1.00      0.86         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       0.75      1.00      0.86         3\n          64       0.00      0.00      0.00         1\n          66       0.30      1.00      0.46        14\n          67       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.12      0.75      0.21         4\n          73       0.85      0.95      0.90       567\n          74       0.00      1.00      0.00         0\n          75       0.76      1.00      0.87        13\n          76       0.55      1.00      0.71         6\n          77       0.50      0.91      0.65        11\n          78       0.60      0.98      0.74        53\n          79       0.55      1.00      0.71         6\n          81       0.50      1.00      0.67         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.41      1.00      0.58         7\n          87       0.66      1.00      0.79        27\n          88       0.30      1.00      0.47         7\n          89       0.10      1.00      0.18         2\n          90       0.52      0.92      0.67        12\n          92       0.00      1.00      0.00         0\n          93       0.33      0.50      0.40         2\n          95       0.75      1.00      0.86        24\n          97       0.50      1.00      0.67         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.13         2\n         102       0.23      1.00      0.38         6\n         103       0.72      0.90      0.80        20\n         104       0.94      0.95      0.94       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.82        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.36      1.00      0.53         8\n         119       0.99      0.83      0.90      5640\n\n    accuracy                           0.88     10056\n   macro avg       0.51      0.88      0.58     10056\nweighted avg       0.94      0.88      0.90     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.20      1.00      0.33         2\n           2       0.67      0.90      0.77        20\n           3       0.94      0.96      0.95       101\n           4       0.24      1.00      0.38         8\n           5       0.16      1.00      0.27         5\n           6       1.00      0.00      0.00         1\n           7       0.56      1.00      0.71        10\n           8       1.00      1.00      1.00         3\n           9       0.11      1.00      0.19         3\n          11       0.40      0.67      0.50         3\n          13       0.70      1.00      0.82         7\n          14       0.25      0.60      0.35         5\n          15       0.83      0.91      0.87        11\n          16       0.31      1.00      0.47        11\n          17       0.18      0.72      0.29        46\n          18       0.84      0.94      0.89        33\n          19       0.75      1.00      0.86         6\n          20       0.91      1.00      0.95        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.65      0.92      0.76        12\n          25       0.98      0.97      0.98       102\n          26       0.92      0.87      0.89        39\n          27       0.63      0.97      0.77        39\n          28       0.83      0.83      0.83        12\n          29       0.50      1.00      0.67         1\n          30       0.12      1.00      0.22         1\n          31       0.44      0.78      0.56         9\n          32       0.85      0.81      0.83        68\n          33       0.07      1.00      0.12         1\n          34       0.96      0.79      0.87        34\n          35       0.85      0.90      0.88        31\n          36       0.62      1.00      0.77         5\n          37       0.33      0.50      0.40         2\n          38       0.44      1.00      0.62        12\n          39       0.55      0.86      0.67         7\n          40       0.90      0.90      0.90        49\n          41       0.71      0.42      0.53        12\n          42       0.61      0.85      0.71        27\n          43       0.08      1.00      0.15         9\n          44       0.60      1.00      0.75         3\n          45       0.24      0.84      0.37        19\n          46       0.86      0.93      0.89       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       0.71      1.00      0.83         5\n          50       0.00      0.00      0.00         1\n          51       0.46      0.86      0.60         7\n          52       0.08      0.20      0.11         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.40      1.00      0.57         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       0.60      1.00      0.75         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.38      1.00      0.55        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.19      0.75      0.30         4\n          73       0.74      0.95      0.83       567\n          74       0.00      1.00      0.00         0\n          75       0.68      1.00      0.81        13\n          76       0.32      1.00      0.48         6\n          77       0.42      1.00      0.59        11\n          78       0.54      0.98      0.69        53\n          79       0.60      1.00      0.75         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       0.86      1.00      0.92         6\n          86       0.43      0.86      0.57         7\n          87       0.66      1.00      0.79        27\n          88       0.29      1.00      0.45         7\n          89       0.10      1.00      0.17         2\n          90       0.50      0.92      0.65        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.96      0.90        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.32      1.00      0.48         6\n         103       0.53      0.80      0.64        20\n         104       0.95      0.95      0.95       110\n         105       0.67      1.00      0.80         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.70      0.98      0.82        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.89      0.99      0.94       132\n         117       0.50      1.00      0.67         2\n         118       0.44      0.88      0.58         8\n         119       0.99      0.80      0.89      5640\n\n    accuracy                           0.87     10056\n   macro avg       0.50      0.88      0.55     10056\nweighted avg       0.93      0.87      0.89     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutGRU02,"{'name': 'sequential_116', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_77'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_77', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'gru_7', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_10', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_77', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_77', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 64)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 64)}}], 'build_input_shape': (None, 46)}",0.9718825221061707,0.2753359228372574,0.17545834183692932,0.9674761295318604,0.2475636973977089,0.1742577776312828,2570.5,976.5,377.5,1106.5,5932.5,0.5994808318347576,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.74      1.00      0.85        20\n           3       0.99      0.97      0.98       101\n           4       0.47      1.00      0.64         8\n           5       0.16      1.00      0.28         5\n           6       1.00      1.00      1.00         1\n           7       0.45      1.00      0.62        10\n           8       0.67      0.67      0.67         3\n           9       0.67      0.67      0.67         3\n          11       0.12      0.67      0.20         3\n          13       0.70      1.00      0.82         7\n          14       0.14      0.60      0.22         5\n          15       0.85      1.00      0.92        11\n          16       0.55      1.00      0.71        11\n          17       0.20      0.74      0.32        46\n          18       0.91      0.91      0.91        33\n          19       0.86      1.00      0.92         6\n          20       0.91      1.00      0.95        61\n          24       0.92      0.92      0.92        12\n          25       0.99      0.94      0.96       102\n          26       0.89      0.85      0.87        39\n          27       0.77      0.95      0.85        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.69      1.00      0.82         9\n          32       0.96      0.81      0.88        68\n          33       0.06      1.00      0.12         1\n          34       1.00      0.79      0.89        34\n          35       0.79      0.87      0.83        31\n          36       1.00      1.00      1.00         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.75      0.86      0.80         7\n          40       0.88      0.94      0.91        49\n          41       0.83      0.42      0.56        12\n          42       0.77      0.85      0.81        27\n          43       0.07      1.00      0.13         9\n          44       1.00      1.00      1.00         3\n          45       0.25      0.89      0.39        19\n          46       0.95      0.94      0.94       656\n          47       1.00      0.80      0.89         5\n          48       0.75      0.86      0.80         7\n          49       1.00      1.00      1.00         5\n          50       0.14      1.00      0.25         1\n          51       0.86      0.86      0.86         7\n          52       0.14      0.20      0.17         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.31      1.00      0.47        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.38      0.75      0.50         4\n          73       0.90      0.95      0.92       567\n          75       0.87      1.00      0.93        13\n          76       0.23      1.00      0.38         6\n          77       1.00      1.00      1.00        11\n          78       0.59      0.98      0.74        53\n          79       1.00      1.00      1.00         6\n          81       0.67      1.00      0.80         2\n          83       0.00      0.00      0.00         2\n          84       0.67      1.00      0.80         6\n          86       0.39      1.00      0.56         7\n          87       0.79      1.00      0.89        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.17         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.09      0.83      0.16         6\n         103       0.72      0.90      0.80        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.94      0.97      0.96       132\n         117       0.67      1.00      0.80         2\n         118       0.89      1.00      0.94         8\n         119       0.99      0.85      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.60      0.88      0.63     10056\nweighted avg       0.96      0.90      0.92     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.40      1.00      0.57         2\n           2       0.83      1.00      0.91        20\n           3       0.99      0.97      0.98       101\n           4       0.26      1.00      0.41         8\n           5       0.24      1.00      0.38         5\n           6       1.00      1.00      1.00         1\n           7       0.56      0.90      0.69        10\n           8       0.67      0.67      0.67         3\n           9       0.22      0.67      0.33         3\n          11       0.17      0.33      0.22         3\n          12       0.00      1.00      0.00         0\n          13       0.78      1.00      0.88         7\n          14       0.25      0.80      0.38         5\n          15       0.92      1.00      0.96        11\n          16       0.58      1.00      0.73        11\n          17       0.25      0.76      0.38        46\n          18       0.84      0.97      0.90        33\n          19       0.75      1.00      0.86         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.92      0.92      0.92        12\n          25       0.98      0.96      0.97       102\n          26       0.89      0.82      0.85        39\n          27       0.90      0.90      0.90        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.12      1.00      0.22         1\n          31       0.64      1.00      0.78         9\n          32       0.96      0.69      0.80        68\n          33       1.00      0.00      0.00         1\n          34       0.96      0.79      0.87        34\n          35       0.70      0.90      0.79        31\n          36       1.00      1.00      1.00         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.88      0.89        49\n          41       0.45      0.42      0.43        12\n          42       0.59      0.85      0.70        27\n          43       0.13      1.00      0.23         9\n          44       1.00      1.00      1.00         3\n          45       0.32      0.74      0.44        19\n          46       0.96      0.92      0.94       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.25      1.00      0.40         1\n          51       0.86      0.86      0.86         7\n          52       0.07      0.20      0.11         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.93      0.98      0.95        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.27      1.00      0.43        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.30      0.75      0.43         4\n          73       0.85      0.95      0.90       567\n          75       1.00      1.00      1.00        13\n          76       0.40      1.00      0.57         6\n          77       0.65      1.00      0.79        11\n          78       0.63      0.98      0.76        53\n          79       0.86      1.00      0.92         6\n          81       0.40      1.00      0.57         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.46      0.86      0.60         7\n          87       0.71      1.00      0.83        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.17         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.96      0.90        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.24      0.83      0.37         6\n         103       0.63      0.85      0.72        20\n         104       0.95      0.94      0.94       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         112       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.92      0.99      0.95       132\n         117       0.67      1.00      0.80         2\n         118       0.62      1.00      0.76         8\n         119       0.99      0.88      0.93      5640\n\n    accuracy                           0.91     10056\n   macro avg       0.58      0.88      0.60     10056\nweighted avg       0.95      0.91      0.93     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5

DropoutLSTM_6402,"{'name': 'sequential_118', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_78'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_78', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'LSTM', 'config': {'name': 'lstm_35', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'Dropout', 'config': {'name': 'dropout_11', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'seed': None, 'noise_shape': None}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_78', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_78', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9721167981624603,0.2715918570756912,0.09921318665146828,0.9677427709102631,0.2440330982208252,0.17256025969982147,2565.0,1023.5,383.0,1141.5,6222.5,0.6104150824683321,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.29      1.00      0.44         2\n           2       0.69      1.00      0.82        20\n           3       0.96      0.96      0.96       101\n           4       0.89      1.00      0.94         8\n           5       0.17      1.00      0.29         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       0.75      1.00      0.86         3\n           9       0.27      1.00      0.43         3\n          11       0.43      1.00      0.60         3\n          13       0.88      1.00      0.93         7\n          14       0.50      0.80      0.62         5\n          15       0.91      0.91      0.91        11\n          16       0.34      1.00      0.51        11\n          17       0.22      0.76      0.34        46\n          18       0.83      0.91      0.87        33\n          19       0.75      1.00      0.86         6\n          20       0.84      1.00      0.91        61\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.97      0.98      0.98       102\n          26       0.89      0.87      0.88        39\n          27       0.88      0.97      0.93        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.50      0.78      0.61         9\n          32       0.90      0.84      0.87        68\n          33       1.00      1.00      1.00         1\n          34       0.96      0.79      0.87        34\n          35       0.82      0.90      0.86        31\n          36       0.71      1.00      0.83         5\n          37       0.20      0.50      0.29         2\n          38       0.44      1.00      0.62        12\n          39       0.75      0.86      0.80         7\n          40       0.91      0.88      0.90        49\n          41       0.50      0.42      0.45        12\n          42       0.85      0.85      0.85        27\n          43       0.06      0.78      0.11         9\n          44       1.00      1.00      1.00         3\n          45       0.29      0.79      0.43        19\n          46       0.94      0.94      0.94       656\n          47       0.80      0.80      0.80         5\n          48       0.86      0.86      0.86         7\n          49       0.62      1.00      0.77         5\n          50       0.14      1.00      0.25         1\n          51       0.86      0.86      0.86         7\n          52       0.10      0.20      0.13         5\n          54       0.71      0.83      0.77         6\n          55       0.00      1.00      0.00         0\n          56       0.55      1.00      0.71         6\n          57       0.00      1.00      0.00         0\n          62       0.90      0.98      0.94        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.34      1.00      0.51        14\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.12      0.75      0.21         4\n          73       0.91      0.93      0.92       567\n          74       0.00      1.00      0.00         0\n          75       0.72      1.00      0.84        13\n          76       0.60      1.00      0.75         6\n          77       0.38      1.00      0.55        11\n          78       0.64      0.98      0.78        53\n          79       0.55      1.00      0.71         6\n          81       0.50      1.00      0.67         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.58      1.00      0.74         7\n          87       0.79      1.00      0.89        27\n          88       0.58      1.00      0.74         7\n          89       1.00      1.00      1.00         2\n          90       0.58      0.92      0.71        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          95       0.77      0.96      0.85        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.11      0.50      0.18         2\n         102       0.28      0.83      0.42         6\n         103       0.55      0.90      0.68        20\n         104       0.93      0.96      0.95       110\n         105       0.67      1.00      0.80         2\n         106       1.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.73      0.98      0.84        58\n         115       0.71      1.00      0.83         5\n         116       0.95      0.98      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.44      1.00      0.62         8\n         119       0.99      0.87      0.93      5640\n\n    accuracy                           0.91     10056\n   macro avg       0.61      0.90      0.65     10056\nweighted avg       0.95      0.91      0.93     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.25      1.00      0.40         2\n           2       0.75      0.90      0.82        20\n           3       0.94      0.99      0.97       101\n           4       0.26      1.00      0.41         8\n           5       0.19      1.00      0.31         5\n           6       1.00      0.00      0.00         1\n           7       0.59      1.00      0.74        10\n           8       0.60      1.00      0.75         3\n           9       0.43      1.00      0.60         3\n          11       0.40      0.67      0.50         3\n          13       0.88      1.00      0.93         7\n          14       0.44      0.80      0.57         5\n          15       0.91      0.91      0.91        11\n          16       0.37      1.00      0.54        11\n          17       0.24      0.78      0.37        46\n          18       0.82      0.94      0.87        33\n          19       0.86      1.00      0.92         6\n          20       0.87      1.00      0.93        61\n          21       0.00      1.00      0.00         0\n          22       0.00      1.00      0.00         0\n          24       0.85      0.92      0.88        12\n          25       0.94      0.97      0.96       102\n          26       0.89      0.87      0.88        39\n          27       0.90      0.95      0.93        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.50      1.00      0.67         1\n          31       0.64      0.78      0.70         9\n          32       0.86      0.84      0.85        68\n          33       1.00      1.00      1.00         1\n          34       0.96      0.79      0.87        34\n          35       0.76      0.90      0.82        31\n          36       0.62      1.00      0.77         5\n          37       0.20      0.50      0.29         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.90      0.92      0.91        49\n          41       0.36      0.42      0.38        12\n          42       0.61      0.85      0.71        27\n          43       0.07      0.78      0.13         9\n          44       1.00      1.00      1.00         3\n          45       0.26      0.84      0.40        19\n          46       0.93      0.94      0.93       656\n          47       0.83      1.00      0.91         5\n          48       0.42      0.71      0.53         7\n          49       0.62      1.00      0.77         5\n          50       0.00      0.00      0.00         1\n          51       0.55      0.86      0.67         7\n          52       0.12      0.20      0.15         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.89      0.98      0.93        64\n          63       1.00      1.00      1.00         3\n          64       0.00      0.00      0.00         1\n          65       0.00      1.00      0.00         0\n          66       0.36      1.00      0.53        14\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.10      1.00      0.18         4\n          73       0.91      0.91      0.91       567\n          74       0.00      1.00      0.00         0\n          75       0.76      1.00      0.87        13\n          76       0.46      1.00      0.63         6\n          77       0.34      1.00      0.51        11\n          78       0.57      0.98      0.72        53\n          79       0.75      1.00      0.86         6\n          81       0.33      1.00      0.50         2\n          83       0.00      0.00      0.00         2\n          84       1.00      1.00      1.00         6\n          86       0.46      0.86      0.60         7\n          87       0.64      1.00      0.78        27\n          88       0.32      1.00      0.48         7\n          89       0.10      1.00      0.18         2\n          90       0.65      0.92      0.76        12\n          92       0.00      1.00      0.00         0\n          93       0.50      1.00      0.67         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.17      0.83      0.28         6\n         103       0.70      0.95      0.81        20\n         104       0.94      0.96      0.95       110\n         105       0.50      1.00      0.67         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.69      0.98      0.81        58\n         115       0.83      1.00      0.91         5\n         116       0.94      0.99      0.97       132\n         117       0.67      1.00      0.80         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.86      0.92      5640\n\n    accuracy                           0.90     10056\n   macro avg       0.54      0.88      0.60     10056\nweighted avg       0.95      0.90      0.92     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5
BidirectionalGRU_64,"{'name': 'sequential_4', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_2'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_2', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_1', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_2', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.986227422952652,0.29106631875038147,0.04623243398964405,0.9802239835262299,0.2612162232398987,0.10471392795443535,2650.0,492.0,298.0,649.5,3894.0,0.6689236922110806,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.33      1.00      0.50         2\n           2       0.95      1.00      0.98        20\n           3       0.98      0.99      0.99       101\n           4       0.42      1.00      0.59         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.77      1.00      0.87        10\n           8       0.60      1.00      0.75         3\n           9       0.60      1.00      0.75         3\n          11       0.50      1.00      0.67         3\n          13       0.88      1.00      0.93         7\n          14       0.33      0.80      0.47         5\n          15       0.92      1.00      0.96        11\n          16       0.52      1.00      0.69        11\n          17       0.43      0.83      0.57        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.79      0.92      0.85        12\n          25       0.99      0.98      0.99       102\n          26       0.97      0.90      0.93        39\n          27       0.93      0.95      0.94        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.17      1.00      0.29         1\n          31       0.75      1.00      0.86         9\n          32       0.89      0.82      0.85        68\n          33       0.33      1.00      0.50         1\n          34       1.00      0.79      0.89        34\n          35       1.00      0.87      0.93        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.52      1.00      0.69        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.98      0.94        49\n          41       1.00      0.42      0.59        12\n          42       0.74      0.85      0.79        27\n          43       0.30      1.00      0.46         9\n          44       1.00      1.00      1.00         3\n          45       0.55      0.84      0.67        19\n          46       0.96      0.96      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.18      0.40      0.25         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.94      0.98      0.96        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.52      0.93      0.67        14\n          70       0.00      1.00      0.00         0\n          71       0.67      1.00      0.80         2\n          72       0.50      1.00      0.67         4\n          73       0.96      0.96      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.69      1.00      0.81        11\n          78       0.68      0.98      0.81        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.41      1.00      0.58         7\n          89       0.20      1.00      0.33         2\n          90       0.92      0.92      0.92        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.21      0.83      0.33         6\n         103       0.75      0.75      0.75        20\n         104       0.96      0.98      0.97       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       0.67      1.00      0.80         2\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.97      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.92      0.96      5640\n\n    accuracy                           0.94     10056\n   macro avg       0.65      0.91      0.69     10056\nweighted avg       0.97      0.94      0.95     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.67      1.00      0.80         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.89      1.00      0.94         8\n           5       0.33      1.00      0.50         5\n           6       1.00      1.00      1.00         1\n           7       0.67      1.00      0.80        10\n           8       0.67      0.67      0.67         3\n           9       0.50      0.67      0.57         3\n          11       0.25      0.67      0.36         3\n          13       0.78      1.00      0.88         7\n          14       0.38      0.60      0.46         5\n          15       0.92      1.00      0.96        11\n          16       0.69      1.00      0.81        11\n          17       0.53      0.85      0.66        46\n          18       0.97      0.97      0.97        33\n          19       0.86      1.00      0.92         6\n          20       0.98      1.00      0.99        61\n          24       0.92      0.92      0.92        12\n          25       0.99      0.98      0.99       102\n          26       0.95      0.90      0.92        39\n          27       0.90      0.95      0.93        39\n          28       0.73      0.92      0.81        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.82      1.00      0.90         9\n          32       0.97      0.84      0.90        68\n          33       0.33      1.00      0.50         1\n          34       0.90      0.79      0.84        34\n          35       0.94      0.94      0.94        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.67      1.00      0.80        12\n          39       0.86      0.86      0.86         7\n          40       0.89      0.98      0.93        49\n          41       0.83      0.42      0.56        12\n          42       0.82      0.85      0.84        27\n          43       0.40      0.89      0.55         9\n          44       1.00      1.00      1.00         3\n          45       0.48      0.79      0.60        19\n          46       0.96      0.95      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.70      1.00      0.82         7\n          49       1.00      1.00      1.00         5\n          50       0.50      1.00      0.67         1\n          51       0.86      0.86      0.86         7\n          52       0.29      0.40      0.33         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.78      1.00      0.88        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.44      1.00      0.62         4\n          73       0.95      0.97      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.86      1.00      0.92         6\n          77       0.83      0.91      0.87        11\n          78       0.63      0.98      0.77        53\n          79       0.75      1.00      0.86         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.37      1.00      0.54         7\n          89       0.29      1.00      0.44         2\n          90       0.73      0.92      0.81        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.81      0.92      0.86        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.38      0.83      0.53         6\n         103       0.70      0.70      0.70        20\n         104       0.96      0.97      0.97       110\n         105       1.00      0.50      0.67         2\n         106       0.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.89      0.98      0.93        58\n         115       0.56      1.00      0.71         5\n         116       0.97      0.99      0.98       132\n         117       0.67      1.00      0.80         2\n         118       0.73      1.00      0.84         8\n         119       0.99      0.94      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.65      0.90      0.69     10056\nweighted avg       0.97      0.95      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5
BidirectionalGRU_128,"{'name': 'sequential_7', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_4'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_4', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_2', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_2', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_4', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9903829991817474,0.2988511919975281,0.021738569252192974,0.9841743409633636,0.26710928976535797,0.08901358395814896,2693.5,349.5,254.5,496.5,2983.0,0.719621559923409,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.80      1.00      0.89         8\n           5       0.56      1.00      0.71         5\n           6       1.00      1.00      1.00         1\n           7       0.71      1.00      0.83        10\n           8       1.00      0.67      0.80         3\n           9       0.67      0.67      0.67         3\n          11       0.60      1.00      0.75         3\n          13       0.88      1.00      0.93         7\n          14       0.36      0.80      0.50         5\n          15       1.00      0.91      0.95        11\n          16       0.69      1.00      0.81        11\n          17       0.75      0.85      0.80        46\n          18       1.00      1.00      1.00        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.80      1.00      0.89        12\n          25       0.99      0.98      0.99       102\n          26       0.95      0.92      0.94        39\n          27       0.84      0.95      0.89        39\n          28       0.92      0.92      0.92        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.69      1.00      0.82         9\n          32       0.95      0.85      0.90        68\n          33       0.33      1.00      0.50         1\n          34       0.96      0.76      0.85        34\n          35       1.00      0.94      0.97        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.71      1.00      0.83        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.98      0.96        49\n          41       0.71      0.42      0.53        12\n          42       0.83      0.93      0.88        27\n          43       0.39      1.00      0.56         9\n          44       1.00      1.00      1.00         3\n          45       0.85      0.89      0.87        19\n          46       0.96      0.96      0.96       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.50      0.40      0.44         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      0.00      0.00         1\n          66       0.87      0.93      0.90        14\n          71       1.00      1.00      1.00         2\n          72       0.40      1.00      0.57         4\n          73       0.95      0.96      0.96       567\n          75       0.93      1.00      0.96        13\n          76       0.75      1.00      0.86         6\n          77       0.59      0.91      0.71        11\n          78       0.66      0.98      0.79        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.33      0.50      0.40         2\n          84       1.00      1.00      1.00         6\n          86       0.50      1.00      0.67         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       0.67      1.00      0.80         2\n          90       0.85      0.92      0.88        12\n          91       0.00      1.00      0.00         0\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.14      0.50      0.22         2\n         102       0.33      0.83      0.48         6\n         103       0.75      0.75      0.75        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         115       0.56      1.00      0.71         5\n         116       0.97      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.70      0.90      0.72     10056\nweighted avg       0.97      0.96      0.96     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.80      1.00      0.89         8\n           5       0.38      1.00      0.56         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       0.67      0.67      0.67         3\n           9       0.67      0.67      0.67         3\n          11       0.75      1.00      0.86         3\n          13       0.88      1.00      0.93         7\n          14       0.33      0.80      0.47         5\n          15       1.00      1.00      1.00        11\n          16       0.65      1.00      0.79        11\n          17       0.68      0.83      0.75        46\n          18       1.00      1.00      1.00        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.80      1.00      0.89        12\n          25       0.99      0.98      0.99       102\n          26       0.92      0.92      0.92        39\n          27       0.90      0.97      0.94        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.64      1.00      0.78         9\n          32       0.92      0.87      0.89        68\n          33       1.00      1.00      1.00         1\n          34       0.97      0.85      0.91        34\n          35       0.88      0.94      0.91        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.86      1.00      0.92        12\n          39       0.86      0.86      0.86         7\n          40       0.98      0.98      0.98        49\n          41       1.00      0.42      0.59        12\n          42       0.83      0.93      0.88        27\n          43       0.38      1.00      0.55         9\n          44       1.00      1.00      1.00         3\n          45       0.73      0.84      0.78        19\n          46       0.95      0.96      0.96       656\n          47       1.00      1.00      1.00         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       1.00      1.00      1.00         1\n          51       0.86      0.86      0.86         7\n          52       0.29      0.40      0.33         5\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.97      0.98      0.98        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.93      0.93      0.93        14\n          71       1.00      1.00      1.00         2\n          72       0.80      1.00      0.89         4\n          73       0.96      0.96      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.73      1.00      0.85        11\n          78       0.64      0.98      0.78        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.41      1.00      0.58         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       0.67      1.00      0.80         2\n          90       0.85      0.92      0.88        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.88      0.96      0.92        24\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.12      0.50      0.20         2\n         102       0.23      0.83      0.36         6\n         103       0.74      0.85      0.79        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.90      0.98      0.94        58\n         113       0.00      1.00      0.00         0\n         115       0.71      1.00      0.83         5\n         116       0.98      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.53      1.00      0.70         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.74      0.91      0.77     10056\nweighted avg       0.97      0.96      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5
BidirectionalGRU_64,"{'name': 'sequential_10', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_6'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_6', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_3', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_3', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_6', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_6', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 128)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 128)}}], 'build_input_shape': (None, 46)}",0.9873869121074677,0.2915613353252411,0.046520547941327095,0.9808661937713623,0.26019057631492615,0.10251652076840401,2680.5,460.0,267.5,610.5,3605.5,0.6471528761606313,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      1.00      0.98        20\n           3       0.98      0.99      0.99       101\n           4       0.62      1.00      0.76         8\n           5       0.38      1.00      0.56         5\n           6       1.00      0.00      0.00         1\n           7       0.83      1.00      0.91        10\n           8       1.00      1.00      1.00         3\n           9       1.00      1.00      1.00         3\n          11       0.67      0.67      0.67         3\n          13       1.00      0.86      0.92         7\n          14       0.22      0.40      0.29         5\n          15       0.73      1.00      0.85        11\n          16       0.44      1.00      0.61        11\n          17       0.54      0.83      0.66        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.86      1.00      0.92        12\n          25       0.93      0.98      0.96       102\n          26       0.92      0.92      0.92        39\n          27       0.93      0.97      0.95        39\n          28       0.79      0.92      0.85        12\n          29       1.00      1.00      1.00         1\n          30       0.00      0.00      0.00         1\n          31       0.70      0.78      0.74         9\n          32       0.91      0.85      0.88        68\n          33       0.00      0.00      0.00         1\n          34       0.93      0.79      0.86        34\n          35       0.96      0.87      0.92        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.46      1.00      0.63        12\n          39       0.86      0.86      0.86         7\n          40       0.92      0.98      0.95        49\n          41       0.71      0.42      0.53        12\n          42       0.85      0.85      0.85        27\n          43       0.27      1.00      0.43         9\n          44       1.00      1.00      1.00         3\n          45       0.74      0.89      0.81        19\n          46       0.96      0.96      0.96       656\n          47       1.00      1.00      1.00         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       0.33      1.00      0.50         1\n          51       0.86      0.86      0.86         7\n          52       0.22      0.40      0.29         5\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          60       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       1.00      1.00      1.00         3\n          64       1.00      1.00      1.00         1\n          66       0.88      1.00      0.93        14\n          68       0.00      1.00      0.00         0\n          69       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.33      1.00      0.50         4\n          73       0.96      0.96      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.86      1.00      0.92         6\n          77       0.69      1.00      0.81        11\n          78       0.65      0.98      0.78        53\n          79       0.75      1.00      0.86         6\n          81       0.50      0.50      0.50         2\n          83       0.00      0.00      0.00         2\n          84       0.86      1.00      0.92         6\n          85       0.00      1.00      0.00         0\n          86       0.33      1.00      0.50         7\n          87       1.00      1.00      1.00        27\n          88       0.37      1.00      0.54         7\n          89       0.10      1.00      0.17         2\n          90       0.69      0.92      0.79        12\n          92       0.00      1.00      0.00         0\n          93       0.67      1.00      0.80         2\n          94       0.00      1.00      0.00         0\n          95       0.84      0.88      0.86        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.22      0.83      0.34         6\n         103       0.68      0.65      0.67        20\n         104       0.95      0.95      0.95       110\n         105       1.00      1.00      1.00         2\n         106       1.00      0.00      0.00         1\n         108       0.50      1.00      0.67         2\n         111       0.90      0.98      0.94        58\n         113       0.00      1.00      0.00         0\n         115       0.50      1.00      0.67         5\n         116       0.96      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.47      1.00      0.64         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.64      0.88      0.66     10056\nweighted avg       0.97      0.95      0.96     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       1.00      1.00      1.00         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.67      1.00      0.80         8\n           5       0.42      1.00      0.59         5\n           6       1.00      1.00      1.00         1\n           7       0.77      1.00      0.87        10\n           8       0.60      1.00      0.75         3\n           9       1.00      0.67      0.80         3\n          11       0.60      1.00      0.75         3\n          13       0.78      1.00      0.88         7\n          14       0.36      0.80      0.50         5\n          15       0.92      1.00      0.96        11\n          16       0.55      1.00      0.71        11\n          17       0.42      0.83      0.55        46\n          18       0.97      1.00      0.99        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          22       0.00      1.00      0.00         0\n          24       0.80      1.00      0.89        12\n          25       0.99      0.98      0.99       102\n          26       0.97      0.90      0.93        39\n          27       0.84      0.95      0.89        39\n          28       0.85      0.92      0.88        12\n          29       1.00      1.00      1.00         1\n          30       0.25      1.00      0.40         1\n          31       0.69      1.00      0.82         9\n          32       0.92      0.87      0.89        68\n          33       0.25      1.00      0.40         1\n          34       0.97      0.82      0.89        34\n          35       0.97      0.90      0.93        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.43      1.00      0.60        12\n          39       0.86      0.86      0.86         7\n          40       0.96      0.98      0.97        49\n          41       0.71      0.42      0.53        12\n          42       0.77      0.85      0.81        27\n          43       0.27      1.00      0.43         9\n          44       1.00      1.00      1.00         3\n          45       0.84      0.84      0.84        19\n          46       0.96      0.95      0.96       656\n          47       1.00      0.80      0.89         5\n          48       0.78      1.00      0.88         7\n          49       1.00      1.00      1.00         5\n          50       0.25      1.00      0.40         1\n          51       0.86      0.86      0.86         7\n          52       0.29      0.40      0.33         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          55       0.00      1.00      0.00         0\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.97      0.98      0.98        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.52      0.93      0.67        14\n          68       0.00      1.00      0.00         0\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.44      1.00      0.62         4\n          73       0.95      0.97      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.86      1.00      0.92         6\n          77       0.79      1.00      0.88        11\n          78       0.60      0.98      0.74        53\n          79       0.75      1.00      0.86         6\n          81       0.50      1.00      0.67         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.30      1.00      0.47         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       0.29      1.00      0.44         2\n          90       0.85      0.92      0.88        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.85      0.92      0.88        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.00      0.00      0.00         2\n         102       0.42      0.83      0.56         6\n         103       0.70      0.70      0.70        20\n         104       0.95      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         109       0.00      1.00      0.00         0\n         111       0.89      0.98      0.93        58\n         113       0.00      1.00      0.00         0\n         115       0.56      1.00      0.71         5\n         116       0.96      0.99      0.97       132\n         117       1.00      1.00      1.00         2\n         118       0.57      1.00      0.73         8\n         119       0.99      0.93      0.96      5640\n\n    accuracy                           0.95     10056\n   macro avg       0.64      0.91      0.67     10056\nweighted avg       0.97      0.95      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5
BidirectionalGRU_128,"{'name': 'sequential_13', 'trainable': True, 'dtype': 'float32', 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': (None, 46), 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_8'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'Embedding', 'config': {'name': 'embedding_8', 'trainable': True, 'dtype': 'float32', 'input_dim': 1001, 'output_dim': 1024, 'embeddings_initializer': {'module': 'keras.initializers', 'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}, 'registered_name': None}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False}, 'registered_name': None, 'build_config': {'input_shape': (None, 46)}}, {'module': 'keras.layers', 'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_4', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'forward_gru_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, 'merge_mode': 'concat', 'backward_layer': {'module': 'keras.layers', 'class_name': 'GRU', 'config': {'name': 'backward_gru_4', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'zero_output_for_mask': True, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'recurrent_initializer': {'module': 'keras.initializers', 'class_name': 'OrthogonalInitializer', 'config': {'gain': 1.0, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'reset_after': True, 'seed': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 1024)}}, {'module': 'keras.layers', 'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_8', 'trainable': True, 'dtype': 'float32', 'layer': {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'dtype': 'float32', 'units': 120, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 256)}}}, 'registered_name': None, 'build_config': {'input_shape': (None, 46, 256)}}], 'build_input_shape': (None, 46)}",0.9896284937858582,0.2962684631347656,0.022766927257180214,0.9839925467967987,0.2653697431087494,0.09229683503508568,2683.0,381.0,265.0,517.5,3228.5,0.6925375230224093,"['              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      1.00      0.98        20\n           3       0.98      0.99      0.99       101\n           4       1.00      1.00      1.00         8\n           5       0.45      1.00      0.62         5\n           6       1.00      1.00      1.00         1\n           7       0.77      1.00      0.87        10\n           8       1.00      0.67      0.80         3\n           9       1.00      0.67      0.80         3\n          11       0.40      0.67      0.50         3\n          13       0.78      1.00      0.88         7\n          14       0.43      0.60      0.50         5\n          15       1.00      1.00      1.00        11\n          16       0.65      1.00      0.79        11\n          17       0.57      0.83      0.67        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          24       0.92      1.00      0.96        12\n          25       0.98      0.98      0.98       102\n          26       0.92      0.92      0.92        39\n          27       0.88      0.95      0.91        39\n          28       1.00      0.92      0.96        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.62      0.89      0.73         9\n          32       0.95      0.88      0.92        68\n          33       0.25      1.00      0.40         1\n          34       1.00      0.85      0.92        34\n          35       0.91      0.94      0.92        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.86      1.00      0.92        12\n          39       0.86      0.86      0.86         7\n          40       0.94      0.98      0.96        49\n          41       0.83      0.42      0.56        12\n          42       0.79      0.85      0.82        27\n          43       0.53      1.00      0.69         9\n          44       1.00      1.00      1.00         3\n          45       0.77      0.89      0.83        19\n          46       0.96      0.96      0.96       656\n          47       1.00      0.80      0.89         5\n          48       1.00      1.00      1.00         7\n          49       1.00      1.00      1.00         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.29      0.40      0.33         5\n          53       0.00      1.00      0.00         0\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          59       0.00      1.00      0.00         0\n          62       0.95      0.98      0.97        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.93      0.93      0.93        14\n          70       0.00      1.00      0.00         0\n          71       1.00      1.00      1.00         2\n          72       0.44      1.00      0.62         4\n          73       0.96      0.96      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.73      1.00      0.85        11\n          78       0.84      0.98      0.90        53\n          79       0.86      1.00      0.92         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          86       0.54      1.00      0.70         7\n          87       1.00      1.00      1.00        27\n          88       0.78      1.00      0.88         7\n          89       0.67      1.00      0.80         2\n          90       0.73      0.92      0.81        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.96      0.87        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.10      0.50      0.17         2\n         102       0.33      0.83      0.48         6\n         103       0.74      0.85      0.79        20\n         104       0.95      0.96      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.90      0.98      0.94        58\n         115       0.71      1.00      0.83         5\n         116       0.96      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.42      1.00      0.59         8\n         119       0.99      0.96      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.72      0.89      0.73     10056\nweighted avg       0.97      0.96      0.97     10056\n', '              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      1702\n           1       0.50      1.00      0.67         2\n           2       0.95      1.00      0.98        20\n           3       0.99      0.99      0.99       101\n           4       0.89      1.00      0.94         8\n           5       0.33      1.00      0.50         5\n           6       1.00      1.00      1.00         1\n           7       0.83      1.00      0.91        10\n           8       0.67      0.67      0.67         3\n           9       0.67      0.67      0.67         3\n          11       0.67      0.67      0.67         3\n          13       1.00      1.00      1.00         7\n          14       0.40      0.80      0.53         5\n          15       1.00      0.91      0.95        11\n          16       0.58      1.00      0.73        11\n          17       0.75      0.83      0.78        46\n          18       1.00      0.97      0.98        33\n          19       0.86      1.00      0.92         6\n          20       1.00      1.00      1.00        61\n          21       0.00      1.00      0.00         0\n          24       0.86      1.00      0.92        12\n          25       0.99      0.97      0.98       102\n          26       0.93      0.95      0.94        39\n          27       0.86      0.97      0.92        39\n          28       0.69      0.92      0.79        12\n          29       1.00      1.00      1.00         1\n          30       0.33      1.00      0.50         1\n          31       0.69      1.00      0.82         9\n          32       0.92      0.87      0.89        68\n          33       0.50      1.00      0.67         1\n          34       0.93      0.82      0.88        34\n          35       0.90      0.90      0.90        31\n          36       1.00      1.00      1.00         5\n          37       0.50      0.50      0.50         2\n          38       0.75      1.00      0.86        12\n          39       0.86      0.86      0.86         7\n          40       0.91      0.98      0.94        49\n          41       0.56      0.42      0.48        12\n          42       0.88      0.85      0.87        27\n          43       0.43      1.00      0.60         9\n          44       0.50      1.00      0.67         3\n          45       0.54      0.74      0.62        19\n          46       0.97      0.96      0.96       656\n          47       1.00      0.80      0.89         5\n          48       0.88      1.00      0.93         7\n          49       1.00      1.00      1.00         5\n          50       1.00      0.00      0.00         1\n          51       0.86      0.86      0.86         7\n          52       0.33      0.40      0.36         5\n          54       0.83      0.83      0.83         6\n          56       0.50      1.00      0.67         6\n          57       0.00      1.00      0.00         0\n          62       0.97      0.98      0.98        64\n          63       0.75      1.00      0.86         3\n          64       1.00      0.00      0.00         1\n          66       0.81      0.93      0.87        14\n          71       1.00      1.00      1.00         2\n          72       0.33      1.00      0.50         4\n          73       0.95      0.97      0.96       567\n          75       1.00      1.00      1.00        13\n          76       0.75      1.00      0.86         6\n          77       0.79      1.00      0.88        11\n          78       0.83      0.98      0.90        53\n          79       0.67      1.00      0.80         6\n          81       0.67      1.00      0.80         2\n          83       0.50      0.50      0.50         2\n          84       1.00      1.00      1.00         6\n          85       0.00      1.00      0.00         0\n          86       0.41      1.00      0.58         7\n          87       1.00      1.00      1.00        27\n          88       0.39      1.00      0.56         7\n          89       1.00      1.00      1.00         2\n          90       0.92      0.92      0.92        12\n          92       0.00      1.00      0.00         0\n          93       1.00      1.00      1.00         2\n          94       0.00      1.00      0.00         0\n          95       0.79      0.92      0.85        24\n          96       0.00      1.00      0.00         0\n          97       1.00      1.00      1.00         1\n          98       0.50      0.50      0.50         2\n          99       0.00      1.00      0.00         0\n         100       0.00      1.00      0.00         0\n         101       0.08      0.50      0.14         2\n         102       0.24      0.83      0.37         6\n         103       0.67      0.60      0.63        20\n         104       0.96      0.97      0.96       110\n         105       1.00      1.00      1.00         2\n         106       0.00      0.00      0.00         1\n         108       1.00      1.00      1.00         2\n         111       0.89      0.98      0.93        58\n         115       0.50      1.00      0.67         5\n         116       0.97      0.99      0.98       132\n         117       1.00      1.00      1.00         2\n         118       0.50      1.00      0.67         8\n         119       0.99      0.95      0.97      5640\n\n    accuracy                           0.96     10056\n   macro avg       0.70      0.89      0.72     10056\nweighted avg       0.97      0.96      0.96     10056\n']",1000,1024,10,32,False,False,False,,post,accuracy,macro,True,False,5
