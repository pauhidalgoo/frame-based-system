{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../INTENT RECOGNITION')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../NAME ENTITY RECOGNITION')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner import NamedEntityRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intent_recognition import IntentRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = { 'use_class_weights':True}\n",
    "hyperparams = {'vocab_size': 500, 'embedding_dim': 768, 'epochs': 10, 'batch_size': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), merge_mode=\"ave\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "basic = IntentRecognition(\n",
    "            model, \n",
    "            hyperparams, \n",
    "            training_times=1, \n",
    "            train_config=train_config, \n",
    "            verbosing=0, \n",
    "            name=\"Bidirectional\",\n",
    "            save_results=False\n",
    "        )\n",
    "basic.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = np.array(basic.tokenizer.texts_to_sequences([\"I want a to eat a steak to Boston please\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    }
   ],
   "source": [
    "probs = basic.model.predict(tokenized)\n",
    "_predicted_labels = np.argmax(probs, axis=1)\n",
    "predicted_labels = basic.label_encoder.inverse_transform(_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['flight'], dtype='<U26')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 189ms/step\n",
      "897\n",
      "897\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/NER_results_complete.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Bidirectional(GRU(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)))\n\u001b[0;32m      4\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m NamedEntityRecognition(\n\u001b[0;32m      5\u001b[0m             model, \n\u001b[0;32m      6\u001b[0m             hyperparams, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m             save_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mner_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\5_Quatri\\TVD\\frame-based\\frame-based-system\\NAME ENTITY RECOGNITION\\ner.py:492\u001b[0m, in \u001b[0;36mNamedEntityRecognition.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_information \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_training_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: average_training_acc,\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_training_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: average_training_f1,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msci_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: sci_f1\n\u001b[0;32m    489\u001b[0m }\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_results(complete_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 492\u001b[0m average_metrics \u001b[38;5;241m=\u001b[39m [{\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture_name,\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_config(),\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_training_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: average_training_acc,\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_training_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: average_training_f1,\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_training_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: average_training_loss,\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_val_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: average_val_acc,\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_val_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: average_val_f1,\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: average_val_loss,\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_tp\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_positive\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m evaluation_results]),\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_fp\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse_positive\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m evaluation_results]),\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_fn\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse_negative\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m evaluation_results]),\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_errors\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_errors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m evaluation_results]),\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m evaluation_results]),\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msci_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(sci_f1),\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m'\u001b[39m: reports,\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparams,\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_config,\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_config\n\u001b[0;32m    511\u001b[0m }]\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_results(average_metrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_average.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    514\u001b[0m best_metrics \u001b[38;5;241m=\u001b[39m [{\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture_name,\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_config(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_config\n\u001b[0;32m    531\u001b[0m }]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\5_Quatri\\TVD\\frame-based\\frame-based-system\\NAME ENTITY RECOGNITION\\ner.py:140\u001b[0m, in \u001b[0;36mNamedEntityRecognition._save_results\u001b[1;34m(self, results_dict, file_path, header)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_results \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfew\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m file_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_complete.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    141\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39mheader \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;28;01melse\u001b[39;00m results_dict[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    142\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader() \u001b[38;5;66;03m# Uncomment if the files aren't created\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/NER_results_complete.csv'"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "\n",
    "ner_model = NamedEntityRecognition(\n",
    "            model, \n",
    "            hyperparams, \n",
    "            training_times=1, \n",
    "            train_config=train_config, \n",
    "            verbosing=0, \n",
    "            name=\"Bidirectional\",\n",
    "            save_results=False\n",
    "        )\n",
    "ner_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12,  55,  14, 171,   1,  78,   1,  11, 105]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized2 = np.array(ner_model.tokenizer.texts_to_sequences([\"i need a ticket to go to san diego\"]))\n",
    "\n",
    "tokenized2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = ner_model.model.input_shape[1]  # Adjust based on your model’s expected input\n",
    "tokenized2_padded = pad_sequences(tokenized2, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n"
     ]
    }
   ],
   "source": [
    "probs = ner_model.model.predict(tokenized2_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "[array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-toloc.city_name',\n",
      "       'I-toloc.city_name', 'B-toloc.state_name', 'I-mod', '<pad>',\n",
      "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
      "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
      "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
      "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>',\n",
      "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], dtype='<U28')]\n"
     ]
    }
   ],
   "source": [
    "probs = ner_model.model.predict(tokenized2_padded)\n",
    "_predicted_labels = np.argmax(probs, axis=2)\n",
    "_predicted_labels\n",
    "predicted_labels = [ner_model.label_encoder.inverse_transform(a) for a in _predicted_labels]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the ATIS-based dialog system! Type 'exit' to end the conversation.\n",
      "You:  information about airports in california\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Bot: Looking up flight information to california.\n",
      "You:  california\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Bot: Looking up flight information to california.\n",
      "You:  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTM.call().\n\n\u001b[1mslice index 0 of dimension 1 out of bounds. for '{{node sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2](sequential_1_1/embedding_1/GatherV2, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack_1, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack_2)' with input shapes: [1,0,768], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <0 1 0>, input[3] = <1 1 1>.\u001b[0m\n\nArguments received by LSTM.call():\n  • sequences=tf.Tensor(shape=(1, 0, 768), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Run the dialog system\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43mchat_with_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 119\u001b[0m, in \u001b[0;36mchat_with_bot\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mdialog_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_user_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[56], line 102\u001b[0m, in \u001b[0;36mDialogSystem.handle_user_input\u001b[1;34m(self, user_input)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_user_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_input):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    Process user input, identify intent and entities, and generate a response.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     intent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intent\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entities(user_input)\n\u001b[0;32m    104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_response(intent, entities)\n",
      "Cell \u001b[1;32mIn[56], line 28\u001b[0m, in \u001b[0;36mDialogSystem.get_intent\u001b[1;34m(self, user_input)\u001b[0m\n\u001b[0;32m     26\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_input(user_input)\n\u001b[0;32m     27\u001b[0m tokenized2_padded \u001b[38;5;241m=\u001b[39m pad_sequences(tokenized_input, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintent_model\u001b[38;5;241m.\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_label)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling LSTM.call().\n\n\u001b[1mslice index 0 of dimension 1 out of bounds. for '{{node sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=5, ellipsis_mask=0, end_mask=5, new_axis_mask=0, shrink_axis_mask=2](sequential_1_1/embedding_1/GatherV2, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack_1, sequential_1_1/bidirectional_1/forward_lstm_1/strided_slice/stack_2)' with input shapes: [1,0,768], [3], [3], [3] and with computed input tensors: input[1] = <0 0 0>, input[2] = <0 1 0>, input[3] = <1 1 1>.\u001b[0m\n\nArguments received by LSTM.call():\n  • sequences=tf.Tensor(shape=(1, 0, 768), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "\n",
    "class DialogSystem:\n",
    "    def __init__(self, intent_model, ner_model):\n",
    "        \"\"\"\n",
    "        Initialize the DialogSystem with intent recognition and named entity recognition models.\n",
    "        \n",
    "        Parameters:\n",
    "        - intent_model: an instance of IntentRecognition trained model.\n",
    "        - ner_model: an instance of NamedEntityRecognition trained model.\n",
    "        \"\"\"\n",
    "        self.intent_model = intent_model\n",
    "        self.ner_model = ner_model\n",
    "    \n",
    "    def preprocess_input(self, user_input):\n",
    "        \"\"\"\n",
    "        Preprocess the user input for tokenization.\n",
    "        \"\"\"\n",
    "        return np.array(self.intent_model.tokenizer.texts_to_sequences([user_input]))\n",
    "\n",
    "    def get_intent(self, user_input):\n",
    "        \"\"\"\n",
    "        Predict the intent of the user input using the intent model.\n",
    "        \"\"\"\n",
    "        tokenized_input = self.preprocess_input(user_input)\n",
    "        tokenized2_padded = pad_sequences(tokenized_input, maxlen=max_length, padding=\"post\")\n",
    "        probs = self.intent_model.model.predict(tokenized2_padded)\n",
    "        predicted_label = np.argmax(probs, axis=1)\n",
    "        return self.intent_model.label_encoder.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    def get_entities(self, user_input):\n",
    "        \"\"\"\n",
    "        Extract entities from the user input using the NER model.\n",
    "        \"\"\"\n",
    "        tokenized_input = np.array(self.ner_model.tokenizer.texts_to_sequences([user_input]))\n",
    "        max_length = self.ner_model.model.input_shape[1]  # Adjust based on your model’s expected input\n",
    "        tokenized2_padded = pad_sequences(tokenized_input, maxlen=max_length, padding=\"post\")\n",
    "        probs = self.ner_model.model.predict(tokenized2_padded)\n",
    "        predicted_labels = np.argmax(probs, axis=2)[0]\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        for i, label in enumerate(predicted_labels[:len(user_input.split())]):\n",
    "            label_str = self.ner_model.label_encoder.inverse_transform([label])[0]\n",
    "            if label_str != 'O':  # Ignore 'O' (Outside) labels\n",
    "                if label_str.startswith(\"B-\"):  # Start of a new entity\n",
    "                    if current_entity:\n",
    "                        entities.append(current_entity)  # Add the previous entity\n",
    "                    current_entity = {\"entity\": label_str, \"text\": user_input.split()[i]}  # Initialize a new entity\n",
    "                elif label_str.startswith(\"I-\"):  # Inside an existing entity\n",
    "                    if current_entity:\n",
    "                        current_entity[\"text\"] += \" \" + user_input.split()[i]  # Add the token to the current entity\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)  # End of an entity\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity:  # Add the last entity\n",
    "            entities.append(current_entity)\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def generate_response(self, intent, entities):\n",
    "        \"\"\"\n",
    "        Generate a response based on the identified intent and entities.\n",
    "        \"\"\"\n",
    "        city_name = None\n",
    "        state_name = None\n",
    "        \n",
    "        # Look for city and state names in the entities\n",
    "        for entity in entities:\n",
    "            if \"city_name\" in entity[\"entity\"]:\n",
    "                city_name = entity[\"text\"]\n",
    "            elif \"state_name\" in entity[\"entity\"]:\n",
    "                state_name = entity[\"text\"]\n",
    "        \n",
    "        if intent == \"flight\":\n",
    "            if city_name:\n",
    "                return f\"Looking up flight information to {city_name}.\"\n",
    "            elif state_name:\n",
    "                return f\"Looking up flight information to {state_name}.\"\n",
    "            else:\n",
    "                return \"Please specify a destination for the flight.\"\n",
    "\n",
    "        elif intent == \"airline\":\n",
    "            return \"Retrieving airline information for you.\"\n",
    "\n",
    "        elif intent == \"airport\":\n",
    "            if city_name or state_name:\n",
    "                location = city_name if city_name else state_name\n",
    "                return f\"Looking up airport information near {location}.\"\n",
    "            else:\n",
    "                return \"Please specify a location for the airport.\"\n",
    "\n",
    "        else:\n",
    "            return \"I'm sorry, I didn't understand your request.\"\n",
    "\n",
    "    def handle_user_input(self, user_input):\n",
    "        \"\"\"\n",
    "        Process user input, identify intent and entities, and generate a response.\n",
    "        \"\"\"\n",
    "        intent = self.get_intent(user_input)\n",
    "        entities = self.get_entities(user_input)\n",
    "        response = self.generate_response(intent, entities)\n",
    "        return response\n",
    "\n",
    "# Initialize the dialog system with the intent and NER models\n",
    "dialog_system = DialogSystem(basic, ner_model)\n",
    "\n",
    "# Define a function to interact in the notebook\n",
    "def chat_with_bot():\n",
    "    print(\"Welcome to the ATIS-based dialog system! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        print(\"You: \", user_input)\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        response = dialog_system.handle_user_input(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Run the dialog system\n",
    "chat_with_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to interact in the notebook\n",
    "def chat_with_bot():\n",
    "    print(\"Welcome to the dialog system! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        response = dialog_system.handle_user_input(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Run the dialog system\n",
    "chat_with_bot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
